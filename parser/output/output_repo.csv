uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs
parser/code_parser.py:get_list_function_name,get_list_function_name,function,17,27,26,352,13.04,1,1,['file_path'],[None],[None],23,"['    """"""The function use to get all functions of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_functions   - List all python functions in the input file\n', '    Example Output:\n', ""        ['func1', 'func2']\n"", '    """"""\n']"
parser/code_parser.py:get_list_class_name,get_list_class_name,function,17,27,26,347,12.85,1,1,['file_path'],[None],[None],45,"['    """"""The function use to get all classes of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_classes     - List all python classes in the input file\n', '    Example Output:\n', ""        ['Class1', 'Class1']\n"", '    """"""\n']"
parser/code_parser.py:get_list_class_methods,get_list_class_methods,function,26,42,37,620,14.76,2,1,['file_path'],[None],[None],70,"['    """"""The function use to get all classes and all methods in this class of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: An array of class info [{dict}, {dict}, ...]\n', '    Example Output:\n', '    [\n', '        {""class_name"": ""Class1"", ""listMethods"": [""method1"", ""method2"", ""method3""]},\n', '        {""class_name"": ""Class2"", ""listMethods"": [""method4"", ""method5"", ""method6""]},\n', '    ]\n', '    """"""\n']"
parser/code_parser.py:get_list_variable_global,get_list_variable_global,function,12,26,24,357,13.73,1,2,['file_path'],[None],[None],102,"['    """"""The function use to get all global variable of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_var         - Array of all global variable\n', '    Example Output:\n', ""        ['Var1', 'Var2']\n"", '    """"""\n']"
parser/code_parser.py:_get_docs,_get_docs,function,12,77,32,669,8.69,1,6,"['all_lines', 'index_1', 'func_lines']","[None, None, None]","[None, None, None]",124,[]
parser/code_parser.py:get_list_function_info,get_list_function_info,function,29,55,48,787,14.31,2,0,['file_path'],[None],[None],164,"['    """"""The function use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of functions, lines of the function, and variable in function\n', '    Example Output:\n', '        [\n', '            {""name"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""name"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']"
parser/code_parser.py:get_list_class_info,get_list_class_info,function,20,49,42,584,11.92,2,0,['file_path'],[None],[None],202,"['    """"""The class use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of functions, lines of the function, and variable in function\n', '    Example Output:\n', '        [\n', '            {""function"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""function"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']"
parser/code_parser.py:get_list_method_info,get_list_method_info,function,35,65,54,966,14.86,3,0,['file_path'],[None],[None],238,"['    """"""get_list_method_info\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of methods in class\n', '    Example Output:\n', '        [\n', '            {""function"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""function"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']"
parser/code_parser.py:get_list_method_stats,get_list_method_stats,function,6,12,11,148,12.33,0,1,['file_path'],[None],[None],280,"['    """"""The function use to get methods stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '                                                    uri                                               name    type  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '    0   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:__init__  method           2       11              11           100           9.090909       0         1      \n', '    1   d:/Project/job/test2/zz936/parser/test/keys.py...                     VerifyingKey:from_public_point  method          10       13              12           185          14.230769       0         0      \n', '    2   d:/Project/job/test2/zz936/parser/test/keys.py...                           VerifyingKey:from_string  method          17       45              39           504          11.200000       0         1      \n', '    3   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:from_pem  method           2        2               2            39          19.500000       0         0      \n', '    4   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:from_der  method          19       64              38           683          10.671875       0         3      \n', '    5   d:/Project/job/test2/zz936/parser/test/keys.py...              VerifyingKey:from_public_key_recovery  method           4        8               8           137          17.125000       0         0      \n', '    6   d:/Project/job/test2/zz936/parser/test/keys.py...  VerifyingKey:from_public_key_recovery_with_digest  method          13       24              23           288          12.000000       0         0      \n', '    7   d:/Project/job/test2/zz936/parser/test/keys.py...                             VerifyingKey:to_string  method           6       11               8           145          13.181818       0         0      \n', '    8   d:/Project/job/test2/zz936/parser/test/keys.py...                                VerifyingKey:to_pem  method           2        4               4            42          10.500000       0         0  \n', '    """"""\n']"
parser/code_parser.py:get_list_class_stats,get_list_class_stats,function,6,12,11,147,12.25,0,1,['file_path'],[None],[None],316,"['    """"""The class use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '                                                    uri               name   type  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '    0  d:/Project/job/test2/zz936/parser/test/keys.py...  BadSignatureError  class           0        1               1             4           4.000000       0         0\n', '    1  d:/Project/job/test2/zz936/parser/test/keys.py...     BadDigestError  class           0        1               1             4           4.000000       0         0\n', '    2  d:/Project/job/test2/zz936/parser/test/keys.py...       VerifyingKey  class          84      301             189          3584          11.906977       0         7\n', '    3  d:/Project/job/test2/zz936/parser/test/keys.py...         SigningKey  class         138      482             310          4615           9.574689       3         9\n', '    """"""\n']"
parser/code_parser.py:get_list_function_stats,get_list_function_stats,function,6,12,11,150,12.5,0,1,['file_path'],[None],[None],347,"['    """"""The function use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '            uri                                 name  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '        0   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...     prepare_target_and_clean_up_test           8       92              32           535           5.815217       0         0\n', '        1   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...                 clean_up_config_test           6       55              19           241           4.381818       0         1\n', '        2   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...         check_default_network_config          22      388              74           955           2.461340       1         5\n', '        3   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...                     check_module_env           9      250              54           553           2.212000       1         1\n', '        4   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...     provision_certificates_to_target           7      101              29           384           3.801980       0         3\n', '        5   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...            config_session_connection           2       14               8            97           6.928571       0         0\n', '        6   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...  config_cipher_suite_and_tcps_action           8      101              30           335           3.316832       0         3\n', '    """"""\n']"
parser/code_parser.py:get_stats,get_stats,function,16,68,46,652,9.59,0,0,"['df', 'file_path']","['pd.DataFrame', 'str']","[None, None]",383,"['    """""" Calculate stats from datafaframe\n', '    Args:\n', '        df: pandas DataFrame\n', '\n', '    Returns:\n', '        pandas DataFrame\n', '\n', '    """"""\n']"
parser/code_parser.py:get_file_stats,get_file_stats,function,11,21,19,244,11.62,1,0,['file_path'],[None],[None],412,"['    """"""The function use to get file stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dict of file stars\n', '    Example Output:\n', '        {\n', '            ""total_functions"": 22,\n', '            ""avg_lines"" : 110.2,\n', '            ""total_class"": 3\n', '        }\n', '    """"""\n']"
parser/code_parser.py:_get_words,_get_words,function,6,23,21,154,6.7,1,1,['row'],[None],[None],440,[]
parser/code_parser.py:_get_avg_char_per_word,_get_avg_char_per_word,function,1,9,9,76,8.44,0,0,['row'],[None],[None],450,[]
parser/code_parser.py:_validate_file,_validate_file,function,5,34,18,227,6.68,0,3,['file_path'],[None],[None],454,"['    """"""Check if the file is existed and it\'s a python file\n', '    """"""\n']"
parser/code_parser.py:_clean_data,_clean_data,function,10,75,27,699,9.32,3,9,['array'],[None],[None],469,"['    """"""Remove empty lines and comment lines start with #\n', '    """"""\n']"
parser/code_parser.py:_remove_empty_line,_remove_empty_line,function,2,7,7,37,5.29,0,1,['line'],[None],[None],512,[]
parser/code_parser.py:_remmove_commemt_line,_remmove_commemt_line,function,2,17,14,97,5.71,0,1,['line'],[None],[None],516,[]
parser/code_parser.py:_get_and_clean_all_lines,_get_and_clean_all_lines,function,5,11,8,124,11.27,0,1,['file_path'],[None],[None],522,"['    """"""Prepare all lines of the file\n', '    """"""\n']"
parser/code_parser.py:_get_all_line,_get_all_line,function,33,195,78,2107,10.81,7,10,['file_path'],[None],[None],532,[]
parser/code_parser.py:_get_all_lines_in_function,_get_all_lines_in_function,function,21,65,44,661,10.17,3,4,"['function_name', 'array', 'indentMethod']","[None, None, None]","[None, None, ""''""]",538,"['    """"""The function use to get all lines of the function\n', '    Args:\n', '        IN: function_name - name of the function will be used to get all line\n', '        IN: array         - list all lines of the file have this input function\n', '        OUT: list_lines   - Array of all line of this function\n', '        OUT: indent       - The indent of this function (this will be used for another calculation)\n', '    """"""\n']"
parser/code_parser.py:_get_all_lines_in_class,_get_all_lines_in_class,function,16,63,44,633,10.05,2,3,"['class_name', 'array']","[None, None]","[None, None]",583,"['    """"""The function use to get all lines of the class\n', '    Args:\n', '        IN: class_name    - name of the class will be used to get all line\n', '        IN: array         - list all lines of the file have this input class\n', '        OUT: list_lines   - Array of all line of this class\n', '    """"""\n']"
parser/code_parser.py:_get_all_lines_define_function,_get_all_lines_define_function,function,20,49,36,538,10.98,2,3,"['function_name', 'array', 'indentMethod']","[None, None, None]","[None, None, ""''""]",622,"['    """"""The function use to get all lines define_function\n', '    Args:\n', '        IN: function_name - name of the function will be used to get all line\n', '        IN: array         - list all lines of the file have this input function\n', '        OUT: list_lines   - Array of all line used to define the function\n', '        OUT: indent       - The indent of this function (this will be used for another calculation)\n', '    """"""\n']"
parser/code_parser.py:_get_define_function_stats,_get_define_function_stats,function,24,145,71,1337,9.22,4,8,['array'],[None],[None],659,"['    """"""The function use to get define function stats: arg_name, arg_type, arg_value\n', '    Args:\n', '        IN: array         - list all lines of function to get variables\n', '        OUT: function stats: arg_name, arg_type, arg_value\n', '    """"""\n']"
parser/code_parser.py:_get_function_stats,_get_function_stats,function,16,206,105,1958,9.5,4,11,"['array', 'indent']","[None, None]","[None, None]",720,"['    """"""The function use to get all lines of the function\n', '    Args:\n', '        IN: indent        - indent string\n', '        IN: array         - list all lines of function to get variables\n', '        OUT: list_var     - Array of all variables\n', '    """"""\n']"
parser/code_parser.py:export_stats_pertype,export_stats_pertype,function,9,46,22,381,8.28,0,4,"['in_path', 'type', 'out_path']","['str', 'str', 'str']","['None', 'None', 'None']",809,"['    """"""\n', '        python code_parser.py type <in_path> <type> <out_path>\n', '    Returns:\n', '\n', '    """"""\n']"
parser/code_parser.py:export_stats_perfile,export_stats_perfile,function,8,36,16,336,9.33,0,3,"['in_path', 'out_path']","['str', 'str']","['None', 'None']",835,"['    """"""\n', '        python code_parser.py  export_stats_perfile <in_path> <out_path>\n', '\n', '    Returns:\n', '\n', '    """"""\n']"
parser/code_parser.py:export_stats_perrepo,export_stats_perrepo,function,12,67,29,640,9.55,1,4,"['in_path', 'out_path']","['str', 'str']","['None', 'None']",859,"['    """""" \n', '        python code_parser.py  export_stats_perfile <in_path> <out_path>\n', '\n', '    Returns:\n', '        1  repo   --->  a single file stats for all sub-diractory\n', '    """"""\n']"
parser/code_parser.py:test_example,test_example,function,3,7,7,233,33.29,0,0,[],[],[],895,[]
parser/generate_md.py:create_markdown_function,create_markdown_function,function,17,63,53,600,9.52,1,0,"['uri', 'name', 'type', 'args_name', 'args_type', 'args_value', 'start_line']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",5,[]
parser/generate_md.py:create_markdown_file,create_markdown_file,function,1,2,2,7,3.5,0,0,['list_info'],[None],[None],31,[]
parser/test/keys.py:prepare_target_and_clean_up_test,prepare_target_and_clean_up_test,function,8,40,30,535,13.38,0,0,"['target', 'prep_target']","[None, None]","[None, None]",376,"['    """"""Set up target init and clean up test config.""""""\n']"
parser/test/keys.py:clean_up_config_test,clean_up_config_test,function,6,17,17,241,14.18,0,1,"['target', 'prep_target']","[None, None]","[None, None]",396,"['    """"""Clean up configuration after testing.""""""\n']"
parser/test/keys.py:check_default_network_config,check_default_network_config,function,22,87,70,955,10.98,1,5,['target'],[None],[None],406,"['    """"""Check and set default network config.""""""\n']"
parser/test/keys.py:check_module_env,check_module_env,function,9,57,52,553,9.7,1,1,"['target', 'prep_target']","[None, None]","[None, None]",447,"['    """"""Verify information about module status and network status.""""""\n']"
parser/test/keys.py:provision_certificates_to_target,provision_certificates_to_target,function,7,37,28,384,10.38,0,3,"['target', 'auth', 'nist', 'root_ca_path', 'client_cert_path', 'client_key_path']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",474,"['    """"""Provide certificates to target.""""""\n']"
parser/test/keys.py:config_session_connection,config_session_connection,function,2,7,7,97,13.86,0,0,"['target', 'prep_target', 'sid']","[None, None, None]","[None, None, '1']",491,"['    """"""Session configuration.""""""\n']"
parser/test/keys.py:config_cipher_suite_and_tcps_action,config_cipher_suite_and_tcps_action,function,8,34,27,335,9.85,0,3,"['target', 'auth', 'nist_value', 'profile_id', 'config_cipher', 'valid']","[None, None, None, None, None, None]","[None, None, None, '0', '1', '1']",495,"['    """"""Verify TCPS operations by using specified cipher suite.""""""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0001,L_RTOS_KTCPTLS_0001,function,11,36,31,434,12.06,0,0,"['target', 'prep_target']","[None, None]","[None, None]",511,"['    """"""TCP over TLS: Server authentication.\n', '    No certificates loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0002,L_RTOS_KTCPTLS_0002,function,12,33,31,439,13.3,0,0,"['target', 'prep_target']","[None, None]","[None, None]",534,"['    """"""TCP over TLS: Server authentication.\n', '    Server Root CA loaded. Connects to server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0003,L_RTOS_KTCPTLS_0003,function,14,33,32,476,14.42,0,0,"['target', 'prep_target']","[None, None]","[None, None]",555,"['    """"""TCP over TLS: Server authentication.\n', '    Server Root CA, Client key, Client root CA loaded. Connects to\n', '    server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0004,L_RTOS_KTCPTLS_0004,function,11,32,29,411,12.84,0,0,"['target', 'prep_target']","[None, None]","[None, None]",576,"['    """"""TCP over TLS: Mutual authentication.\n', '    No certificates loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0005,L_RTOS_KTCPTLS_0005,function,12,32,30,426,13.31,0,0,"['target', 'prep_target']","[None, None]","[None, None]",596,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0006,L_RTOS_KTCPTLS_0006,function,13,32,31,445,13.91,0,0,"['target', 'prep_target']","[None, None]","[None, None]",616,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA loaded. Client root CA loaded. Should not connect to\n', '    server.\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0014,L_RTOS_KTCPTLS_0014,function,14,30,29,480,16.0,0,0,"['target', 'prep_target']","[None, None]","[None, None]",637,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\n', '    (AT+KSSLCRYPTO=0,8,1,8192,4,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0016,L_RTOS_KTCPTLS_0016,function,14,30,29,482,16.07,0,0,"['target', 'prep_target']","[None, None]","[None, None]",659,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-GCM-\n', '    SHA256 (AT+KSSLCRYPTO=0,8,2,8192,4,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0017,L_RTOS_KTCPTLS_0017,function,14,30,29,483,16.1,0,0,"['target', 'prep_target']","[None, None]","[None, None]",681,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-GCM-\n', '    SHA384 (AT+KSSLCRYPTO=0,8,2,16384,8,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0018,L_RTOS_KTCPTLS_0018,function,14,30,29,473,15.77,0,0,"['target', 'prep_target']","[None, None]","[None, None]",703,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-CCM\n', '    (AT+KSSLCRYPTO=0,8,2,16,0,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0019,L_RTOS_KTCPTLS_0019,function,14,30,29,473,15.77,0,0,"['target', 'prep_target']","[None, None]","[None, None]",725,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-CCM\n', '    (AT+KSSLCRYPTO=0,8,2,32,0,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0020,L_RTOS_KTCPTLS_0020,function,14,30,29,476,15.87,0,0,"['target', 'prep_target']","[None, None]","[None, None]",747,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-CCM-8\n', '    (AT+KSSLCRYPTO=0,8,2,256,0,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0021,L_RTOS_KTCPTLS_0021,function,14,30,29,476,15.87,0,0,"['target', 'prep_target']","[None, None]","[None, None]",769,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-CCM-8\n', '    (AT+KSSLCRYPTO=0,8,2,512,0,4,3).\n', '    """"""\n']"
parser/test/keys.py:L_RTOS_KTCPTLS_0022,L_RTOS_KTCPTLS_0022,function,31,163,125,1554,9.53,1,1,"['target', 'prep_target']","[None, None]","[None, None]",791,"['    """"""TCP over TLS: Module as Server.\n', '    Server certicate, Server key loaded.\n', '    """"""\n']"
parser/test/keys.py:BadSignatureError,BadSignatureError,class,0,1,1,4,4.0,0,0,[],[],[],14,[]
parser/test/keys.py:BadDigestError,BadDigestError,class,0,1,1,4,4.0,0,0,[],[],[],18,[]
parser/test/keys.py:VerifyingKey,VerifyingKey,class,84,301,189,3584,11.91,0,7,[],[],[],22,[]
parser/test/keys.py:SigningKey,SigningKey,class,104,381,237,4038,10.6,1,9,[],[],[],145,[]
parser/test/keys.py:VerifyingKey:__init__,VerifyingKey:__init__,method,2,11,11,100,9.09,0,1,"['self', '_error__please_use_generate']","[None, None]","[None, 'None']",23,[]
parser/test/keys.py:VerifyingKey:from_public_point,VerifyingKey:from_public_point,method,10,13,12,185,14.23,0,0,"['klass', 'point', 'curve', 'hashfunc']","[None, None, None, None]","[None, None, 'NIST192p', 'sha1']",29,[]
parser/test/keys.py:VerifyingKey:from_string,VerifyingKey:from_string,method,17,45,39,504,11.2,0,1,"['klass', 'string', 'curve', 'hashfunc', 'validate_point']","[None, None, None, None, None]","[None, None, 'NIST192p', 'sha1', 'True']",38,[]
parser/test/keys.py:VerifyingKey:from_pem,VerifyingKey:from_pem,method,2,2,2,39,19.5,0,0,"['klass', 'string']","[None, None]","[None, None]",56,[]
parser/test/keys.py:VerifyingKey:from_der,VerifyingKey:from_der,method,19,64,38,683,10.67,0,3,"['klass', 'string']","[None, None]","[None, None]",60,[]
parser/test/keys.py:VerifyingKey:from_public_key_recovery,VerifyingKey:from_public_key_recovery,method,4,8,8,137,17.12,0,0,"['klass', 'signature', 'data', 'curve', 'hashfunc', 'sigdecode']","[None, None, None, None, None, None]","[None, None, None, None, 'sha1', 'sigdecode_string']",83,[]
parser/test/keys.py:VerifyingKey:from_public_key_recovery_with_digest,VerifyingKey:from_public_key_recovery_with_digest,method,13,24,23,288,12.0,0,0,"['klass', 'signature', 'digest', 'curve', 'hashfunc', 'sigdecode']","[None, None, None, None, None, None]","[None, None, None, None, 'sha1', 'sigdecode_string']",91,[]
parser/test/keys.py:VerifyingKey:to_string,VerifyingKey:to_string,method,6,11,8,145,13.18,0,0,['self'],[None],[None],106,[]
parser/test/keys.py:VerifyingKey:to_pem,VerifyingKey:to_pem,method,2,4,4,42,10.5,0,0,['self'],[None],[None],115,[]
parser/test/keys.py:VerifyingKey:to_der,VerifyingKey:to_der,method,9,16,14,290,18.12,0,0,['self'],[None],[None],118,[]
parser/test/keys.py:VerifyingKey:verify,VerifyingKey:verify,method,18,51,45,496,9.73,0,2,"['self', 'signature', 'data', 'hashfunc', 'sigdecode']","[None, None, None, None, None]","[None, None, None, 'None', 'sigdecode_string']",127,[]
parser/test/keys.py:VerifyingKey:verify_digest,VerifyingKey:verify_digest,method,13,36,34,303,8.42,0,2,"['self', 'signature', 'digest', 'sigdecode']","[None, None, None, None]","[None, None, None, 'sigdecode_string']",132,[]
parser/test/keys.py:SigningKey:__init__,SigningKey:__init__,method,2,10,10,95,9.5,0,1,"['self', '_error__please_use_generate']","[None, None]","[None, 'None']",23,[]
parser/test/keys.py:SigningKey:generate,SigningKey:generate,method,4,7,7,93,13.29,0,0,"['klass', 'curve', 'entropy', 'hashfunc']","[None, None, None, None]","[None, 'NIST192p', 'None', 'sha1']",151,[]
parser/test/keys.py:SigningKey:from_secret_exponent,SigningKey:from_secret_exponent,method,21,34,31,414,12.18,0,0,"['klass', 'secexp', 'curve', 'hashfunc']","[None, None, None, None]","[None, None, 'NIST192p', 'sha1']",161,[]
parser/test/keys.py:SigningKey:from_string,SigningKey:from_string,method,4,11,11,148,13.45,0,0,"['klass', 'string', 'curve', 'hashfunc']","[None, None, None, None]","[None, None, 'NIST192p', 'sha1']",178,[]
parser/test/keys.py:SigningKey:from_pem,SigningKey:from_pem,method,7,13,13,168,12.92,0,1,"['klass', 'string', 'hashfunc']","[None, None, None]","[None, None, 'sha1']",184,[]
parser/test/keys.py:SigningKey:from_der,SigningKey:from_der,method,22,88,59,735,8.35,0,5,"['klass', 'string', 'hashfunc']","[None, None, None]","[None, None, 'sha1']",193,[]
parser/test/keys.py:SigningKey:to_string,SigningKey:to_string,method,5,7,6,91,13.0,0,0,['self'],[None],[None],106,[]
parser/test/keys.py:SigningKey:to_pem,SigningKey:to_pem,method,2,5,5,45,9.0,0,0,['self'],[None],[None],115,[]
parser/test/keys.py:SigningKey:to_der,SigningKey:to_der,method,7,11,11,263,23.91,0,0,['self'],[None],[None],118,[]
parser/test/keys.py:SigningKey:get_verifying_key,SigningKey:get_verifying_key,method,2,2,2,24,12.0,0,0,['self'],[None],[None],250,[]
parser/test/keys.py:SigningKey:sign_deterministic,SigningKey:sign_deterministic,method,6,12,11,184,15.33,0,0,"['self', 'data', 'hashfunc', 'sigencode', 'extra_entropy']","[None, None, None, None, None]","[None, None, 'None', 'sigencode_string', ""b''""]",253,[]
parser/test/keys.py:SigningKey:sign_digest_deterministic,SigningKey:sign_digest_deterministic,method,17,43,34,364,8.47,1,0,"['self', 'digest', 'hashfunc', 'sigencode', 'extra_entropy']","[None, None, None, None, None]","[None, None, 'None', 'sigencode_string', ""b''""]",263,"['        """"""\n', ""        Calculates 'k' from data itself, removing the need for strong\n"", '        random generator and producing deterministic (reproducible) signatures.\n', '        See RFC 6979 for more details.\n', '        """"""\n']"
parser/test/keys.py:SigningKey:sign,SigningKey:sign,method,41,148,103,1387,9.37,1,2,"['self', 'data', 'hashfunc', 'sigencode', 'extra_entropy']","[None, None, None, None, None]","[None, None, 'None', 'sigencode_string', ""b''""]",289,[]
parser/test/keys.py:SigningKey:sign_digest,SigningKey:sign_digest,method,17,43,34,364,8.47,1,0,"['self', 'digest', 'hashfunc', 'sigencode', 'extra_entropy']","[None, None, None, None, None]","[None, None, 'None', 'sigencode_string', ""b''""]",306,"['""""""Automation of L_RTOS_KTCPTLS_*.""""""\n']"
parser/test/keys.py:SigningKey:sign_number,SigningKey:sign_number,method,11,25,23,165,6.6,0,1,"['self', 'number', 'entropy', 'k']","[None, None, None, None]","[None, None, 'None', 'None']",315,[]
parser/test/test2.py:prepare_target_and_clean_up_test,prepare_target_and_clean_up_test,function,8,40,30,535,13.38,0,0,"['target', 'prep_target']","[None, None]","[None, None]",44,"['    """"""Set up target init and clean up test config.""""""\n']"
parser/test/test2.py:clean_up_config_test,clean_up_config_test,function,6,17,17,241,14.18,0,1,"['target', 'prep_target']","[None, None]","[None, None]",64,"['    """"""Clean up configuration after testing.""""""\n']"
parser/test/test2.py:check_default_network_config,check_default_network_config,function,22,87,70,955,10.98,1,5,['target'],[None],[None],74,"['    """"""Check and set default network config.""""""\n']"
parser/test/test2.py:check_module_env,check_module_env,function,9,57,52,553,9.7,1,1,"['target', 'prep_target']","[None, None]","[None, None]",115,"['    """"""Verify information about module status and network status.""""""\n']"
parser/test/test2.py:provision_certificates_to_target,provision_certificates_to_target,function,7,37,28,384,10.38,0,3,"['target', 'auth', 'nist', 'root_ca_path', 'client_cert_path', 'client_key_path']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",142,"['    """"""Provide certificates to target.""""""\n']"
parser/test/test2.py:config_session_connection,config_session_connection,function,2,7,7,97,13.86,0,0,"['target', 'prep_target', 'sid']","[None, None, None]","[None, None, '1']",159,"['    """"""Session configuration.""""""\n']"
parser/test/test2.py:config_cipher_suite_and_tcps_action,config_cipher_suite_and_tcps_action,function,8,34,27,335,9.85,0,3,"['target', 'auth', 'nist_value', 'profile_id', 'config_cipher', 'valid']","[None, None, None, None, None, None]","[None, None, None, '0', '1', '1']",163,"['    """"""Verify TCPS operations by using specified cipher suite.""""""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0001,L_RTOS_KTCPTLS_0001,function,11,36,31,434,12.06,0,0,"['target', 'prep_target']","[None, None]","[None, None]",179,"['    """"""TCP over TLS: Server authentication.\n', '    No certificates loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0002,L_RTOS_KTCPTLS_0002,function,12,33,31,439,13.3,0,0,"['target', 'prep_target']","[None, None]","[None, None]",202,"['    """"""TCP over TLS: Server authentication.\n', '    Server Root CA loaded. Connects to server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0003,L_RTOS_KTCPTLS_0003,function,14,33,32,476,14.42,0,0,"['target', 'prep_target']","[None, None]","[None, None]",223,"['    """"""TCP over TLS: Server authentication.\n', '    Server Root CA, Client key, Client root CA loaded. Connects to\n', '    server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0004,L_RTOS_KTCPTLS_0004,function,11,32,29,411,12.84,0,0,"['target', 'prep_target']","[None, None]","[None, None]",244,"['    """"""TCP over TLS: Mutual authentication.\n', '    No certificates loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0005,L_RTOS_KTCPTLS_0005,function,12,32,30,426,13.31,0,0,"['target', 'prep_target']","[None, None]","[None, None]",264,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA loaded. Should not connect to server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0006,L_RTOS_KTCPTLS_0006,function,13,32,31,445,13.91,0,0,"['target', 'prep_target']","[None, None]","[None, None]",284,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA loaded. Client root CA loaded. Should not connect to\n', '    server.\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0014,L_RTOS_KTCPTLS_0014,function,14,30,29,480,16.0,0,0,"['target', 'prep_target']","[None, None]","[None, None]",305,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-RSA-WITH-AES-128-GCM-SHA256\n', '    (AT+KSSLCRYPTO=0,8,1,8192,4,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0016,L_RTOS_KTCPTLS_0016,function,14,30,29,482,16.07,0,0,"['target', 'prep_target']","[None, None]","[None, None]",327,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-GCM-\n', '    SHA256 (AT+KSSLCRYPTO=0,8,2,8192,4,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0017,L_RTOS_KTCPTLS_0017,function,14,30,29,483,16.1,0,0,"['target', 'prep_target']","[None, None]","[None, None]",349,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-GCM-\n', '    SHA384 (AT+KSSLCRYPTO=0,8,2,16384,8,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0018,L_RTOS_KTCPTLS_0018,function,14,30,29,473,15.77,0,0,"['target', 'prep_target']","[None, None]","[None, None]",371,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-CCM\n', '    (AT+KSSLCRYPTO=0,8,2,16,0,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0019,L_RTOS_KTCPTLS_0019,function,14,30,29,473,15.77,0,0,"['target', 'prep_target']","[None, None]","[None, None]",393,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-CCM\n', '    (AT+KSSLCRYPTO=0,8,2,32,0,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0020,L_RTOS_KTCPTLS_0020,function,14,30,29,476,15.87,0,0,"['target', 'prep_target']","[None, None]","[None, None]",415,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-128-CCM-8\n', '    (AT+KSSLCRYPTO=0,8,2,256,0,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0021,L_RTOS_KTCPTLS_0021,function,14,30,29,476,15.87,0,0,"['target', 'prep_target']","[None, None]","[None, None]",437,"['    """"""TCP over TLS: Mutual authentication.\n', '    Server Root CA, Client root CA, Client key loaded. Connects to\n', '    server. Use Cipher(NIST Name): TLS-ECDHE-ECDSA-WITH-AES-256-CCM-8\n', '    (AT+KSSLCRYPTO=0,8,2,512,0,4,3).\n', '    """"""\n']"
parser/test/test2.py:L_RTOS_KTCPTLS_0022,L_RTOS_KTCPTLS_0022,function,31,163,125,1554,9.53,1,1,"['target', 'prep_target']","[None, None]","[None, None]",459,"['    """"""TCP over TLS: Module as Server.\n', '    Server certicate, Server key loaded.\n', '    """"""\n']"
parser/test/test2.py:_new_function,_new_function,function,1,1,1,18,18.0,0,0,"['asd', 'zxc']","['str', None]","['None', '""asdad""']",514,[]
parser/test/test2.py:_new_2,_new_2,function,0,1,1,4,4.0,0,0,[],[],[],518,[]
parser/test/test2.py:test_function_multiL_lines,test_function_multiL_lines,function,0,1,1,4,4.0,0,0,"['test', 'multi', 'line']","['float', ' str', ' int']","[None, '""asd""', '12']",523,[]
parser/test/test2.py:test_special_chars,test_special_chars,function,0,1,1,4,4.0,0,0,"['test', 'interger', 'true ']","[' str', ' int ', None]","[' ""Hello, World!""', ' 0', ' True']",529,[]
parser/test2/model_gefs.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],12,[]
parser/test2/model_gefs.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],15,[]
parser/test2/model_gefs.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[]
parser/test2/model_gefs.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",23,[]
parser/test2/model_gefs.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],28,[]
parser/test2/model_gefs.py:fit,fit,function,27,90,80,884,9.82,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",73,"['    """"""\n', '    """"""\n']"
parser/test2/model_gefs.py:eval,eval,function,18,48,46,495,10.31,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",118,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']"
parser/test2/model_gefs.py:predict,predict,function,20,47,36,582,12.38,0,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",138,[]
parser/test2/model_gefs.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",162,[]
parser/test2/model_gefs.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",174,[]
parser/test2/model_gefs.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",187,[]
parser/test2/model_gefs.py:get_dataset,get_dataset,function,7,49,33,414,8.45,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",198,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']"
parser/test2/model_gefs.py:test,test,function,45,327,217,2946,9.01,0,0,['n_sample '],[None],[' 100'],228,[]
parser/test2/model_gefs.py:test_helper,test_helper,function,11,55,39,695,12.64,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",335,[]
parser/test2/model_gefs.py:pd_colcat_get_catcount,pd_colcat_get_catcount,function,16,35,29,262,7.49,1,2,"['df', 'colcat', 'coly', 'continuous_ids']","[None, None, None, None]","[None, None, None, 'None']",373,"['    """"""  Learns the number of categories in each variable and standardizes the df.\n', '        ncat: numpy m The number of categories of each variable. One if the variable is continuous.\n', '    """"""\n']"
parser/test2/model_gefs.py:is_continuous,is_continuous,function,5,29,27,232,8.0,0,1,['v_array'],[None],[None],391,"['    """""" Returns true if df was sampled from a continuous variables, and false\n', '    """"""\n']"
parser/test2/model_gefs.py:test2,test2,function,39,78,74,892,11.44,0,0,[],[],[],404,[]
parser/test2/model_gefs.py:get_dummies,get_dummies,function,9,18,15,160,8.89,1,1,['data'],[None],[None],446,[]
parser/test2/model_gefs.py:learncats,learncats,function,16,35,29,262,7.49,1,2,"['data', 'classcol', 'continuous_ids']","[None, None, None]","[None, 'None', '[]']",456,"['    """"""\n', '        Learns the number of categories in each variable and standardizes the data.\n', '        ----------\n', '        data: numpy n x m\n', '            Numpy array comprising n realisations (instances) of m variables.\n', '        classcol: int  The column index of the class variables (if any).\n', '        continuous_ids: list of ints\n', '            List containing the indices of known continuous variables. Useful for\n', '            discrete data like age, which is better modeled as continuous.\n', '        Returns\n', '        -------\n', '        ncat: numpy m  The number of categories of each variable. One if the variable is  continuous.\n', '    """"""\n']"
parser/test2/model_gefs.py:gef_is_continuous,gef_is_continuous,function,5,28,25,225,8.04,0,1,['data'],[None],[None],482,"['    """"""\n', '        Returns true if data was sampled from a continuous variables, and false\n', '    """"""\n']"
parser/test2/model_gefs.py:gef_get_stats,gef_get_stats,function,29,90,50,703,7.81,1,3,"['data', 'ncat']","[None, None]","[None, 'None']",497,"['    """"""\n', '        Compute univariate statistics for continuous variables.\n', '    """"""\n']"
parser/test2/model_gefs.py:gef_normalize_data,gef_normalize_data,function,9,17,15,123,7.24,1,1,"['data', 'maxv', 'minv']","[None, None, None]","[None, None, None]",527,"['    """"""\n', '        Normalizes the data given the maximum and minimum values of each variable.\n', '    """"""\n']"
parser/test2/model_gefs.py:gef_standardize_data,gef_standardize_data,function,9,23,19,140,6.09,1,1,"['data', 'mean', 'std']","[None, None, None]","[None, None, None]",538,"['    """"""\n', '        Standardizes the data given the mean and standard deviations values of\n', '    """"""\n']"
parser/test2/model_gefs.py:train_test_split2,train_test_split2,function,25,65,41,686,10.55,0,1,"['data', 'ncat', 'train_ratio', 'prep']","[None, None, None, None]","[None, None, '0.7', ""'std'""]",554,[]
parser/test2/model_gefs.py:test_converion,test_converion,function,30,72,66,779,10.82,0,0,[],[],[],587,"['    """"""\n', '    General comments on the API¶\n', '    There are four different functions to do classification with GeFs.\n', '\n', '    classify\n', '    classify_avg\n', '    classify_lspn\n', '    classify_avg_lspn\n', '    The first two, classify and classify_avg, exploit class factorised leaves to run inference faster\n', '    (propagate the probabilities of all classes at once). That, of course, only works if the leaves\n', '    are class factorised (e.g. learnsp=np.Inf). Otherwise, one should use classify_lspn and classify_avg_lspn which work\n', '     with any PC (in particular those with a LearnSPN network at the leaves, hence the name).\n', '\n', ""    The other important distinction is that avg methods assume a model learned as an ensemble and performs inference by 'averaging' the distribution of each of the base models. These are the methods that match the original Random Forest in terms of classification (with complete data, and class factorised leaves). In contrast, the other methods run inference as if the model is a single PC. One can interpret that as giving different weights to each of the base models according to the likelihood of the instance to be classified (base models under which the instance is more likely are given higher weights).\n"", '    This inference method is referred to as GeF+ in the paper, as it defines a mixture over the base models.\n', '\n', '\n', '    :return:\n', '    """"""\n']"
parser/test2/model_gefs.py:train_test_split,train_test_split,function,25,65,41,686,10.55,0,1,"['data', 'ncat', 'train_ratio', 'prep']","[None, None, None, None]","[None, None, '0.7', ""'std'""]",636,[]
parser/test2/model_gefs.py:adult,adult,function,9,34,31,427,12.56,0,0,['data'],[None],[None],657,[]
parser/test2/model_gefs.py:australia,australia,function,11,35,34,337,9.63,0,0,['data'],[None],[None],667,[]
parser/test2/model_gefs.py:bank,bank,function,10,40,38,474,11.85,0,0,['data'],[None],[None],676,[]
parser/test2/model_gefs.py:credit,credit,function,9,43,42,466,10.84,0,0,['data'],[None],[None],687,[]
parser/test2/model_gefs.py:electricity,electricity,function,9,25,24,292,11.68,0,0,['data'],[None],[None],698,[]
parser/test2/model_gefs.py:segment,segment,function,11,38,37,545,14.34,0,0,['data'],[None],[None],707,[]
parser/test2/model_gefs.py:german,german,function,9,33,32,227,6.88,0,0,['data'],[None],[None],718,[]
parser/test2/model_gefs.py:vowel,vowel,function,8,13,12,180,13.85,0,0,['data'],[None],[None],726,[]
parser/test2/model_gefs.py:cmc,cmc,function,9,21,20,378,18.0,0,0,['data'],[None],[None],733,[]
parser/test2/model_gefs.py:get_data,get_data,function,7,49,33,414,8.45,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",742,"['""""""\n', 'def test_dataset_classi_fake(nrows=500):\n', '    from sklearn import datasets as sklearn_datasets\n', '    ndim=11\n', ""    coly   = ['y']\n"", '    colnum = [""colnum_"" +str(i) for i in range(0, ndim) ]\n', ""    colcat = ['colcat_1']\n"", '    X, y    = sklearn_datasets.make_classification(\n', '        n_samples=1000,\n', '        n_features=ndim,\n', '        # No n_targets param for make_classification\n', '        # n_targets=1,\n', '        # Fake dataset, classification on 2 classes\n', '        n_classes=2,\n', '        # In classification, n_informative should be less than n_features\n', '        n_informative=ndim - 2\n', '    )\n', '    df         = pd.DataFrame(X,  columns= colnum)\n', '    df[coly]   = y.reshape(-1, 1)\n', '    for ci in colcat :\n', '      df[colcat] = np.random.randint(2, len(df))\n', '    return df, colnum, colcat, coly\n', '""""""\n']"
parser/test2/model_gefs.py:Model,Model,class,32,71,57,567,7.99,0,2,[],[],[],51,[]
parser/test2/model_gefs.py:Model:__init__,Model:__init__,method,31,66,52,499,7.56,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",52,[]
parser/test2/model_sklearn.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],12,[]
parser/test2/model_sklearn.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],15,[]
parser/test2/model_sklearn.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[]
parser/test2/model_sklearn.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",23,[]
parser/test2/model_sklearn.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],28,[]
parser/test2/model_sklearn.py:model_automl,model_automl,function,7,32,26,367,11.47,0,0,[],[],[],43,[]
parser/test2/model_sklearn.py:fit,fit,function,12,33,29,365,11.06,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",71,"['    """"""\n', '    """"""\n']"
parser/test2/model_sklearn.py:predict,predict,function,15,43,36,448,10.42,0,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",85,[]
parser/test2/model_sklearn.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",106,[]
parser/test2/model_sklearn.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",118,[]
parser/test2/model_sklearn.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",131,[]
parser/test2/model_sklearn.py:get_dataset_split_for_model_pandastuple,get_dataset_split_for_model_pandastuple,function,22,69,53,627,9.09,1,3,"['Xtrain', 'ytrain', 'data_pars', '']","[None, None, None, None]","[None, 'None', 'None', None]",143,"['    """"""  Split data for moel input/\n', '    Xtrain  ---> Split INTO  tuple of data  Xtuple= (df1, df2, df3) to fit model input.\n', '    :param Xtrain:\n', '    :param coldataloader_received:\n', '    :param colmodel_ref:\n', '    :return:\n', '    """"""\n']"
parser/test2/model_sklearn.py:get_dataset2,get_dataset2,function,17,58,38,618,10.66,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",174,"['    """"""\n', '       Raw Data (Path)   --->  Input Object (ie Pandas, ...) for Model training\n', '    """"""\n']"
parser/test2/model_sklearn.py:get_dataset,get_dataset,function,39,186,116,1820,9.78,1,11,"['Xtrain', 'ytrain', 'data_pars', '']","[None, None, None, None]","[None, 'None', 'None', None]",199,[]
parser/test2/model_sklearn.py:get_params_sklearn,get_params_sklearn,function,2,2,2,39,19.5,0,0,['deep'],[None],['False'],227,[]
parser/test2/model_sklearn.py:get_params,get_params,function,15,34,30,377,11.09,0,1,['deep'],[None],['False'],231,[]
parser/test2/model_sklearn.py:test,test,function,35,232,163,2289,9.87,1,0,['n_sample          '],[None],[' 1000'],251,[]
parser/test2/model_sklearn.py:zz_eval,zz_eval,function,16,42,41,495,11.79,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",382,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']"
parser/test2/model_sklearn.py:zz_preprocess,zz_preprocess,function,21,68,38,740,10.88,0,2,['prepro_pars'],[None],[None],406,[]
parser/test2/model_sklearn.py:Model,Model,class,14,32,29,393,12.28,0,2,[],[],[],54,[]
parser/test2/model_sklearn.py:Model:__init__,Model:__init__,method,13,26,23,324,12.46,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",55,[]
parser/test3/arrow_dataset.py:transmit_format,transmit_format,function,26,109,87,1119,10.27,1,4,['func'],[None],[None],166,"['    """"""Wrapper for dataset transforms that recreate a new Dataset to transmit the format of the original dataset to the new dataset""""""\n']"
parser/test3/arrow_dataset.py:update_metadata_with_features,update_metadata_with_features,function,15,33,27,554,16.79,0,2,"['table', 'features']","[' Table', ' Features']","[None, None]",207,"['    """"""To be used in dataset transforms that modify the features of the dataset, in order to update the features stored in the metadata of its schema.""""""\n']"
parser/test3/arrow_dataset.py:_check_table,_check_table,function,5,22,19,189,8.59,0,1,['table'],[None],[None],223,"['    """"""We check the table type to make sure it\'s an instance of :class:`datasets.table.Table`""""""\n']"
parser/test3/arrow_dataset.py:concatenate_datasets,concatenate_datasets,function,38,202,121,1767,8.75,3,9,"['dsets', 'info', 'split', 'axis', '']","[' List[Dataset]', ' Optional[Any] ', ' Optional[Any] ', ' int ', None]","[None, ' None', ' None', ' 0', None]",3228,"['    """"""\n', '    Converts a list of :class:`Dataset` with the same schema into a single :class:`Dataset`.\n', '\n', '    Args:\n', '        dsets (:obj:`List[datasets.Dataset]`): List of Datasets to concatenate.\n', '        info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '        split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '        axis (``{0, 1}``, default ``0``, meaning over rows):\n', '            Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n', '            (horizontally).\n', '\n', '            .. versionadded:: 1.6.0\n', '    """"""\n']"
parser/test3/arrow_dataset.py:map_function,map_function,function,23,90,62,735,8.17,4,4,"['batch', '*args', 'function', 'with_indices', '**fn_kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",3319,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin,DatasetInfoMixin,class,32,107,48,1096,10.24,0,0,[],[],[],90,[]
parser/test3/arrow_dataset.py:DatasetTransformationNotAllowedError,DatasetTransformationNotAllowedError,class,0,1,1,4,4.0,0,0,[],[],[],162,[]
parser/test3/arrow_dataset.py:Dataset,Dataset,class,627,5811,1942,60396,10.39,36,199,[],[],[],235,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:__init__,DatasetInfoMixin:__init__,method,4,4,4,33,8.25,0,0,"['self', 'info', 'split']","[None, ' DatasetInfo', ' Optional[NamedSplit]']","[None, None, None]",95,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:info,DatasetInfoMixin:info,method,2,2,2,16,8.0,0,0,['self'],[None],[None],100,"['        """""":class:`datasets.DatasetInfo` object containing all the metadata in the dataset.""""""\n']"
parser/test3/arrow_dataset.py:DatasetInfoMixin:split,DatasetInfoMixin:split,method,2,2,2,17,8.5,0,0,['self'],[None],[None],105,"['        """""":class:`datasets.NamedSplit` object corresponding to a named dataset split.""""""\n']"
parser/test3/arrow_dataset.py:DatasetInfoMixin:builder_name,DatasetInfoMixin:builder_name,method,2,2,2,29,14.5,0,0,['self'],[None],[None],110,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:citation,DatasetInfoMixin:citation,method,2,2,2,25,12.5,0,0,['self'],[None],[None],114,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:config_name,DatasetInfoMixin:config_name,method,2,2,2,28,14.0,0,0,['self'],[None],[None],118,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:dataset_size,DatasetInfoMixin:dataset_size,method,2,2,2,29,14.5,0,0,['self'],[None],[None],122,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:description,DatasetInfoMixin:description,method,2,2,2,28,14.0,0,0,['self'],[None],[None],126,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:download_checksums,DatasetInfoMixin:download_checksums,method,2,2,2,35,17.5,0,0,['self'],[None],[None],130,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:download_size,DatasetInfoMixin:download_size,method,2,2,2,30,15.0,0,0,['self'],[None],[None],134,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:features,DatasetInfoMixin:features,method,2,2,2,25,12.5,0,0,['self'],[None],[None],138,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:homepage,DatasetInfoMixin:homepage,method,2,2,2,25,12.5,0,0,['self'],[None],[None],142,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:license,DatasetInfoMixin:license,method,2,2,2,24,12.0,0,0,['self'],[None],[None],146,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:size_in_bytes,DatasetInfoMixin:size_in_bytes,method,2,2,2,30,15.0,0,0,['self'],[None],[None],150,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:supervised_keys,DatasetInfoMixin:supervised_keys,method,2,2,2,32,16.0,0,0,['self'],[None],[None],154,[]
parser/test3/arrow_dataset.py:DatasetInfoMixin:version,DatasetInfoMixin:version,method,2,2,2,24,12.0,0,0,['self'],[None],[None],158,[]
parser/test3/arrow_dataset.py:Dataset:__init__,Dataset:__init__,method,64,231,141,2226,9.64,1,9,"['self', 'arrow_table', 'info', 'split', 'indices_table', 'fingerprint', '']","[None, ' Table', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[Table] ', ' Optional[str] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",238,[]
parser/test3/arrow_dataset.py:Dataset:from_file,Dataset:from_file,method,6,21,19,281,13.38,0,1,"['cls', 'filename', 'info', 'split', 'indices_filename', 'in_memory', '']","[None, ' str', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[str] ', ' bool ', None]","[None, None, ' None', ' None', ' None', ' False', None]",310,"['        """"""Instantiate a Dataset backed by an Arrow table at filename.\n', '\n', '        Args:\n', '            filename (:obj:`str`): File name of the dataset.\n', '            info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '            indices_filename (:obj:`str`, optional): File names of the indices.\n', '            in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_buffer,Dataset:from_buffer,method,6,17,15,206,12.12,0,1,"['cls', 'buffer', 'info', 'split', 'indices_buffer', '']","[None, ' pa.Buffer', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[pa.Buffer] ', None]","[None, None, ' None', ' None', ' None', None]",345,"['        """"""Instantiate a Dataset backed by an Arrow buffer.\n', '\n', '        Args:\n', '            buffer (:obj:`pyarrow.Buffer`): Arrow buffer.\n', '            info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '            indices_buffer (:obj:`pyarrow.Buffer`, optional): Indices Arrow buffer.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_pandas,Dataset:from_pandas,method,10,66,35,459,6.95,0,4,"['cls', 'df', 'features', 'info', 'split', '']","[None, ' pd.DataFrame', ' Optional[Features] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', None]","[None, None, ' None', ' None', ' None', None]",373,"['        """"""\n', '        Convert :obj:`pandas.DataFrame` to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\n', '\n', '        The column types in the resulting Arrow Table are inferred from the dtypes of the pandas.Series in the\n', '        DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n', '        case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n', '\n', ""        Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n"", '        type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n', '        contains None/nan objects, the type is set to null. This behavior can be avoided by constructing explicit\n', '        features and passing it to this function.\n', '\n', '        Args:\n', '            df (:obj:`pandas.DataFrame`): Dataframe that contains the dataset.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_dict,Dataset:from_dict,method,18,87,48,660,7.59,1,5,"['cls', 'mapping', 'features', 'info', 'split', '']","[None, ' dict', ' Optional[Features] ', ' Optional[Any] ', ' Optional[Any] ', None]","[None, None, ' None', ' None', ' None', None]",415,"['        """"""\n', '        Convert :obj:`dict` to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\n', '\n', '        Args:\n', '            mapping (:obj:`Mapping`): Mapping of strings to Arrays or Python lists.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_csv,Dataset:from_csv,method,4,13,13,169,13.0,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",456,"['        """"""Create Dataset from CSV file(s).\n', '\n', '        Args:\n', '            path_or_paths (path-like or list of path-like): Path(s) of the CSV file(s).\n', '            split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (:obj:`str`, optional, default ``""~/.cache/huggingface/datasets""``): Directory to cache data.\n', '            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '            **kwargs: Keyword arguments to be passed to :meth:`pandas.read_csv`.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_json,Dataset:from_json,method,4,14,14,191,13.64,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'field', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[str] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', None, None]",485,"['        """"""Create Dataset from JSON or JSON Lines file(s).\n', '\n', '        Args:\n', '            path_or_paths (path-like or list of path-like): Path(s) of the JSON or JSON Lines file(s).\n', '            split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (:obj:`str`, optional, default ``""~/.cache/huggingface/datasets""``): Directory to cache data.\n', '            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '            field (:obj:`str`, optional): Field name of the JSON file where the dataset is contained in.\n', '            **kwargs: Keyword arguments to be passed to :class:`JsonConfig`.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_parquet,Dataset:from_parquet,method,4,14,14,204,14.57,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'columns', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[List[str]] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', None, None]",522,"['        """"""Create Dataset from Parquet file(s).\n', '\n', '        Args:\n', '            path_or_paths (path-like or list of path-like): Path(s) of the Parquet file(s).\n', '            split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (:obj:`str`, optional, default ``""~/.cache/huggingface/datasets""``): Directory to cache data.\n', '            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '            columns (:obj:`List[str]`, optional): If not None, only these columns will be read from the file.\n', ""                A column name may be a prefix of a nested field, e.g. 'a' will select\n"", ""                'a.b', 'a.c', and 'a.d.e'.\n"", '            **kwargs: Keyword arguments to be passed to :class:`ParquetConfig`.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:from_text,Dataset:from_text,method,4,13,13,172,13.23,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",561,"['        """"""Create Dataset from text file(s).\n', '\n', '        Args:\n', '            path_or_paths (path-like or list of path-like): Path(s) of the text file(s).\n', '            split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (:obj:`str`, optional, default ``""~/.cache/huggingface/datasets""``): Directory to cache data.\n', '            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '            **kwargs: Keyword arguments to be passed to :class:`TextConfig`.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:__del__,Dataset:__del__,method,4,10,7,83,8.3,0,2,['self'],[None],[None],589,[]
parser/test3/arrow_dataset.py:Dataset:__enter__,Dataset:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],595,[]
parser/test3/arrow_dataset.py:Dataset:__exit__,Dataset:__exit__,method,1,1,1,14,14.0,0,0,"['self', 'exc_type', 'exc_val', 'exc_tb']","[None, None, None, None]","[None, None, None, None]",598,[]
parser/test3/arrow_dataset.py:Dataset:save_to_disk,Dataset:save_to_disk,method,37,201,133,2251,11.2,3,4,"['self', 'dataset_path', 'fs']","[None, ' str', None]","[None, None, 'None']",602,"['        """"""\n', '        Saves a dataset to a dataset directory, or in a filesystem using either :class:`~filesystems.S3FileSystem` or\n', '        any implementation of ``fsspec.spec.AbstractFileSystem``.\n', '\n', '\n', '        Note regarding sliced datasets:\n', '\n', '        If you sliced the dataset in some way (using shard, train_test_split or select for example), then an indices mapping\n', '        is added to avoid having to rewrite a new arrow Table (save time + disk/memory usage).\n', '        It maps the indices used by __getitem__ to the right rows if the arrow Table.\n', '        By default save_to_disk does save the full dataset table + the mapping.\n', '\n', '        If you want to only save the shard of the dataset instead of the original arrow file and the indices,\n', '        then you have to call :func:`datasets.Dataset.flatten_indices` before saving.\n', '        This will create a new arrow table by using the right rows of the original table.\n', '\n', '        Args:\n', '            dataset_path (:obj:`str`): Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n', '                of the dataset directory where the dataset will be saved to.\n', '            fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, defaults ``None``):\n', '                Instance of the remote filesystem used to download the files from.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:load_from_disk,Dataset:load_from_disk,method,40,142,97,1811,12.75,2,5,"['dataset_path', 'fs', 'keep_in_memory']","[' str', None, ' Optional[bool] ']","[None, 'None', ' None']",697,"['        """"""\n', '        Loads a dataset that was previously saved using :meth:`save_to_disk` from a dataset directory, or from a\n', '        filesystem using either :class:`~filesystems.S3FileSystem` or any implementation of\n', '        ``fsspec.spec.AbstractFileSystem``.\n', '\n', '        Args:\n', '            dataset_path (:obj:`str`): Path (e.g. `dataset/train`) or remote URI (e.g. `s3//my-bucket/dataset/train`) of\n', '                the dataset directory where the dataset will be loaded from.\n', '            fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n', '                Instance of the remote filesystem used to download the files from.\n', '            keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the\n', '                dataset will not be copied in-memory unless explicitly enabled by setting\n', '                `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n', '                :ref:`load_dataset_enhancing_performance` section.\n', '\n', '        Returns:\n', '            :class:`Dataset` or :class:`DatasetDict`.\n', '                - if `dataset_path` is a path of a dataset directory: the :class:`Dataset` requested,\n', '                - if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:data,Dataset:data,method,2,2,2,16,8.0,0,0,['self'],[None],[None],771,"['        """"""The Apache Arrow table backing the dataset.""""""\n']"
parser/test3/arrow_dataset.py:Dataset:cache_files,Dataset:cache_files,method,4,17,16,189,11.12,0,1,['self'],[None],[None],776,"['        """"""The cache files containing the Apache Arrow table backing the dataset.""""""\n']"
parser/test3/arrow_dataset.py:Dataset:num_columns,Dataset:num_columns,method,2,2,2,28,14.0,0,0,['self'],[None],[None],784,"['        """"""Number of columns in the dataset.""""""\n']"
parser/test3/arrow_dataset.py:Dataset:num_rows,Dataset:num_rows,method,4,9,8,80,8.89,0,1,['self'],[None],[None],789,"['        """"""Number of rows in the dataset (same as :meth:`Dataset.__len__`).""""""\n']"
parser/test3/arrow_dataset.py:Dataset:column_names,Dataset:column_names,method,2,2,2,29,14.5,0,0,['self'],[None],[None],796,"['        """"""Names of the columns in the dataset.""""""\n']"
parser/test3/arrow_dataset.py:Dataset:shape,Dataset:shape,method,3,10,9,102,10.2,0,1,['self'],[None],[None],801,"['        """"""Shape of the dataset (number of columns, number of rows).""""""\n']"
parser/test3/arrow_dataset.py:Dataset:unique,Dataset:unique,method,21,68,53,518,7.62,0,2,"['self', 'column']","[None, ' str']","[None, None]",807,"['        """"""Return a list of the unique elements in a column.\n', '\n', '        This is implemented in the low-level backend and as such, very fast.\n', '\n', '        Args:\n', '            column (:obj:`str`): Column name (list all the column names with :func:`datasets.Dataset.column_names`).\n', '\n', '        Returns:\n', '            :obj:`list`: List of unique elements in the given column.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:class_encode_column,Dataset:class_encode_column,method,20,77,58,787,10.22,0,3,"['self', 'column']","[None, ' str']","[None, None]",830,"['        """"""Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the table.\n', '\n', '        Args:\n', '            column (`str`): The name of the column to cast (list all the column names with :func:`datasets.Dataset.column_names`)\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:dictionary_encode_column_,Dataset:dictionary_encode_column_,method,19,35,32,563,16.09,0,1,"['self', 'column']","[None, ' str']","[None, None]",867,"['        """"""Dictionary encode a column.\n', '\n', '        Dictionary encode can reduce the size of a column with many repetitions (e.g. string labels columns)\n', '        by storing a dictionary of the strings. This only affect the internal storage.\n', '\n', '        .. deprecated:: 1.4.0\n', '\n', '        Args:\n', '            column (:obj:`str`):\n', '\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:flatten_,Dataset:flatten_,method,10,39,31,380,9.74,1,1,"['self', 'max_depth']","[None, None]","[None, '16']",892,"['        """"""In-place version of :meth:`Dataset.flatten`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`Dataset.flatten` instead.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:flatten,Dataset:flatten,method,10,39,31,380,9.74,1,1,"['self', 'max_depth']","[None, None]","[None, '16']",910,"['        """"""Flatten the table.\n', '        Each column with a struct type is flattened into one column per struct field.\n', '        Other columns are left unchanged.\n', '\n', '        Returns:\n', '            :class:`Dataset`: A copy of the dataset with flattened columns.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:cast_,Dataset:cast_,method,23,59,52,747,12.66,0,1,"['self', 'features', 'batch_size', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'num_proc', '']","[None, ' Features', ' Optional[int] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[int] ', None]","[None, None, ' 10_000', ' False', ' True', ' None', ' 10_000', ' None', None]",933,"['        """"""In-place version of :meth:`Dataset.cast`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`Dataset.cast` instead.\n', '\n', '        Args:\n', '            features (:class:`datasets.Features`): New features to cast the dataset to.\n', '                The name of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n', '            batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to cast.\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to cast.\n', '            keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n', '            load_from_cache_file (:obj:`bool`, default `True` if caching is enabled): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`Optional[str]`, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', ""            num_proc (`Optional[int]`, default `None`): Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:cast,Dataset:cast,method,24,82,71,978,11.93,0,1,"['self', 'features', 'batch_size', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'num_proc', '']","[None, ' Features', ' Optional[int] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[int] ', None]","[None, None, ' 10_000', ' False', ' True', ' None', ' 10_000', ' None', None]",990,[]
parser/test3/arrow_dataset.py:Dataset:remove_columns_,Dataset:remove_columns_,method,10,42,30,424,10.1,2,2,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",1050,"['        """"""In-place version of :meth:`Dataset.remove_columns`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`Dataset.remove_columns` instead.\n', '\n', '        Args:\n', '            column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:remove_columns,Dataset:remove_columns,method,10,42,30,424,10.1,2,2,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",1076,"['        """"""\n', '        Remove one or several column(s) in the dataset and the features associated to them.\n', '\n', '        You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n', ""        is in-place (doesn't copy the data to a new dataset) and is thus faster.\n"", '\n', '        Args:\n', '            column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n', '            new_fingerprint\n', '\n', '        Returns:\n', '            :class:`Dataset`: A copy of the dataset object without the columns to remove.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:rename_column_,Dataset:rename_column_,method,15,111,56,969,8.73,1,5,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",1111,"['        """"""In-place version of :meth:`Dataset.rename_column`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`Dataset.rename_column` instead.\n', '\n', '        Args:\n', '            original_column_name (:obj:`str`): Name of the column to rename.\n', '            new_column_name (:obj:`str`): New name for the column.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:rename_column,Dataset:rename_column,method,15,111,56,969,8.73,1,5,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",1153,"['        """"""\n', '        Rename a column in the dataset, and move the features associated to the original column under the new column\n', '        name.\n', '\n', '        Args:\n', '            original_column_name (:obj:`str`): Name of the column to rename.\n', '            new_column_name (:obj:`str`): New name for the column.\n', '            new_fingerprint\n', '\n', '        Returns:\n', '            :class:`Dataset`: A copy of the dataset with a renamed column.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:rename_columns,Dataset:rename_columns,method,23,119,82,1228,10.32,2,5,"['self', 'column_mapping', 'str]', 'new_fingerprint']","[None, ' Dict[str', None, None]","[None, None, None, None]",1201,"['        """"""\n', '        Rename several columns in the dataset, and move the features associated to the original columns under\n', '        the new column names.\n', '\n', '        Args:\n', '            column_mapping (:obj:`Dict[str, str]`): A mapping of columns to rename to their new names\n', '\n', '        Returns:\n', '            :class:`Dataset`: A copy of the dataset with renamed columns\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:__len__,Dataset:__len__,method,2,2,2,19,9.5,0,0,['self'],[None],[None],1251,"['        """"""Number of rows in the dataset.""""""\n']"
parser/test3/arrow_dataset.py:Dataset:__iter__,Dataset:__iter__,method,11,20,20,328,16.4,1,0,['self'],[None],[None],1255,"['        """"""Iterate through the examples.\n', '\n', '        If a formatting is set with :meth:`Dataset.set_format` rows will be returned with the\n', '        selected format.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:__repr__,Dataset:__repr__,method,2,6,6,90,15.0,0,0,['self'],[None],[None],1274,[]
parser/test3/arrow_dataset.py:Dataset:format,Dataset:format,method,1,17,17,200,11.76,0,0,['self'],[None],[None],1278,[]
parser/test3/arrow_dataset.py:Dataset:formatted_as,Dataset:formatted_as,method,10,19,19,339,17.84,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",1287,"['        """"""To be used in a `with` statement. Set __getitem__ return format (type and columns).\n', '\n', '        Args:\n', ""            type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                None means __getitem__ returns all columns (default)\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:set_format,Dataset:set_format,method,18,108,75,835,7.73,0,3,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",1315,"['        """"""Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format ``type`` (for example ""numpy"") is used to format batches when using __getitem__.\n', ""        It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.set_transform`.\n"", '\n', '        Args:\n', '            type (Optional ``str``):\n', ""                Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output.\n', '                None means __getitem__ returns all columns (default).\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        It is possible to call ``map`` after calling ``set_format``. Since ``map`` may add new columns, then the list of formatted columns\n', '        gets updated. In this case, if you apply ``map`` on a dataset to add a new column, then this column will be formatted:\n', '\n', '            new formatted columns = (all columns - previously unformatted columns)\n', '\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:reset_format,Dataset:reset_format,method,1,1,1,17,17.0,0,0,['self'],[None],[None],1369,"['        """"""Reset __getitem__ return format to python objects and all columns.\n', '\n', '        Same as ``self.set_format()``\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:set_transform,Dataset:set_transform,method,1,4,4,99,24.75,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",1376,"['        """"""Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n', '        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n', '\n', '        Args:\n', '            transform (Optional ``Callable``): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in __getitem__.\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:with_format,Dataset:with_format,method,4,8,7,141,17.62,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",1397,"['        """"""Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format ``type`` (for example ""numpy"") is used to format batches when using __getitem__.\n', '\n', ""        It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.with_transform`.\n"", '\n', '        Contrary to :func:`datasets.Dataset.set_format`, ``with_format`` returns a new Dataset object.\n', '\n', '        Args:\n', '            type (Optional ``str``):\n', ""                Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                None means __getitem__ returns all columns (default)\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:with_transform,Dataset:with_transform,method,4,7,6,138,19.71,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",1424,"['        """"""Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n', '\n', '        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`.\n', '\n', '        Contrary to :func:`datasets.Dataset.set_transform`, ``with_transform`` returns a new Dataset object.\n', '\n', '        Args:\n', '            transform (Optional ``Callable``): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in __getitem__.\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:prepare_for_task,Dataset:prepare_for_task,method,23,153,97,1421,9.29,3,6,"['self', 'task', 'TaskTemplate]']","[None, ' Union[str', None]","[None, None, None]",1450,"['        """"""Prepare a dataset for the given task by casting the dataset\'s :class:`Features` to standardized column names and types as detailed in :py:mod:`datasets.tasks`.\n', '\n', '        Casts :attr:`datasets.DatasetInfo.features` according to a task-specific schema. Intended for single-use only, so all task templates are removed from :attr:`datasets.DatasetInfo.task_templates` after casting.\n', '\n', '        Args:\n', '            task (:obj:`Union[str, TaskTemplate]`): The task to prepare the dataset for during training and evaluation. If :obj:`str`, supported tasks include:\n', '\n', '                - :obj:`""text-classification""`\n', '                - :obj:`""question-answering""`\n', '\n', '                If :obj:`TaskTemplate`, must be one of the task templates in :py:mod:`datasets.tasks`.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:_getitem,Dataset:_getitem,method,8,33,24,364,11.03,0,1,"['self', 'key', 'slice', 'str]', 'format_type', 'format_columns', 'output_all_columns', 'format_kwargs', '']","[None, ' Union[int', None, None, None, None, None, None, None]","[None, None, None, None, 'None', 'None', 'False', 'None', None]",1496,"['        """"""\n', '        Can be used to index columns (by string names) or rows (by integer index, slices, or iter of indices or bools)\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:__getitem__,Dataset:__getitem__,method,2,8,8,175,21.88,0,0,"['self', 'key', 'slice', 'str]']","[None, ' Union[int', None, None]","[None, None, None, None]",1515,"['        """"""Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""\n']"
parser/test3/arrow_dataset.py:Dataset:cleanup_cache_files,Dataset:cleanup_cache_files,method,17,55,44,685,12.45,2,3,['self'],[None],[None],1525,"['        """"""Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is\n', '        one.\n', '\n', '        Be careful when running this command that no other process is currently using other cache files.\n', '\n', '        Returns:\n', '            :obj:`int`: Number of removed files.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:_get_cache_file_path,Dataset:_get_cache_file_path,method,9,22,17,361,16.41,0,1,"['self', 'fingerprint']","[None, None]","[None, None]",1553,[]
parser/test3/arrow_dataset.py:Dataset:map,Dataset:map,method,47,275,176,3012,10.95,2,11,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'num_proc', 'suffix_template', 'new_fingerprint', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[List[str]] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[int] ', ' str ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' False', None, ' None', ' False', ' 1000', ' False', ' None', ' False', ' None', ' None', ' 1000', ' None', ' False', ' None', ' None', ' ""_{rank:05d}_of_{num_proc:05d}""', ' None', ' None', None]",1563,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does update examples).\n', '\n', '        Args:\n', '            function (:obj:`Callable`): Function with one of the following signatures:\n', '\n', '                - `function(example: Union[Dict, Any]) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Union[Dict, Any], indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Union[Dict[List], List[Any]]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Union[Dict[List], List[Any]], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n', '\n', '                If no function is provided, default to identity function: ``lambda x: x``.\n', '            with_indices (:obj:`bool`, default `False`): Provide example indices to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, default `None`): The columns to be passed into `function`\n', '                as positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (:obj:`bool`, default `False`): Provide batch of examples to `function`.\n', '            batch_size (`Optional[int]`, default `1000`): Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`.\n', '            drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`Optional[List[str]]`, default `None`): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True` if caching is enabled): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`Optional[str]`, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            features (`Optional[datasets.Features]`, default `None`): Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (:obj:`bool`, default `True`): Disallow null values in the table.\n', '            fn_kwargs (`Optional[Dict]`, default `None`): Keyword arguments to be passed to `function`.\n', ""            num_proc (`Optional[int]`, default `None`): Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            suffix_template (:obj:`str`):\n', '                If cache_file_name is specified, then this suffix\n', '                will be added at the end of the base name of each: defaults to ""_{rank:05d}_of_{num_proc:05d}"". For example, if cache_file_name is ""processed.arrow"", then for\n', '                rank=1 and num_proc=4, the resulting file would be ""processed_00001_of_00004.arrow"" for the default suffix.\n', '            new_fingerprint (`Optional[str]`, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '            desc (`Optional[str]`, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:_map_single,Dataset:_map_single,method,123,778,383,7193,9.25,6,46,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'new_fingerprint', 'rank', 'offset', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[List[str]] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[str] ', ' Optional[int] ', ' int ', ' Optional[str] ', None]","[None, ' None', ' False', None, ' None', ' False', ' 1000', ' False', ' None', ' False', ' None', ' None', ' 1000', ' None', ' False', ' None', ' None', ' None', ' 0', ' None', None]",1745,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does update examples).\n', '\n', '        Args:\n', '            function (:obj:`Callable`): with one of the following signature:\n', '                - `function(example: Union[Dict, Any]) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Union[Dict, Any], indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Union[Dict[List], List[Any]]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Union[Dict[List], List[Any]], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n', '                If no function is provided, default to identity function: lambda x: x\n', '            with_indices (:obj:`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (:obj:`bool`, defaults to `False`): Provide batch of examples to `function`\n', '            batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n', '            drop_last_batch (:obj:`bool`, default: `False`): Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (:obj:`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, defaults to `True` if caching is enabled): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`Optional[str]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (:obj:`bool`, defaults to `True`): Disallow null values in the table.\n', '            fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n', '            new_fingerprint (`Optional[str]`, defaults to `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '            rank: (`Optional[int]`, defaults to `None`): If specified, this is the process rank when doing multiprocessing\n', '            offset: (:obj:`int`, defaults to 0): If specified, this is an offset applied to the indices passed to `function` if `with_indices=True`\n', '            desc (`Optional[str]`, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:filter,Dataset:filter,method,28,107,83,1064,9.94,1,6,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batch_size', 'remove_columns', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'fn_kwargs', 'num_proc', 'suffix_template', 'new_fingerprint', '']","[None, ' Optional[Callable] ', None, ' Optional[Union[str', None, ' Optional[int] ', ' Optional[List[str]] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[dict] ', ' Optional[int] ', ' str ', ' Optional[str] ', None]","[None, ' None', 'False', None, ' None', ' 1000', ' None', ' False', ' True', ' None', ' 1000', ' None', ' None', ' ""_{rank:05d}_of_{num_proc:05d}""', ' None', None]",2064,"['        """"""Apply a filter function to all the elements in the table in batches\n', '        and update the table so that the dataset only includes examples according to the filter function.\n', '\n', '        Args:\n', '            function (:obj:`Callable`): Callable with one of the following signatures:\n', '\n', '                - ``function(example: Union[Dict, Any]) -> bool`` if ``with_indices=False``\n', '                - ``function(example: Union[Dict, Any], indices: int) -> bool`` if ``with_indices=True``\n', '\n', '                If no function is provided, defaults to an always True function: ``lambda x: True``.\n', '            with_indices (:obj:`bool`, default `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (:obj:`str` or `List[str]`, optional): The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batch_size (:obj:`int`, optional, default `1000`): Number of examples per batch provided to `function` if\n', '                ``batched = True``. If ``batch_size <= 0`` or ``batch_size == None``: provide the full dataset as a\n', '                single batch to `function`\n', '            remove_columns (`List[str]`, optional): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            fn_kwargs (:obj:`dict`, optional): Keyword arguments to be passed to `function`\n', ""            num_proc (:obj:`int`, optional): Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            suffix_template (:obj:`str`):\n', '                If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n', '                For example, if `cache_file_name` is `""processed.arrow""`, then for ``rank = 1`` and ``num_proc = 4``,\n', '                the resulting file would be `""processed_00001_of_00004.arrow""` for the default suffix (default\n', '                `_{rank:05d}_of_{num_proc:05d}`)\n', '            new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:flatten_indices,Dataset:flatten_indices,method,2,13,13,228,17.54,0,0,"['self', 'keep_in_memory', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'new_fingerprint', '']","[None, ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[str] ', None]","[None, ' False', ' None', ' 1000', ' None', ' True', ' None', None]",2162,"['        """"""Create and cache a new Dataset by flattening the indices mapping.\n', '\n', '        Args:\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            cache_file_name (`Optional[str]`, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            features (`Optional[datasets.Features]`, default `None`): Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (:obj:`bool`, default `True`): Allow null values in the table.\n', '            new_fingerprint (`Optional[str]`, default `None`): The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:_new_dataset_with_indices,Dataset:_new_dataset_with_indices,method,11,54,41,504,9.33,0,1,"['self', 'indices_cache_file_name', 'indices_buffer', 'fingerprint', '']","[None, ' Optional[str] ', ' Optional[pa.Buffer] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', None]",2197,"['        """"""Return a new Dataset obtained by adding indices (provided in indices_cache_file_name or in a buffer) to the\n', '        current Dataset.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:select,Dataset:select,method,29,164,112,1776,10.83,0,8,"['self', 'indices', 'keep_in_memory', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Iterable', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, ' False', ' None', ' 1000', ' None', None]",2229,"['        """"""Create a new dataset with rows selected following the list/array of indices.\n', '\n', '        Args:\n', '            indices (sequence, iterable, ndarray or Series): List or 1D-array of integer indices for indexing.\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the indices mapping in memory instead of writing it to a cache file.\n', '            indices_cache_file_name (`Optional[str]`, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                indices mapping instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            new_fingerprint (`Optional[str]`, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:sort,Dataset:sort,method,17,114,88,1181,10.36,0,7,"['self', 'column', 'reverse', 'kind', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' str', ' bool ', ' str ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, ' False', ' None', ' False', ' True', ' None', ' 1000', ' None', None]",2312,"['        """"""Create a new dataset sorted according to a column.\n', '\n', '        Currently sorting according to a column name uses numpy sorting algorithm under the hood.\n', '        The column should thus be a numpy compatible type (in particular not a nested type).\n', '        This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n', '\n', '        Args:\n', '            column (:obj:`str`): column name to sort by.\n', '            reverse (:obj:`bool`, default `False`): If True, sort by descending order rather then ascending.\n', '            kind (:obj:`str`, optional): Numpy algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n', '                The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n', '                the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the sorted indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the sorted indices\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_name (`Optional[str]`, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                sorted indices instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                Higher value gives smaller cache files, lower value consume less temporary memory.\n', '            new_fingerprint (`Optional[str]`, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:shuffle,Dataset:shuffle,method,21,133,96,1236,9.29,0,8,"['self', 'seed', 'generator', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Optional[int] ', ' Optional[np.random.Generator] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' None', ' None', ' False', ' True', ' None', ' 1000', ' None', None]",2392,"['        """"""Create a new Dataset where the rows are shuffled.\n', '\n', '        Currently shuffling uses numpy random generators.\n', ""        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n"", '\n', '        Args:\n', '            seed (:obj:`int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n', '                If None, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '            generator (:obj:`numpy.random.Generator`, optional): Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the shuffled indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the shuffled indices\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                shuffled indices instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            new_fingerprint (:obj:`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:train_test_split,Dataset:train_test_split,method,47,443,210,3949,8.91,0,21,"['self', 'test_size', 'int', 'None] ', 'train_size', 'int', 'None] ', 'shuffle', 'seed', 'generator', 'keep_in_memory', 'load_from_cache_file', 'train_indices_cache_file_name', 'test_indices_cache_file_name', 'writer_batch_size', 'train_new_fingerprint', 'test_new_fingerprint', '']","[None, ' Union[float', None, None, ' Union[float', None, None, ' bool ', ' Optional[int] ', ' Optional[np.random.Generator] ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, ' None', None, None, ' None', ' True', ' None', ' None', ' False', ' True', ' None', ' None', ' 1000', ' None', ' None', None]",2473,"['        """"""Return a dictionary (:obj:`datasets.DatsetDict`) with two random train and test subsets (`train` and `test` ``Dataset`` splits).\n', '        Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n', '\n', '        This method is similar to scikit-learn `train_test_split` with the omission of the stratified options.\n', '\n', '        Args:\n', '            test_size (:obj:`numpy.random.Generator`, optional): Size of the test split\n', '                If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n', '                If int, represents the absolute number of test samples.\n', '                If None, the value is set to the complement of the train size.\n', '                If train_size is also None, it will be set to 0.25.\n', '            train_size (:obj:`numpy.random.Generator`, optional): Size of the train split\n', '                If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n', '                If int, represents the absolute number of train samples.\n', '                If None, the value is automatically set to the complement of the test size.\n', '            shuffle (:obj:`bool`, optional, default `True`): Whether or not to shuffle the data before splitting.\n', '            seed (:obj:`int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n', '                If None, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '            generator (:obj:`numpy.random.Generator`, optional): Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the splits indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the splits indices\n', '                can be identified, use it instead of recomputing.\n', '            train_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                train split indices instead of the automatically generated cache file name.\n', '            test_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                test split indices instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            train_new_fingerprint (:obj:`str`, optional, defaults to `None`): the new fingerprint of the train set after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '            test_new_fingerprint (:obj:`str`, optional, defaults to `None`): the new fingerprint of the test set after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:shard,Dataset:shard,method,12,51,40,425,8.33,0,1,"['self', 'num_shards', 'index', 'contiguous', 'keep_in_memory', 'indices_cache_file_name', 'writer_batch_size', '']","[None, ' int', ' int', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', None]","[None, None, None, ' False', ' False', ' None', ' 1000', None]",2667,"['        """"""Return the `index`-nth shard from dataset split into `num_shards` pieces.\n', '\n', '        This shards deterministically. dset.shard(n, i) will contain all elements of dset whose\n', '        index mod n = i.\n', '\n', '        dset.shard(n, i, contiguous=True) will instead split dset into contiguous chunks,\n', '        so it can be easily concatenated back together after processing. If n % i == l, then the\n', '        first l shards will have length (n // i) + 1, and the remaining shards will have length (n // i).\n', '        `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n', '        a dataset with the same order as the original.\n', '\n', '        Be sure to shard before using any randomizing operator (such as shuffle).\n', '        It is best if the shard operator is used early in the dataset pipeline.\n', '\n', '\n', '        Args:\n', '            num_shards (:obj:`int`): How many shards to split the dataset into.\n', '            index (:obj:`int`): Which shard to select and return.\n', '            contiguous: (:obj:`bool`, default `False`): Whether to select contiguous blocks of indices for shards.\n', '            keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                indices of each shard instead of the automatically generated cache file name.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:export,Dataset:export,method,45,202,143,2170,10.74,1,4,"['self', 'filename', 'format', '']","[None, ' str', ' str ', None]","[None, None, ' ""tfrecord""', None]",2721,"['        """"""Writes the Arrow dataset to a TFRecord file.\n', '\n', '        The dataset must already be in tensorflow format. The records will be written with\n', '        keys from `dataset._format_columns`.\n', '\n', '        Args:\n', '            filename (:obj:`str`): The filename, including the `.tfrecord` extension, to write to.\n', '            format (`str`, optional, default `""tfrecord""`): The type of output file. Currently this is a no-op, as\n', '                TFRecords are the only option. This enables a more flexible function signature later.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:to_csv,Dataset:to_csv,method,4,9,9,120,13.33,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', '**to_csv_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, ' None', None, None]",2803,"['        """"""Exports the dataset to csv\n', '\n', '        Args:\n', '            path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n', '            batch_size (Optional ``int``): Size of the batch to load in memory and write at once.\n', '                Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', ""            to_csv_kwargs: Parameters to pass to pandas's :func:`pandas.DataFrame.to_csv`\n"", '\n', '        Returns:\n', '            int: The number of characters or bytes written\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:to_dict,Dataset:to_dict,method,10,47,28,404,8.6,1,4,"['self', 'batch_size', 'batched']","[None, ' Optional[int] ', ' bool ']","[None, ' None', ' False']",2825,"['        """"""Returns the dataset as a Python dict. Can also return a generator for large datasets.\n', '\n', '        Args:\n', '            batched (``bool``): Set to :obj:`True` to return a generator that yields the dataset as batches\n', '                of ``batch_size`` rows. Defaults to :obj:`False` (returns the whole datasetas once)\n', '            bacth_size (Optional ``int``): The size (number of rows) of the batches if ``batched`` is `True`.\n', '                Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '\n', '        Returns:\n', '            `dict` or `Iterator[dict]`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:to_json,Dataset:to_json,method,4,9,9,124,13.78,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', '**to_json_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, ' None', None, None]",2854,"['        """"""Export the dataset to JSON Lines or JSON.\n', '\n', '        Args:\n', '            path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n', '            batch_size (:obj:`int`, optional): Size of the batch to load in memory and write at once.\n', '                Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            lines (:obj:`bool`, default ``True``): Whether output JSON lines format.\n', '                Only possible if ``orient=""records""`. It will throw ValueError with ``orient`` different from\n', '                ``""records""``, since the others are not list-like.\n', '            orient (:obj:`str`, default ``""records""``): Format of the JSON:\n', '\n', '                - ``""records""``: list like ``[{column -> value}, … , {column -> value}]``\n', '                - ``""split""``: dict like ``{""index"" -> [index], ""columns"" -> [columns], ""data"" -> [values]}``\n', '                - ``""index""``: dict like ``{index -> {column -> value}}``\n', '                - ``""columns""``: dict like ``{column -> {index -> value}}``\n', '                - ``""values""``: just the values array\n', '                - ``""table""``: dict like ``{""schema"": {schema}, ""data"": {data}}``\n', ""            **to_json_kwargs: Parameters to pass to pandas's `pandas.DataFrame.to_json\n"", '                <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html>`_.\n', '\n', '        Returns:\n', '            int: The number of characters or bytes written.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:to_pandas,Dataset:to_pandas,method,10,47,28,404,8.6,1,4,"['self', 'batch_size', 'batched']","[None, ' Optional[int] ', ' bool ']","[None, ' None', ' False']",2888,"['        """"""Returns the dataset as a :class:`pandas.DataFrame`. Can also return a generator for large datasets.\n', '\n', '        Args:\n', '            batched (``bool``): Set to :obj:`True` to return a generator that yields the dataset as batches\n', '                of ``batch_size`` rows. Defaults to :obj:`False` (returns the whole datasetas once)\n', '            bacth_size (Optional ``int``): The size (number of rows) of the batches if ``batched`` is `True`.\n', '                Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '\n', '        Returns:\n', '            `pandas.DataFrame` or `Iterator[pandas.DataFrame]`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:to_parquet,Dataset:to_parquet,method,4,9,9,140,15.56,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', '**parquet_writer_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, ' None', None, None]",2919,"['        """"""Exports the dataset to parquet\n', '\n', '        Args:\n', '            path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n', '            batch_size (Optional ``int``): Size of the batch to load in memory and write at once.\n', '                Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', ""            parquet_writer_kwargs: Parameters to pass to PyArrow's :class:`pyarrow.parquet.ParquetWriter`\n"", '\n', '        Returns:\n', '            int: The number of characters or bytes written\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:add_column,Dataset:add_column,method,10,19,18,377,19.84,0,0,"['self', 'name', 'column', 'np.array]', 'new_fingerprint']","[None, ' str', ' Union[list', None, ' str']","[None, None, None, None, None]",2943,"['        """"""Add column to Dataset.\n', '\n', '        .. versionadded:: 1.7\n', '\n', '        Args:\n', '            name (str): Column name.\n', '            column (list or np.array): Column data to be added.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:add_faiss_index,Dataset:add_faiss_index,method,13,45,40,602,13.38,0,0,"['self', 'column', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', '# noqa', 'faiss_verbose', 'dtype', '']","[None, ' str', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' F821train_size: Optional[int] ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', ' False', 'np.float32', None]",2964,[]
parser/test3/arrow_dataset.py:Dataset:add_faiss_index_from_external_arrays,Dataset:add_faiss_index_from_external_arrays,method,9,10,10,267,26.7,0,0,"['self', 'external_arrays', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', '# noqa', 'faiss_verbose', 'dtype', '']","[None, ' np.array', ' str', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' F821train_size: Optional[int] ', ' bool ', None, None]","[None, None, None, ' None', ' None', ' None', ' None', ' None', ' False', 'np.float32', None]",3037,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        The index is created using the vectors of `external_arrays`.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n', '        You can find more information about Faiss here:\n', '\n', '        - For `string factory <https://github.com/facebookresearch/faiss/wiki/The-index-factory>`__\n', '\n', '        Args:\n', '            external_arrays (:obj:`np.array`):\n', '                If you want to use arrays from outside the lib for the index, you can set :obj:`external_arrays`.\n', '                It will use :obj:`external_arrays` to create the Faiss index instead of the arrays in the given :obj:`column`.\n', '            index_name (:obj:`str`):\n', '                The index_name/identifier of the index.\n', '                This is the index_name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n', '            device (Optional :obj:`int`):\n', '                If not None, this is the index of the GPU to use.\n', '                By default it uses the CPU.\n', '            string_factory (Optional :obj:`str`):\n', '                This is passed to the index factory of Faiss to create the index.\n', '                Default index class is ``IndexFlat``.\n', '            metric_type (Optional :obj:`int`):\n', '                Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n', '            custom_index (Optional :obj:`faiss.Index`):\n', '                Custom Faiss index that you already have instantiated and configured for your needs.\n', '            train_size (Optional :obj:`int`):\n', '                If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (:obj:`bool`, defaults to False):\n', '                Enable the verbosity of the Faiss index.\n', '            dtype (:obj:`numpy.dtype`): The dtype of the numpy arrays that are indexed. Default is np.float32.\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:add_elasticsearch_index,Dataset:add_elasticsearch_index,method,0,2,2,3,1.5,0,0,"['self', 'column', 'index_name', 'host', 'port', 'es_client', '# noqa', 'es_index_config', '']","[None, ' str', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""elasticsearch.Elasticsearch""] ', ' F821es_index_name: Optional[str] ', ' Optional[dict] ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', None]",3090,"['        """"""Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n', '\n', '        Args:\n', '            column (:obj:`str`):\n', '                The column of the documents to add to the index.\n', '            index_name (Optional :obj:`str`):\n', '                The index_name/identifier of the index.\n', '                This is the index name that is used to call :meth:`Dataset.get_nearest_examples` or :meth:`Dataset.search`.\n', '                By default it corresponds to :obj:`column`.\n', '            host (Optional :obj:`str`, defaults to localhost):\n', '                host of where ElasticSearch is running\n', '            port (Optional :obj:`str`, defaults to 9200):\n', '                port of where ElasticSearch is running\n', '            es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n', '                The elasticsearch client used to create the index if host and port are None.\n', '            es_index_name (Optional :obj:`str`):\n', '                The elasticsearch index name used to create the index.\n', '            es_index_config (Optional :obj:`dict`):\n', '                The configuration of the elasticsearch index.\n', '                Default config is::\n', '\n', '                    {\n', '                        ""settings"": {\n', '                            ""number_of_shards"": 1,\n', '                            ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                        },\n', '                        ""mappings"": {\n', '                            ""properties"": {\n', '                                ""text"": {\n', '                                    ""type"": ""text"",\n', '                                    ""analyzer"": ""standard"",\n', '                                    ""similarity"": ""BM25""\n', '                                },\n', '                            }\n', '                        },\n', '                    }\n', '\n', '        Example:\n', '            .. code-block:: python\n', '\n', '                es_client = elasticsearch.Elasticsearch()\n', ""                ds = datasets.load_dataset('crime_and_punish', split='train')\n"", '                ds.add_elasticsearch_index(column=\'line\', es_client=es_client, es_index_name=""my_es_index"")\n', ""                scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n"", '\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:add_item,Dataset:add_item,method,18,42,37,587,13.98,0,1,"['self', 'item', 'new_fingerprint']","[None, ' dict', ' str']","[None, None, None]",3160,"['        """"""Add item to Dataset.\n', '\n', '        .. versionadded:: 1.7\n', '\n', '        Args:\n', '            item (dict): Item data to be added.\n', '\n', '        Returns:\n', '            :class:`Dataset`\n', '        """"""\n']"
parser/test3/arrow_dataset.py:Dataset:align_labels_with_mapping,Dataset:align_labels_with_mapping,method,16,42,35,569,13.55,1,0,"['self', 'label2id', 'label_column']","[None, ' Dict', ' str']","[None, None, None]",3191,"['        """"""Align the dataset\'s label ID and label name mapping to match an input :obj:`label2id` mapping.\n', ""        This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n"", '        The alignment in done using the lowercase label names.\n', '\n', '        Args:\n', '            label2id (:obj:`dict`):\n', '                The label name to ID mapping to align the dataset with.\n', '            label_column (:obj:`str`):\n', '                The column name of labels to align on.\n', '\n', '        Example:\n', '            .. code-block:: python\n', '\n', ""                # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n"", '                ds = load_dataset(""glue"", ""mnli"", split=""train"")\n', '                # mapping to align with\n', ""                label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n"", '                ds_aligned = ds.align_labels_with_mapping(label2id, ""label"")\n', '        """"""\n']"
parser/test3/arrow_reader.py:make_file_instructions,make_file_instructions,function,11,22,22,363,16.5,1,1,"['name', 'split_infos', 'instruction', 'filetype_suffix']","[None, None, None, None]","[None, None, None, 'None']",94,"['    """"""Returns instructions of the split dict.\n', '\n', '    Args:\n', '        name: Name of the dataset.\n', '        split_infos: `List[SplitInfo]`, Dataset splits information\n', '        instruction: `ReadInstruction` or `str`\n', ""        filetype_suffix: `Optional[str]` suffix of dataset files, e.g. 'arrow' or 'parquet'\n"", '\n', '    Returns:\n', '        file_intructions: FileInstructions instance\n', '    """"""\n']"
parser/test3/arrow_reader.py:_make_file_instructions_from_absolutes,_make_file_instructions_from_absolutes,function,16,50,42,562,11.24,1,2,"['name', 'name2len', 'absolute_instructions', 'filetype_suffix']","[None, None, None, None]","[None, None, None, 'None']",117,"['    """"""Returns the files instructions from the absolute instructions list.""""""\n']"
parser/test3/arrow_reader.py:_str_to_read_instruction,_str_to_read_instruction,function,11,36,30,367,10.19,0,2,['spec'],[None],[None],393,"['    """"""Returns ReadInstruction for given string.""""""\n']"
parser/test3/arrow_reader.py:_pct_to_abs_pct1,_pct_to_abs_pct1,function,6,34,34,212,6.24,0,1,"['boundary', 'num_examples']","[None, None]","[None, None]",408,[]
parser/test3/arrow_reader.py:_pct_to_abs_closest,_pct_to_abs_closest,function,1,4,4,45,11.25,0,0,"['boundary', 'num_examples']","[None, None]","[None, None]",419,[]
parser/test3/arrow_reader.py:_rel_to_abs_instr,_rel_to_abs_instr,function,23,122,63,803,6.58,0,10,"['rel_instr', 'name2len']","[None, None]","[None, None]",423,"['    """"""Returns _AbsoluteInstruction instance for given RelativeInstruction.\n', '\n', '    Args:\n', '        rel_instr: RelativeInstruction instance.\n', '        name2len: dict {split_name: num_examples}.\n', '    """"""\n']"
parser/test3/arrow_reader.py:MissingFilesOnHfGcs,MissingFilesOnHfGcs,class,0,1,1,4,4.0,0,0,[],[],[],73,[]
parser/test3/arrow_reader.py:FileInstructions,FileInstructions,class,3,4,4,45,11.25,0,0,[],[],[],80,[]
parser/test3/arrow_reader.py:BaseReader,BaseReader,class,62,267,191,3040,11.39,5,5,[],[],[],138,[]
parser/test3/arrow_reader.py:ArrowReader,ArrowReader,class,19,75,52,695,9.27,0,2,[],[],[],285,[]
parser/test3/arrow_reader.py:ParquetReader,ParquetReader,class,13,59,42,531,9.0,0,1,[],[],[],330,[]
parser/test3/arrow_reader.py:_AbsoluteInstruction,_AbsoluteInstruction,class,5,14,11,70,5.0,0,0,[],[],[],362,[]
parser/test3/arrow_reader.py:_RelativeInstruction,_RelativeInstruction,class,18,131,62,802,6.12,0,3,[],[],[],371,[]
parser/test3/arrow_reader.py:ReadInstruction,ReadInstruction,class,49,224,139,2122,9.47,1,5,[],[],[],457,[]
parser/test3/arrow_reader.py:BaseReader:__init__,BaseReader:__init__,method,7,9,9,100,11.11,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",143,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']"
parser/test3/arrow_reader.py:BaseReader:_get_table_from_filename,BaseReader:_get_table_from_filename,method,1,2,2,24,12.0,0,0,"['self', 'filename_skip_take', 'in_memory']","[None, None, None]","[None, None, 'False']",154,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']"
parser/test3/arrow_reader.py:BaseReader:_read_files,BaseReader:_read_files,method,17,94,71,716,7.62,3,2,"['self', 'files', 'in_memory']","[None, None, None]","[None, None, 'False']",158,"['        """"""Returns Dataset for given file instructions.\n', '\n', '        Args:\n', '            files: List[dict(filename, skip, take)], the files information.\n', '                The filenames contain the absolute path, not relative.\n', '                skip/take indicates which example read in the file: `ds.slice(skip, take)`\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '        """"""\n']"
parser/test3/arrow_reader.py:BaseReader:get_file_instructions,BaseReader:get_file_instructions,method,5,11,10,164,14.91,0,0,"['self', 'name', 'instruction', 'split_infos']","[None, None, None, None]","[None, None, None, None]",184,"['        """"""Return list of dict {\'filename\': str, \'skip\': int, \'take\': int}""""""\n']"
parser/test3/arrow_reader.py:BaseReader:read,BaseReader:read,method,7,33,33,367,11.12,0,1,"['self', 'name', 'instructions', 'split_infos', 'in_memory', '']","[None, None, None, None, None, None]","[None, None, None, None, 'False', None]",192,[]
parser/test3/arrow_reader.py:BaseReader:read_files,BaseReader:read_files,method,10,25,23,262,10.48,0,1,"['self', 'files', 'original_instructions', '""ReadInstruction""', '""Split""] ', 'in_memory', '']","[None, ' List[dict]', ' Union[None', None, None, None, None]","[None, None, None, None, ' None', 'False', None]",219,"['        """"""Returns single Dataset instance for the set of file instructions.\n', '\n', '        Args:\n', '            files: List[dict(filename, skip, take)], the files information.\n', '                The filenames contains the relative path, not absolute.\n', '                skip/take indicates which example read in the file: `ds.skip().take()`\n', '            original_instructions: store the original instructions used to build the dataset split in the dataset.\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            kwargs to build a Dataset instance.\n', '        """"""\n']"
parser/test3/arrow_reader.py:BaseReader:download_from_hf_gcs,BaseReader:download_from_hf_gcs,method,20,56,47,1004,17.93,2,1,"['self', 'download_config', 'relative_data_dir']","[None, ' DownloadConfig', None]","[None, None, None]",249,"['        """"""\n', '        Download the dataset files from the Hf GCS\n', '\n', '        Args:\n', '            dl_cache_dir: `str`, the local cache directory used to download files\n', '            relative_data_dir: `str`, the relative directory of the remote files from\n', '                the `datasets` directory on GCS.\n', '\n', '        """"""\n']"
parser/test3/arrow_reader.py:ArrowReader:__init__,ArrowReader:__init__,method,2,4,4,57,14.25,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",143,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']"
parser/test3/arrow_reader.py:ArrowReader:_get_table_from_filename,ArrowReader:_get_table_from_filename,method,9,45,29,349,7.76,0,1,"['self', 'filename_skip_take', 'in_memory']","[None, None, None]","[None, None, 'False']",154,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']"
parser/test3/arrow_reader.py:ArrowReader:read_table,ArrowReader:read_table,method,6,8,8,91,11.38,0,1,"['filename', 'in_memory']","[None, None]","[None, 'False']",315,"['        """"""\n', '        Read table from file.\n', '\n', '        Args:\n', '            filename (str): File name of the table.\n', '            in_memory (bool, default=False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            pyarrow.Table\n', '        """"""\n']"
parser/test3/arrow_reader.py:ParquetReader:__init__,ParquetReader:__init__,method,2,4,4,59,14.75,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",143,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']"
parser/test3/arrow_reader.py:ParquetReader:_get_table_from_filename,ParquetReader:_get_table_from_filename,method,9,45,29,351,7.8,0,1,"['self', 'filename_skip_take', '**kwargs']","[None, None, None]","[None, None, None]",346,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']"
parser/test3/arrow_reader.py:_RelativeInstruction:__post_init__,_RelativeInstruction:__post_init__,method,9,95,42,581,6.12,0,3,['self'],[None],[None],380,[]
parser/test3/arrow_reader.py:ReadInstruction:_init,ReadInstruction:_init,method,2,2,2,49,24.5,0,0,"['self', 'relative_instructions']","[None, None]","[None, None]",498,[]
parser/test3/arrow_reader.py:ReadInstruction:_read_instruction_from_relative_instructions,ReadInstruction:_read_instruction_from_relative_instructions,method,4,8,7,104,13.0,0,0,"['cls', 'relative_instructions']","[None, None]","[None, None]",503,"['        """"""Returns ReadInstruction obj initialized with relative_instructions.""""""\n']"
parser/test3/arrow_reader.py:ReadInstruction:__init__,ReadInstruction:__init__,method,1,5,5,69,13.8,0,0,"['self', 'split_name', 'rounding', 'from_', 'to', 'unit']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",510,"['        """"""Initialize ReadInstruction.\n', '\n', '        Args:\n', ""            split_name (str): name of the split to read. Eg: 'train'.\n"", '            rounding (str, optional): The rounding behaviour to use when percent slicing is\n', '                used. Ignored when slicing with absolute indices.\n', '                Possible values:\n', ""                 - 'closest' (default): The specified percentages are rounded to the\n"", '                     closest value. Use this if you want specified percents to be as\n', '                     much exact as possible.\n', ""                 - 'pct1_dropremainder': the specified percentages are treated as\n"", '                     multiple of 1%. Use this option if you want consistency. Eg:\n', '                         len(5%) == 5 * len(1%).\n', '                     Using this option, one might not be able to use the full set of\n', '                     examples, if the number of those is not a multiple of 100.\n', '            from_ (int):\n', '            to (int): alternative way of specifying slicing boundaries. If any of\n', '                {from_, to, unit} argument is used, slicing cannot be specified as\n', '                string.\n', '            unit (str): optional, one of:\n', ""                '%': to set the slicing unit as percents of the split size.\n"", ""                'abs': to set the slicing unit as absolute numbers.\n"", '        """"""\n']"
parser/test3/arrow_reader.py:ReadInstruction:from_spec,ReadInstruction:from_spec,method,9,38,35,284,7.47,0,1,"['cls', 'spec']","[None, None]","[None, None]",540,"['        """"""Creates a ReadInstruction instance out of a string spec.\n', '\n', '        Args:\n', '            spec (str): split(s) + optional slice(s) to read + optional rounding\n', '                        if percents are used as the slicing unit. A slice can be specified,\n', '                        using absolute numbers (int) or percentages (int). E.g.\n', '                            `test`: test split.\n', '                            `test + validation`: test split + validation split.\n', '                            `test[10:]`: test split, minus its first 10 records.\n', '                            `test[:10%]`: first 10% records of test split.\n', '                            `test[:20%](pct1_dropremainder)`: first 10% records, rounded with\n', '                                                              the `pct1_dropremainder` rounding.\n', '                            `test[:-5%]+train[40%:60%]`: first 95% of test + middle 20% of\n', '                                                         train.\n', '\n', '        Returns:\n', '            ReadInstruction instance.\n', '        """"""\n']"
parser/test3/arrow_reader.py:ReadInstruction:to_spec,ReadInstruction:to_spec,method,19,79,42,569,7.2,1,2,['self'],[None],[None],566,[]
parser/test3/arrow_reader.py:ReadInstruction:__add__,ReadInstruction:__add__,method,11,49,43,531,10.84,0,2,"['self', 'other']","[None, None]","[None, None]",586,"['        """"""Returns a new ReadInstruction obj, result of appending other to self.""""""\n']"
parser/test3/arrow_reader.py:ReadInstruction:__str__,ReadInstruction:__str__,method,2,2,2,20,10.0,0,0,['self'],[None],[None],601,[]
parser/test3/arrow_reader.py:ReadInstruction:__repr__,ReadInstruction:__repr__,method,1,4,4,55,13.75,0,0,['self'],[None],[None],604,[]
parser/test3/arrow_reader.py:ReadInstruction:to_absolute,ReadInstruction:to_absolute,method,1,7,7,86,12.29,0,0,"['self', 'name2len']","[None, None]","[None, None]",607,"['        """"""Translate instruction into a list of absolute instructions.\n', '\n', '        Those absolute instructions are then to be added together.\n', '\n', '        Args:\n', '            name2len: dict associating split names to number of examples.\n', '\n', '        Returns:\n', '            list of _AbsoluteInstruction instances (corresponds to the + in spec).\n', '        """"""\n']"
parser/test3/arrow_writer.py:parquet_to_arrow,parquet_to_arrow,function,25,46,41,556,12.09,3,1,"['sources', 'destination']","[None, None]","[None, None]",536,"['    """"""Convert parquet files to arrow file. Inputs can be str paths or file-like objects""""""\n']"
parser/test3/arrow_writer.py:TypedSequence,TypedSequence,class,36,217,121,1790,8.25,0,9,[],[],[],39,[]
parser/test3/arrow_writer.py:OptimizedTypedSequence,OptimizedTypedSequence,class,10,59,54,520,8.81,0,1,[],[],[],136,[]
parser/test3/arrow_writer.py:ArrowWriter,ArrowWriter,class,152,757,360,7636,10.09,6,36,[],[],[],149,[]
parser/test3/arrow_writer.py:BeamWriter,BeamWriter,class,82,268,194,2889,10.78,2,5,[],[],[],436,[]
parser/test3/arrow_writer.py:TypedSequence:__init__,TypedSequence:__init__,method,13,32,27,199,6.22,0,1,"['self', 'data', 'type', 'try_type', 'optimized_int_type']","[None, None, None, None, None]","[None, None, 'None', 'None', 'None']",79,[]
parser/test3/arrow_writer.py:TypedSequence:__arrow_array__,TypedSequence:__arrow_array__,method,26,176,100,1482,8.42,0,8,"['self', 'type']","[None, None]","[None, 'None']",86,"['        """"""This function is called when calling pa.array(typed_sequence)""""""\n']"
parser/test3/arrow_writer.py:OptimizedTypedSequence:__init__,OptimizedTypedSequence:__init__,method,9,52,47,439,8.44,0,1,"['self', 'data', 'type', 'try_type', 'col', 'optimized_int_type']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",137,[]
parser/test3/arrow_writer.py:ArrowWriter:__init__,ArrowWriter:__init__,method,45,117,80,1172,10.02,0,5,"['self', 'schema', 'features', 'path', 'stream', 'fingerprint', 'writer_batch_size', 'hash_salt', 'check_duplicates', 'disable_nullable', 'update_features', 'with_metadata', 'unit', '']","[None, ' Optional[pa.Schema] ', ' Optional[Features] ', ' Optional[str] ', ' Optional[pa.NativeFile] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[bool] ', ' bool ', ' bool ', ' bool ', ' str ', None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' False', ' False', ' False', ' True', ' ""examples""', None]",152,[]
parser/test3/arrow_writer.py:ArrowWriter:__len__,ArrowWriter:__len__,method,2,4,4,74,18.5,0,0,['self'],[None],[None],212,"['        """"""Return the number of writed and staged examples""""""\n']"
parser/test3/arrow_writer.py:ArrowWriter:__enter__,ArrowWriter:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],216,[]
parser/test3/arrow_writer.py:ArrowWriter:__exit__,ArrowWriter:__exit__,method,1,1,1,12,12.0,0,0,"['self', 'exc_type', 'exc_val', 'exc_tb']","[None, None, None, None]","[None, None, None, None]",219,[]
parser/test3/arrow_writer.py:ArrowWriter:close,ArrowWriter:close,method,11,30,25,224,7.47,0,2,['self'],[None],[None],222,[]
parser/test3/arrow_writer.py:ArrowWriter:_build_writer,ArrowWriter:_build_writer,method,32,72,51,850,11.81,2,6,"['self', 'inferred_schema']","[None, ' pa.Schema']","[None, None]",232,[]
parser/test3/arrow_writer.py:ArrowWriter:schema,ArrowWriter:schema,method,2,9,8,47,5.22,0,1,['self'],[None],[None],256,[]
parser/test3/arrow_writer.py:ArrowWriter:_build_metadata,ArrowWriter:_build_metadata,method,8,35,33,270,7.71,0,1,"['info', 'fingerprint']","[' DatasetInfo', ' Optional[str] ']","[None, ' None']",260,[]
parser/test3/arrow_writer.py:ArrowWriter:write_examples_on_file,ArrowWriter:write_examples_on_file,method,34,150,88,1321,8.81,1,5,['self'],[None],[None],269,"['        """"""Write stored examples from the write-pool of examples. It makes a table out of the examples and write it.""""""\n']"
parser/test3/arrow_writer.py:ArrowWriter:write_rows_on_file,ArrowWriter:write_rows_on_file,method,5,9,9,134,14.89,0,1,['self'],[None],[None],308,"['        """"""Write stored rows from the write-pool of rows. It concatenates the single-row tables and it writes the resulting table.""""""\n']"
parser/test3/arrow_writer.py:ArrowWriter:write,ArrowWriter:write,method,38,175,105,1603,9.16,1,6,['self'],[None],[None],316,[]
parser/test3/arrow_writer.py:ArrowWriter:check_duplicate_keys,ArrowWriter:check_duplicate_keys,method,7,15,14,124,8.27,1,1,['self'],[None],[None],349,"['        """"""Raises error if duplicates found in a batch""""""\n']"
parser/test3/arrow_writer.py:ArrowWriter:write_row,ArrowWriter:write_row,method,5,9,9,134,14.89,0,1,['self'],[None],[None],358,"['        """"""Add a given single-row Table to the write-pool of rows which is written to file.\n', '\n', '        Args:\n', '            row: the row to add.\n', '        """"""\n']"
parser/test3/arrow_writer.py:ArrowWriter:write_batch,ArrowWriter:write_batch,method,19,59,35,612,10.37,1,2,"['self', 'batch_examples', 'List[Any]]', 'writer_batch_size', '']","[None, ' Dict[str', None, ' Optional[int] ', None]","[None, None, None, ' None', None]",370,"['        """"""Write a batch of Example to file.\n', '\n', '        Args:\n', '            example: the Example to add.\n', '        """"""\n']"
parser/test3/arrow_writer.py:ArrowWriter:write_table,ArrowWriter:write_table,method,15,36,26,463,12.86,1,2,"['self', 'pa_table', 'writer_batch_size']","[None, ' pa.Table', ' Optional[int] ']","[None, None, ' None']",391,"['        """"""Write a Table to file.\n', '\n', '        Args:\n', '            example: the Table to add.\n', '        """"""\n']"
parser/test3/arrow_writer.py:ArrowWriter:finalize,ArrowWriter:finalize,method,16,55,45,516,9.38,0,4,"['self', 'close_stream']","[None, None]","[None, 'True']",410,[]
parser/test3/arrow_writer.py:BeamWriter:__init__,BeamWriter:__init__,method,21,63,43,536,8.51,0,3,"['self', 'features', 'schema', 'path', 'namespace', 'cache_dir', '']","[None, ' Optional[Features] ', ' Optional[pa.Schema] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' None', ' None', None]",152,[]
parser/test3/arrow_writer.py:BeamWriter:write_from_pcollection,BeamWriter:write_from_pcollection,method,14,54,46,518,9.59,0,0,"['self', 'pcoll_examples']","[None, None]","[None, None]",468,"['        """"""Add the final steps of the beam pipeline: write to parquet files.""""""\n']"
parser/test3/arrow_writer.py:BeamWriter:finalize,BeamWriter:finalize,method,50,125,103,1566,12.53,2,2,"['self', 'metrics_query_result']","[None, ' dict']","[None, None]",490,"['        """"""\n', '        Run after the pipeline has finished.\n', '        It converts the resulting parquet files to arrow and it completes the info from the pipeline metrics.\n', '\n', '        Args:\n', '            metrics_query_result: `dict` obtained from pipeline_results.metrics().query(m_filter). Make sure\n', '                that the filter keeps only the metrics for the considered split, under the namespace `split_name`.\n', '        """"""\n']"
parser/test3/builder.py:InvalidConfigName,InvalidConfigName,class,0,1,1,4,4.0,0,0,[],[],[],53,[]
parser/test3/builder.py:BuilderConfig,BuilderConfig,class,44,243,160,2411,9.92,5,12,[],[],[],58,[]
parser/test3/builder.py:DatasetBuilder,DatasetBuilder,class,298,1658,868,19732,11.9,16,65,[],[],[],173,[]
parser/test3/builder.py:GeneratorBasedBuilder,GeneratorBasedBuilder,class,36,80,76,1287,16.09,1,0,[],[],[],995,[]
parser/test3/builder.py:ArrowBasedBuilder,ArrowBasedBuilder,class,26,61,58,936,15.34,1,1,[],[],[],1086,[]
parser/test3/builder.py:MissingBeamOptions,MissingBeamOptions,class,0,1,1,4,4.0,0,0,[],[],[],1144,[]
parser/test3/builder.py:BeamBasedBuilder,BeamBasedBuilder,class,86,294,222,3595,12.23,1,3,[],[],[],1148,[]
parser/test3/builder.py:BuilderConfig:__post_init__,BuilderConfig:__post_init__,method,4,41,37,312,7.61,1,1,['self'],[None],[None],78,[]
parser/test3/builder.py:BuilderConfig:__eq__,BuilderConfig:__eq__,method,2,16,14,139,8.69,0,1,"['self', 'o']","[None, None]","[None, None]",91,[]
parser/test3/builder.py:BuilderConfig:create_config_id,BuilderConfig:create_config_id,method,29,153,95,1642,10.73,4,10,"['self', 'config_kwargs', 'custom_features']","[None, ' dict', ' Optional[Features] ']","[None, None, ' None']",98,"['        """"""\n', '        The config id is used to build the cache directory.\n', '        By default it is equal to the config name.\n', '        However the name of a config is not sufficent to have a unique identifier for the dataset being generated since\n', ""        it doesn't take into account:\n"", '        - the config kwargs that can be used to overwrite attributes\n', '        - the custom features used to write the dataset\n', '        - the data_files for json/text/csv/pandas datasets\n', '        Therefore the config id is just the config name with an optional suffix based on these.\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:__init__,DatasetBuilder:__init__,method,55,144,109,1556,10.81,1,5,"['self', 'cache_dir', 'name', 'hash', 'base_path', 'features', '**config_kwargs', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[Features] ', None, None]","[None, ' None', ' None', ' None', ' None', ' None', None, None]",202,"['        """"""Constructs a DatasetBuilder.\n', '\n', '        Callers must pass arguments as keyword arguments.\n', '\n', '        Args:\n', '            cache_dir: `str`, directory to read/write data. Defaults to ""~/datasets"".\n', '            name: `str` name, optional configuration for the dataset that affects the data generated on disk. Different\n', '                `builder_config`s will have their own subdirectories and versions.\n', '                If not provided, uses the first configuration in self.BUILDER_CONFIGS\n', '            hash: a hash specific to the dataset code. Used to update the caching directory when the dataset loading\n', '                script code is updated (to avoid reusing old data).\n', '                The typical caching directory (defined in ``self._relative_data_dir``) is: ``name/version/hash/``\n', '            base_path: `str`, base path for relative paths that are used to download files. This can be a remote url.\n', '            features: `Features`, optional features that will be used to read/write the dataset\n', '                It can be used to changed the :obj:`datasets.Features` description of a dataset for example.\n', '            config_kwargs: will override the defaults kwargs in config\n', '\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:manual_download_instructions,DatasetBuilder:manual_download_instructions,method,1,2,2,10,5.0,0,0,['self'],[None],[None],288,[]
parser/test3/builder.py:DatasetBuilder:get_all_exported_dataset_infos,DatasetBuilder:get_all_exported_dataset_infos,method,5,9,8,216,24.0,0,1,['cls'],[None],[None],292,"['        """"""Empty dict if doesn\'t exist""""""\n']"
parser/test3/builder.py:DatasetBuilder:get_exported_dataset_info,DatasetBuilder:get_exported_dataset_info,method,2,3,3,79,26.33,0,0,['self'],[None],[None],299,"['        """"""Empty DatasetInfo if doesn\'t exist""""""\n']"
parser/test3/builder.py:DatasetBuilder:_create_builder_config,DatasetBuilder:_create_builder_config,method,32,212,124,2055,9.69,1,14,"['self', 'name', 'custom_features', '**config_kwargs']","[None, None, None, None]","[None, 'None', 'None', None]",303,"['        """"""Create and validate BuilderConfig object as well as a unique config id for this config.\n', '        Raises ValueError if there are multiple builder configs and name and DEFAULT_CONFIG_NAME are None.\n', '        config_kwargs override the defaults kwargs in config\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:builder_configs,DatasetBuilder:builder_configs,method,7,30,24,249,8.3,2,1,['cls'],[None],[None],376,"['        """"""Pre-defined list of configurations for this builder class.""""""\n']"
parser/test3/builder.py:DatasetBuilder:cache_dir,DatasetBuilder:cache_dir,method,2,2,2,21,10.5,0,0,['self'],[None],[None],385,[]
parser/test3/builder.py:DatasetBuilder:_relative_data_dir,DatasetBuilder:_relative_data_dir,method,12,28,18,357,12.75,0,3,"['self', 'with_version', 'with_hash']","[None, None, None]","[None, 'True', 'True']",388,"['        """"""Relative path of this dataset in cache_dir:\n', '        Will be:\n', '            self.name/self.config.version/self.hash/\n', '        If any of these element is missing or if ``with_version=False`` the corresponding subfolders are dropped.\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_build_cache_dir,DatasetBuilder:_build_cache_dir,method,23,73,60,958,13.12,1,3,['self'],[None],[None],406,"['        """"""Return the data directory for the current version.""""""\n']"
parser/test3/builder.py:DatasetBuilder:_info,DatasetBuilder:_info,method,1,2,2,24,12.0,0,0,['self'],[None],[None],445,"['        """"""Construct the DatasetInfo object. See `DatasetInfo` for details.\n', '\n', '        Warning: This function is only called once and the result is cached for all\n', '        following .info() calls.\n', '\n', '        Returns:\n', '            info: (DatasetInfo) The dataset information\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:get_imported_module_dir,DatasetBuilder:get_imported_module_dir,method,2,2,2,62,31.0,0,0,['cls'],[None],[None],457,"['        """"""Return the path of the module of this class or subclass.""""""\n']"
parser/test3/builder.py:DatasetBuilder:download_and_prepare,DatasetBuilder:download_and_prepare,method,62,303,211,3616,11.93,0,12,"['self', 'download_config', 'download_mode', 'ignore_verifications', 'try_from_hf_gcs', 'dl_manager', 'base_path', 'use_auth_token', 'str]] ', '**download_and_prepare_kwargs', '']","[None, ' Optional[DownloadConfig] ', ' Optional[GenerateMode] ', ' bool ', ' bool ', ' Optional[DownloadManager] ', ' Optional[str] ', ' Optional[Union[bool', None, None, None]","[None, ' None', ' None', ' False', ' True', ' None', ' None', None, ' None', None, None]",461,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        Args:\n', '            download_config (Optional ``datasets.DownloadConfig``: specific download configuration parameters.\n', '            download_mode (Optional `datasets.GenerateMode`): select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS\n', '            ignore_verifications (bool): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...)\n', '            save_infos (bool): Save the dataset information (checksums/size/splits/...)\n', '            try_from_hf_gcs (bool): If True, it will try to download the already prepared dataset from the Hf google cloud storage\n', '            dl_manager (Optional ``datasets.DownloadManager``): specific Download Manger to use\n', '            base_path ( Optional ``str``): base path for relative paths that are used to download files. This can be a remote url.\n', '                If not specified, the value of the ``base_path`` attribute (``self.base_path``) will be used instead.\n', '            use_auth_token (Optional ``Union[str, bool]``): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '                If True, will get token from ~/.huggingface.\n', '\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_download_prepared_from_hf_gcs,DatasetBuilder:_download_prepared_from_hf_gcs,method,22,63,55,895,14.21,2,1,"['self', 'download_config']","[None, ' DownloadConfig']","[None, None]",601,[]
parser/test3/builder.py:DatasetBuilder:_download_and_prepare,DatasetBuilder:_download_and_prepare,method,19,83,72,993,11.96,1,3,"['self', 'dl_manager', 'verify_infos', '**prepare_split_kwargs']","[None, None, None, None]","[None, None, None, None]",623,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `download_and_prepare`. It should download all required data and generate\n', '        the pre-processed datasets files.\n', '\n', '        Args:\n', '            dl_manager: (DownloadManager) `DownloadManager` used to download and cache\n', '                data.\n', '            verify_infos: bool, if False, do not perform checksums and size tests.\n', '            prepare_split_kwargs: Additional options.\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:download_post_processing_resources,DatasetBuilder:download_post_processing_resources,method,14,46,37,605,13.15,2,3,"['self', 'dl_manager']","[None, None]","[None, None]",677,[]
parser/test3/builder.py:DatasetBuilder:_load_info,DatasetBuilder:_load_info,method,2,2,2,49,24.5,0,0,['self'],[None],[None],693,[]
parser/test3/builder.py:DatasetBuilder:_save_info,DatasetBuilder:_save_info,method,7,19,12,395,20.79,0,0,['self'],[None],[None],696,[]
parser/test3/builder.py:DatasetBuilder:_save_infos,DatasetBuilder:_save_infos,method,5,9,9,213,23.67,0,0,['self'],[None],[None],701,[]
parser/test3/builder.py:DatasetBuilder:_make_split_generators_kwargs,DatasetBuilder:_make_split_generators_kwargs,method,3,4,4,32,8.0,0,0,"['self', 'prepare_split_kwargs']","[None, None]","[None, None]",706,"['        """"""Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.""""""\n']"
parser/test3/builder.py:DatasetBuilder:as_dataset,DatasetBuilder:as_dataset,method,13,82,63,690,8.41,1,3,"['self', 'split', 'run_post_process', 'ignore_verifications', 'in_memory']","[None, ' Optional[Split] ', None, None, None]","[None, ' None', 'True', 'False', 'False']",711,"['        """"""Return a Dataset for the specified split.\n', '\n', '        Args:\n', '            split (`datasets.Split`): Which subset of the data to return.\n', '            run_post_process (bool, default=True): Whether to run post-processing dataset transforms and/or add\n', '                indexes.\n', '            ignore_verifications (bool, default=False): Whether to ignore the verifications of the\n', '                downloaded/processed dataset information (checksums/size/splits/...).\n', '            in_memory (bool, default=False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            datasets.Dataset\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_build_single_dataset,DatasetBuilder:_build_single_dataset,method,54,174,112,2335,13.42,4,12,"['self', 'split', 'ReadInstruction', 'Split]', 'run_post_process', 'ignore_verifications', 'in_memory', '']","[None, ' Union[str', None, None, ' bool', ' bool', ' bool ', None]","[None, None, None, None, None, None, ' False', None]",760,"['        """"""as_dataset for a single split.""""""\n']"
parser/test3/builder.py:DatasetBuilder:_as_dataset,DatasetBuilder:_as_dataset,method,10,13,13,262,20.15,0,0,"['self', 'split', 'Split] ', 'in_memory']","[None, ' Union[ReadInstruction', None, ' bool ']","[None, None, ' Split.TRAIN', ' False']",836,"['        """"""Constructs a `Dataset`.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `as_dataset`. It should read the pre-processed datasets files and generate\n', '        the `Dataset` object.\n', '\n', '        Args:\n', '            split: `datasets.Split` which subset of the data to read.\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            `Dataset`\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_get_dataset_fingerprint,DatasetBuilder:_get_dataset_fingerprint,method,6,16,15,222,13.88,0,0,"['self', 'split', 'Split]']","[None, ' Union[ReadInstruction', None]","[None, None, None]",860,"['        """"""The dataset fingerprint is the hash of the relative directory dataset_name/config_name/version/hash, as well as the split specs.""""""\n']"
parser/test3/builder.py:DatasetBuilder:as_streaming_dataset,DatasetBuilder:as_streaming_dataset,method,20,93,72,1021,10.98,1,4,"['self', 'split', 'base_path', 'use_auth_token', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', None]",868,[]
parser/test3/builder.py:DatasetBuilder:_as_streaming_dataset_single,DatasetBuilder:_as_streaming_dataset_single,method,4,6,6,145,24.17,0,0,"['self', 'splits_generator', '']","[None, None, None]","[None, None, None]",909,[]
parser/test3/builder.py:DatasetBuilder:_post_process,DatasetBuilder:_post_process,method,2,11,10,81,7.36,0,0,"['self', 'dataset', 'resources_paths', 'str]']","[None, ' Dataset', ' Dict[str', None]","[None, None, None, None]",916,"['        """"""Run dataset transforms or add indexes""""""\n']"
parser/test3/builder.py:DatasetBuilder:_post_processing_resources,DatasetBuilder:_post_processing_resources,method,1,2,2,8,4.0,0,0,"['self', 'split']","[None, ' str']","[None, None]",920,"['        """"""Mapping resource_name -> resource_file_name""""""\n']"
parser/test3/builder.py:DatasetBuilder:_download_post_processing_resources,DatasetBuilder:_download_post_processing_resources,method,4,23,16,332,14.43,0,0,"['self', 'split', 'resource_name', 'dl_manager']","[None, ' str', ' str', ' DownloadManager']","[None, None, None, None]",924,"['        """"""Download the resource using the download manager and return the downloaded path""""""\n']"
parser/test3/builder.py:DatasetBuilder:_split_generators,DatasetBuilder:_split_generators,method,1,2,2,26,13.0,0,0,"['self', 'dl_manager']","[None, ' DownloadManager']","[None, None]",930,"['        """"""Specify feature dictionary generators and dataset splits.\n', '\n', '        This function returns a list of `SplitGenerator`s defining how to generate\n', '        data and what splits to use.\n', '\n', '        Example:\n', '\n', '            return [\n', '                    datasets.SplitGenerator(\n', '                            name=datasets.Split.TRAIN,\n', ""                            gen_kwargs={'file': 'train_data.zip'},\n"", '                    ),\n', '                    datasets.SplitGenerator(\n', '                            name=datasets.Split.TEST,\n', ""                            gen_kwargs={'file': 'test_data.zip'},\n"", '                    ),\n', '            ]\n', '\n', ""        The above code will first call `_generate_examples(file='train_data.zip')`\n"", ""        to write the train data, then `_generate_examples(file='test_data.zip')` to\n"", '        write the test data.\n', '\n', '        Datasets are typically split into different subsets to be used at various\n', '        stages of training and evaluation.\n', '\n', '        Note that for datasets without a `VALIDATION` split, you can use a\n', '        fraction of the `TRAIN` data for evaluation as you iterate on your model\n', '        so as not to overfit to the `TEST` data.\n', '\n', '        For downloads and extractions, use the given `download_manager`.\n', '        Note that the `DownloadManager` caches downloads, so it is fine to have each\n', '        generator attempt to download the source data.\n', '\n', '        A good practice is to download all data in this function, and then\n', '        distribute the relevant parts to each split with the `gen_kwargs` argument\n', '\n', '        Args:\n', '            dl_manager: (DownloadManager) Download manager to download the data\n', '\n', '        Returns:\n', '            `list<SplitGenerator>`.\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_prepare_split,DatasetBuilder:_prepare_split,method,1,2,2,26,13.0,0,0,"['self', 'split_generator', '**kwargs']","[None, ' SplitGenerator', None]","[None, None, None]",976,"['        """"""Generate the examples and record them on disk.\n', '\n', '        Args:\n', '            split_generator: `SplitGenerator`, Split generator to process\n', '            **kwargs: Additional kwargs forwarded from _download_and_prepare (ex:\n', '                beam pipeline)\n', '        """"""\n']"
parser/test3/builder.py:DatasetBuilder:_get_examples_iterable_for_split,DatasetBuilder:_get_examples_iterable_for_split,method,1,2,2,26,13.0,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",986,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator: `SplitGenerator`, Split generator to process\n', '        """"""\n']"
parser/test3/builder.py:GeneratorBasedBuilder:__init__,GeneratorBasedBuilder:__init__,method,5,7,7,132,18.86,0,0,"['self', '*args', 'writer_batch_size', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",1013,[]
parser/test3/builder.py:GeneratorBasedBuilder:_generate_examples,GeneratorBasedBuilder:_generate_examples,method,1,2,2,26,13.0,0,0,"['self', '**kwargs']","[None, None]","[None, None]",1022,"['        """"""Default function generating examples for each `SplitGenerator`.\n', '\n', '        This function preprocess the examples from the raw data to the preprocessed\n', '        dataset files.\n', '        This function is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples yielded here will be written on\n', '        disk.\n', '\n', '        Args:\n', '            **kwargs: `dict`, Arguments forwarded from the SplitGenerator.gen_kwargs\n', '\n', '        Yields:\n', '            key: `str` or `int`, a unique deterministic example identification key.\n', '                * Unique: An error will be raised if two examples are yield with the\n', '                    same key.\n', '                * Deterministic: When generating the dataset twice, the same example\n', '                    should have the same key.\n', '                Good keys can be the image id, or line number if examples are extracted\n', '                from a text file.\n', '                The key will be hashed and sorted to shuffle examples deterministically,\n', '                such as generating the dataset multiple times keep examples in the\n', '                same order.\n', '            example: `dict<str feature_name, feature_value>`, a feature dictionary\n', '                ready to be encoded and written to disk. The example will be\n', '                encoded with `self.info.features.encode_example({...})`.\n', '        """"""\n']"
parser/test3/builder.py:GeneratorBasedBuilder:_prepare_split,GeneratorBasedBuilder:_prepare_split,method,22,46,45,752,16.35,1,0,"['self', 'split_generator']","[None, None]","[None, None]",1051,[]
parser/test3/builder.py:GeneratorBasedBuilder:_get_examples_iterable_for_split,GeneratorBasedBuilder:_get_examples_iterable_for_split,method,2,3,3,74,24.67,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",986,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator: `SplitGenerator`, Split generator to process\n', '        """"""\n']"
parser/test3/builder.py:ArrowBasedBuilder:_generate_tables,ArrowBasedBuilder:_generate_tables,method,1,2,2,26,13.0,0,0,"['self', '**kwargs']","[None, None]","[None, None]",1093,"['        """"""Default function generating examples for each `SplitGenerator`.\n', '\n', '        This function preprocess the examples from the raw data to the preprocessed\n', '        dataset files.\n', '        This function is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples yielded here will be written on\n', '        disk.\n', '\n', '        Args:\n', '            **kwargs: `dict`, Arguments forwarded from the SplitGenerator.gen_kwargs\n', '\n', '        Yields:\n', '            key: `str` or `int`, a unique deterministic example identification key.\n', '                * Unique: An error will be raised if two examples are yield with the\n', '                    same key.\n', '                * Deterministic: When generating the dataset twice, the same example\n', '                    should have the same key.\n', '                Good keys can be the image id, or line number if examples are extracted\n', '                from a text file.\n', '                The key will be hashed and sorted to shuffle examples deterministically,\n', '                such as generating the dataset multiple times keep examples in the\n', '                same order.\n', '            example: `pyarrow.Table`, a feature table\n', '                ready to be encoded and written to disk.\n', '        """"""\n']"
parser/test3/builder.py:ArrowBasedBuilder:_prepare_split,ArrowBasedBuilder:_prepare_split,method,19,39,38,577,14.79,1,1,"['self', 'split_generator']","[None, None]","[None, None]",1051,[]
parser/test3/builder.py:ArrowBasedBuilder:_get_examples_iterable_for_split,ArrowBasedBuilder:_get_examples_iterable_for_split,method,2,5,5,121,24.2,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",986,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator: `SplitGenerator`, Split generator to process\n', '        """"""\n']"
parser/test3/builder.py:BeamBasedBuilder:__init__,BeamBasedBuilder:__init__,method,7,15,14,203,13.53,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1154,[]
parser/test3/builder.py:BeamBasedBuilder:_make_split_generators_kwargs,BeamBasedBuilder:_make_split_generators_kwargs,method,5,12,11,254,21.17,0,1,"['self', 'prepare_split_kwargs']","[None, None]","[None, None]",706,"['        """"""Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.""""""\n']"
parser/test3/builder.py:BeamBasedBuilder:_build_pcollection,BeamBasedBuilder:_build_pcollection,method,0,1,1,1,1.0,0,0,"['self', 'pipeline', '**kwargs']","[None, None, None]","[None, None, None]",1171,"['        """"""Build the beam pipeline examples for each `SplitGenerator`.\n', '\n', '        This function extracts examples from the raw data with parallel transforms\n', '        in a Beam pipeline. It is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples from the PCollection will be\n', '        encoded and written to disk.\n', '\n', '        Warning: When running in a distributed setup, make sure that the data\n', '        which will be read (download_dir, manual_dir,...) and written (cache_dir)\n', '        can be accessed by the workers jobs. The data should be located in a\n', '        shared filesystem, like GCS.\n', '\n', '        Example:\n', '\n', '        ```\n', '        def _build_pcollection(pipeline, extracted_dir):\n', '            return (\n', '                    pipeline\n', '                    | beam.Create(gfile.io.listdir(extracted_dir))\n', '                    | beam.Map(_process_file)\n', '            )\n', '        ```\n', '\n', '        Args:\n', '            pipeline: `beam.Pipeline`, root Beam pipeline\n', '            **kwargs: Arguments forwarded from the SplitGenerator.gen_kwargs\n', '\n', '        Returns:\n', '            pcollection: `PCollection`, an Apache Beam PCollection containing the\n', '                example to send to `self.info.features.encode_example(...)`.\n', '        """"""\n']"
parser/test3/builder.py:BeamBasedBuilder:_download_and_prepare,BeamBasedBuilder:_download_and_prepare,method,46,164,131,1634,9.96,1,1,"['self', 'dl_manager', 'verify_infos']","[None, None, None]","[None, None, None]",1205,[]
parser/test3/builder.py:BeamBasedBuilder:_save_info,BeamBasedBuilder:_save_info,method,11,22,17,322,14.64,0,1,['self'],[None],[None],696,"['        """"""Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.""""""\n']"
parser/test3/builder.py:BeamBasedBuilder:_prepare_split,BeamBasedBuilder:_prepare_split,method,23,54,48,838,15.52,0,0,"['self', 'split_generator', 'pipeline']","[None, None, None]","[None, None, None]",1269,[]
parser/test3/combine.py:interleave_datasets,interleave_datasets,function,14,101,63,806,7.98,1,4,"['datasets', 'probabilities', 'seed']","[' List[DatasetType]', ' Optional[List[float]] ', ' Optional[int] ']","[None, ' None', ' None']",20,"['    """"""\n', '    Interleave several datasets (sources) into a single dataset.\n', '    The new dataset is constructed by alternating between the sources to get the examples.\n', '\n', '    You can use this function on a list of :class:`Dataset` objects, or on a list of :class:`IterableDataset` objects.\n', '\n', '    If ``probabilities`` is ``None`` (default) the new dataset is constructed by cycling between each source to get the examples.\n', '    If ``probabilities`` is not ``None``, the new dataset is constructed by getting examples from a random source at a time according to the provided probabilities.\n', '\n', '    The resulting dataset ends when one of the source datasets runs out of examples.\n', '\n', '    Examples:\n', '\n', '        For regular datasets (map-style):\n', '\n', '        >>> from datasets import Dataset, interleave_datasets\n', '        >>> d1 = Dataset.from_dict({""a"": [0, 1, 2]})\n', '        >>> d2 = Dataset.from_dict({""a"": [10, 11, 12]})\n', '        >>> d3 = Dataset.from_dict({""a"": [20, 21, 22]})\n', '        >>> dataset = interleave_datasets([d1, d2, d3])\n', '        >>> dataset[""a""]\n', '        [0, 10, 20, 1, 11, 21, 2, 12, 22]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n', '        >>> dataset[""a""]\n', '        [10, 0, 11, 1, 2, 20, 12]\n', '\n', '        For datasets in streaming mode (iterable):\n', '\n', '        >>> from datasets import load_dataset, interleave_datasets\n', '        >>> d1 = load_dataset(""oscar"", ""unshuffled_deduplicated_en"", split=""train"", streaming=True)\n', '        >>> d2 = load_dataset(""oscar"", ""unshuffled_deduplicated_fr"", split=""train"", streaming=True)\n', '        >>> dataset = interleave_datasets([d1, d2])\n', '        >>> iterator = iter(dataset)\n', '        >>> next(iterator)\n', ""        {'text': 'Mtendere Village was inspired by the vision...\n"", '        >>> next(iterator)\n', '        {\'text\': ""Média de débat d\'idées, de culture...\n', '\n', '    Args:\n', '        datasets (:obj:`List[Dataset]` or :obj:`List[IterableDataset]`): list of datasets to interleave\n', '        probabilities (:obj:`List[float]`, optional, default None): If specified, the new dataset is constructued by sampling\n', '            examples from one source at a time according to these probabilities.\n', '        seed (:obj:`int`, optional, default None): The random seed used to choose a source for each example.\n', '        **kwargs: For map-style datasets:\n', '            Keyword arguments to be passed to :meth:`datasets.Datasets.select` when selecting the indices used to interleave the datasets.\n', '\n', '    Output:\n', '        :class:`datasets.Dataset` if the input is a list of :class:`datasets.Dataset`\n', '        or :class:`datasets.IterableDataset` if the input is a list of :class:`datasets.IterableDataset`\n', '    """"""\n']"
parser/test3/combine.py:_interleave_map_style_datasets,_interleave_map_style_datasets,function,26,103,79,1038,10.08,2,4,"['datasets', 'probabilities', 'seed', 'info', 'split', '**kwargs', '']","[' List[""Dataset""]', ' Optional[List[float]] ', ' Optional[int] ', ' Optional[Any] ', ' Optional[Any] ', None, None]","[None, ' None', ' None', ' None', ' None', None, None]",95,"['    """"""\n', '    Interleave several map-style datasets (sources) into a single map-style dataset.\n', '    The new dataset is constructed by alternating between the sources to get the examples.\n', '    If `probabilities = None` (default) the new dataset is constructed by cycling between each source to get the examples.\n', '    If `probabilities` is not `None, the new dataset is constructed by getting examples from a random source at a time according to the provided probabilities.\n', '\n', '    Args:\n', '        datasets (:obj:`List[Dataset]`): list of datasets to interleave\n', '        probabilities (:obj:`List[float]`, optional, default None): If specified, the new dataset is constructued by sampling\n', '            examples from one source at a time according to these probabilities.\n', '        seed (:obj:`int`, optional, default None): The random seed used to choose a source for each example.\n', '        **kwargs: Keyword arguments to be passed to :meth:`datasets.Datasets.select` when selecting the indices used to interleave the datasets.\n', '\n', '    Output:\n', '        :class:`datasets.Dataset`\n', '    """"""\n']"
parser/test3/combine.py:_interleave_iterable_datasets,_interleave_iterable_datasets,function,12,52,42,644,12.38,0,2,"['datasets', 'probabilities', 'seed', 'info', 'split', '']","[' List[""IterableDataset""]', ' Optional[List[float]] ', ' Optional[int] ', ' Optional[Any] ', ' Optional[Any] ', None]","[None, ' None', ' None', ' None', ' None', None]",161,"['    """"""\n', '    Interleave several iterable datasets (sources) into a single iterable dataset.\n', '    The new iterable dataset alternates between the sources to yield examples.\n', '    If `probabilities = None` (default) the iterable dataset will cycles through the sources in order for each next example in the iteration.\n', '    If `probabilities` is not `None, the iterable dataset will sample a random source according to the provided probabilities for each next examples in the iteration.\n', '\n', '    Args:\n', '        datasets (:obj:`List[IterableDataset]`): list of datasets to interleave\n', '        probabilities (:obj:`List[float]`, optional, default None): If specified, the new iterable dataset samples\n', '            examples from one source at a time according to these probabilities.\n', '        seed (:obj:`int`, optional, default None): The random seed used to choose a source for each example.\n', '\n', '    Output:\n', '        :class:`datasets.IterableDataset`\n', '    """"""\n']"
parser/test3/dataset_dict.py:DatasetDict,DatasetDict,class,119,1168,398,12955,11.09,33,15,[],[],[],25,[]
parser/test3/dataset_dict.py:IterableDatasetDict,IterableDatasetDict,class,0,1,1,4,4.0,0,0,[],[],[],871,[]
parser/test3/dataset_dict.py:DatasetDict:_check_values_type,DatasetDict:_check_values_type,method,4,22,20,159,7.23,1,1,['self'],[None],[None],28,[]
parser/test3/dataset_dict.py:DatasetDict:__getitem__,DatasetDict:__getitem__,method,7,49,43,438,8.94,0,2,"['self', 'k']","[None, None]","[None, None]",35,[]
parser/test3/dataset_dict.py:DatasetDict:data,DatasetDict:data,method,6,9,9,74,8.22,1,0,['self'],[None],[None],50,"['        """"""The Apache Arrow tables backing each split.""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:cache_files,DatasetDict:cache_files,method,6,9,9,81,9.0,1,0,['self'],[None],[None],56,"['        """"""The cache files containing the Apache Arrow table backing each split.""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:num_columns,DatasetDict:num_columns,method,6,9,9,81,9.0,1,0,['self'],[None],[None],62,"['        """"""Number of columns in each split of the dataset.""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:num_rows,DatasetDict:num_rows,method,6,9,9,78,8.67,1,0,['self'],[None],[None],68,"['        """"""Number of rows in each split of the dataset (same as :func:`datasets.Dataset.__len__`).""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:column_names,DatasetDict:column_names,method,6,9,9,82,9.11,1,0,['self'],[None],[None],74,"['        """"""Names of the columns in each split of the dataset.""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:shape,DatasetDict:shape,method,6,9,9,75,8.33,1,0,['self'],[None],[None],80,"['        """"""Shape of each split of the dataset (number of columns, number of rows).""""""\n']"
parser/test3/dataset_dict.py:DatasetDict:dictionary_encode_column_,DatasetDict:dictionary_encode_column_,method,4,6,6,101,16.83,1,0,"['self', 'column']","[None, ' str']","[None, None]",86,"['        """"""Dictionary encode a column in each split.\n', '\n', '        Dictionary encode can reduce the size of a column with many repetitions (e.g. string labels columns)\n', '        by storing a dictionary of the strings. This only affect the internal storage.\n', '\n', '        .. deprecated:: 1.4.0\n', '\n', '        Args:\n', '            column (:obj:`str`):\n', '\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:flatten_,DatasetDict:flatten_,method,4,6,6,90,15.0,1,0,"['self', 'max_depth']","[None, None]","[None, '16']",103,"['        """"""In-place version of :meth:`DatasetDict.flatten`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`DatasetDict.flatten` instead.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:flatten,DatasetDict:flatten,method,7,20,16,248,12.4,1,0,"['self', 'max_depth']","[None, None]","[None, '16']",113,[]
parser/test3/dataset_dict.py:DatasetDict:unique,DatasetDict:unique,method,3,9,9,84,9.33,0,0,"['self', 'column']","[None, ' str']","[None, None]",121,"['        """"""Return a list of the unique elements in a column for each split.\n', '\n', '        This is implemented in the low-level backend and as such, very fast.\n', '\n', '        Args:\n', '            column (:obj:`str`):\n', '                column name (list all the column names with :func:`datasets.Dataset.column_names`)\n', '\n', '        Returns:\n', '            Dict[:obj:`str`, :obj:`list`]: Dictionary of unique elements in the given column.\n', '\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:cleanup_cache_files,DatasetDict:cleanup_cache_files,method,3,9,9,91,10.11,0,0,['self'],[None],[None],137,"['        """"""Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n', '        Be carefull when running this command that no other process is currently using other cache files.\n', '\n', '        Return:\n', '            Dict with the number of removed files for each split\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:__repr__,DatasetDict:__repr__,method,4,18,16,120,6.67,0,0,['self'],[None],[None],147,[]
parser/test3/dataset_dict.py:DatasetDict:cast_,DatasetDict:cast_,method,4,10,10,134,13.4,0,0,"['self', 'features']","[None, ' Features']","[None, None]",153,"['        """"""In-place version of :meth:`DatasetDict.cast`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`DatasetDict.cast` instead.\n', '\n', '        Args:\n', '            features (:class:`datasets.Features`): New features to cast the dataset to.\n', '                The name and order of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:cast,DatasetDict:cast,method,7,25,19,289,11.56,0,0,"['self', 'features']","[None, ' Features']","[None, None]",169,[]
parser/test3/dataset_dict.py:DatasetDict:remove_columns_,DatasetDict:remove_columns_,method,4,10,10,152,15.2,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",187,"['        """"""In-place version of :meth:`DatasetDict.remove_columns`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`DatasetDict.remove_columns` instead.\n', '\n', '        Args:\n', '            column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:remove_columns,DatasetDict:remove_columns,method,7,26,20,351,13.5,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",200,[]
parser/test3/dataset_dict.py:DatasetDict:rename_column_,DatasetDict:rename_column_,method,8,13,13,202,15.54,1,0,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",217,"['        """"""In-place version of :meth:`DatasetDict.rename_column`.\n', '\n', '        .. deprecated:: 1.4.0\n', '            Use :meth:`DatasetDict.rename_column` instead.\n', '\n', '        Args:\n', '            original_column_name (:obj:`str`): Name of the column to rename.\n', '            new_column_name (:obj:`str`): New name for the column.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:rename_column,DatasetDict:rename_column,method,11,35,24,463,13.23,2,0,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",234,[]
parser/test3/dataset_dict.py:DatasetDict:class_encode_column,DatasetDict:class_encode_column,method,3,9,9,117,13.0,0,0,"['self', 'column']","[None, ' str']","[None, None]",255,"['        """"""Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the tables.\n', '\n', '        Args:\n', '            column (`str`): The name of the column to cast\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:formatted_as,DatasetDict:formatted_as,method,15,51,29,542,10.63,5,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",265,"['        """"""To be used in a `with` statement. Set __getitem__ return format (type and columns)\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', ""            type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                None means __getitem__ returns all columns (default)\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:set_format,DatasetDict:set_format,method,4,9,9,152,16.89,1,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",297,"['        """"""Set __getitem__ return format (type and columns)\n', '        The format is set for every dataset in the dataset dictionary\n', '\n', '        Args:\n', ""            type (Optional ``str``): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output.\n', '                None means __getitem__ returns all columns (default).\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        It is possible to call ``map`` after calling ``set_format``. Since ``map`` may add new columns, then the list of formatted columns\n', '        gets updated. In this case, if you apply ``map`` on a dataset to add a new column, then this column will be formatted:\n', '\n', '            new formatted columns = (all columns - previously unformatted columns)\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:reset_format,DatasetDict:reset_format,method,4,6,6,73,12.17,1,0,['self'],[None],[None],324,"['        """"""Reset __getitem__ return format to python objects and all columns.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Same as ``self.set_format()``\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:set_transform,DatasetDict:set_transform,method,4,9,9,155,17.22,1,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",334,"['        """"""Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n', '        The transform is set for every dataset in the dataset dictionary\n', '        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n', '\n', '        Args:\n', '            transform (Optional ``Callable``): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in __getitem__.\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:with_format,DatasetDict:with_format,method,4,8,7,141,17.62,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",358,"['        """"""Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format ``type`` (for example ""numpy"") is used to format batches when using __getitem__.\n', '        The format is set for every dataset in the dataset dictionary\n', '\n', ""        It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.with_transform`.\n"", '\n', '        Contrary to :func:`datasets.DatasetDict.set_format`, ``with_format`` returns a new DatasetDict object with new Dataset objects.\n', '\n', '        Args:\n', '            type (Optional ``str``):\n', ""                Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n"", '                None means __getitem__ returns python objects (default)\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                None means __getitem__ returns all columns (default)\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '            format_kwargs: keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:with_transform,DatasetDict:with_transform,method,4,7,6,138,19.71,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",386,"['        """"""Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n', '        The transform is set for every dataset in the dataset dictionary\n', '\n', '        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`.\n', '\n', '        Contrary to :func:`datasets.DatasetDict.set_transform`, ``with_transform`` returns a new DatasetDict object with new Dataset objects.\n', '\n', '        Args:\n', '            transform (Optional ``Callable``): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in __getitem__.\n', '            columns (Optional ``List[str]``): columns to format in the output\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (``bool`` default to False): keep un-formatted columns as well in the output (as python objects)\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:map,DatasetDict:map,method,9,40,37,555,13.88,2,1,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'remove_columns', 'keep_in_memory', 'load_from_cache_file', 'cache_file_names', 'Optional[str]]] ', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'num_proc', 'desc', '']","[None, None, ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' Optional[List[str]] ', ' bool ', ' bool ', ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, ' False', None, ' None', ' False', ' 1000', ' None', ' False', ' True', None, ' None', ' 1000', ' None', ' False', ' None', ' None', ' None', None]",413,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does updated examples).\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`callable`): with one of the following signature:\n', '                - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n', '            with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n', '            batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n', '            remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `True`): Disallow null values in the table.\n', '            fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n', ""            num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            desc (`Optional[str]`, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:filter,DatasetDict:filter,method,9,36,33,476,13.22,2,1,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batch_size', 'remove_columns', 'keep_in_memory', 'load_from_cache_file', 'cache_file_names', 'Optional[str]]] ', 'writer_batch_size', 'fn_kwargs', 'num_proc', '']","[None, None, None, ' Optional[Union[str', None, ' Optional[int] ', ' Optional[List[str]] ', ' bool ', ' bool ', ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[dict] ', ' Optional[int] ', None]","[None, None, 'False', None, ' None', ' 1000', ' None', ' False', ' True', None, ' None', ' 1000', ' None', ' None', None]",493,"['        """"""Apply a filter function to all the elements in the table in batches\n', '        and update the table so that the dataset only includes examples according to the filter function.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`callable`): with one of the following signature:\n', '                - `function(example: Dict) -> bool` if `with_indices=False`\n', '                - `function(example: Dict, indices: int) -> bool` if `with_indices=True`\n', '            with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batch_size (`Optional[int]`, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n', '            remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            fn_kwargs (`Optional[Dict]`, defaults to `None`): Keyword arguments to be passed to `function`\n', ""            num_proc (`Optional[int]`, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:sort,DatasetDict:sort,method,9,32,29,380,11.88,2,1,"['self', 'column', 'reverse', 'kind', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_names', 'Optional[str]]] ', 'writer_batch_size', '']","[None, ' str', ' bool ', ' str ', ' bool ', ' bool ', ' Optional[Dict[str', None, ' Optional[int] ', None]","[None, None, ' False', ' None', ' False', ' True', None, ' None', ' 1000', None]",558,"['        """"""Create a new dataset sorted according to a column.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Currently sorting according to a column name uses numpy sorting algorithm under the hood.\n', '        The column should thus be a numpy compatible type (in particular not a nested type).\n', '        This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n', '\n', '        Args:\n', '            column (`str`): column name to sort by.\n', '            reverse: (`bool`, defaults to `False`): If True, sort by descending order rather then ascending.\n', '            kind (Optional `str`): Numpy algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n', '                The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n', '                the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n', '            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n', '                indices mapping instead of the automatically generated cache file name.\n', '                You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:shuffle,DatasetDict:shuffle,method,15,92,45,651,7.08,5,5,"['self', 'seeds', 'Dict[str', 'Optional[int]]]] ', 'seed', 'generators', 'np.random.Generator]] ', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_names', 'Optional[str]]] ', 'writer_batch_size', '']","[None, ' Optional[Union[int', None, None, ' Optional[int] ', ' Optional[Dict[str', None, ' bool ', ' bool ', ' Optional[Dict[str', None, ' Optional[int] ', None]","[None, None, None, ' None', ' None', None, ' None', ' False', ' True', None, ' None', ' 1000', None]",609,"['        """"""Create a new Dataset where the rows are shuffled.\n', '\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Currently shuffling uses numpy random generators.\n', ""        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n"", '\n', '        Args:\n', '            seeds (`Dict[str, int]` or `int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n', '                If None, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '                You can provide one :obj:`seed` per dataset in the dataset dictionary.\n', '            seed (Optional `int`): A seed to initialize the default BitGenerator if ``generator=None``. Alias for seeds (the seed argument has priority over seeds if both arguments are provided).\n', '            generators (Optional `Dict[str, np.random.Generator]`): Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n', '                You have to provide one :obj:`generator` per dataset in the dataset dictionary.\n', '            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_names (`Dict[str, str]`, optional): Provide the name of a path for the cache file. It is used to store the\n', '                indices mappings instead of the automatically generated cache file name.\n', '                You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:save_to_disk,DatasetDict:save_to_disk,method,13,27,26,452,16.74,1,1,"['self', 'dataset_dict_path', 'fs']","[None, ' str', None]","[None, None, 'None']",671,"['        """"""\n', '        Saves a dataset dict to a filesystem using either :class:`~filesystems.S3FileSystem` or\n', '        ``fsspec.spec.AbstractFileSystem``.\n', '\n', '        Args:\n', '            dataset_dict_path (``str``): Path (e.g. `dataset/train`) or remote URI\n', '                (e.g. `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset dict will be\n', '                saved to.\n', '            fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, defaults ``None``):\n', '                Instance of the remote filesystem used to download the files from.\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:load_from_disk,DatasetDict:load_from_disk,method,19,69,59,995,14.42,1,3,"['dataset_dict_path', 'fs', 'keep_in_memory']","[' str', None, ' Optional[bool] ']","[None, 'None', ' None']",698,"['        """"""\n', '        Load a dataset that was previously saved using :meth:`save_to_disk` from a filesystem using either\n', '        :class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``.\n', '\n', '        Args:\n', '            dataset_dict_path (:obj:`str`): Path (e.g. ``""dataset/train""``) or remote URI (e.g.\n', '                ``""s3//my-bucket/dataset/train""``) of the dataset dict directory where the dataset dict will be loaded\n', '                from.\n', '            fs (:class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n', '                Instance of the remote filesystem used to download the files from.\n', '            keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the\n', '                dataset will not be copied in-memory unless explicitly enabled by setting\n', '                `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n', '                :ref:`load_dataset_enhancing_performance` section.\n', '\n', '        Returns:\n', '            :class:`DatasetDict`\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:from_csv,DatasetDict:from_csv,method,4,12,12,157,13.08,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",739,"['        """"""Create DatasetDict from CSV file(s).\n', '\n', '        Args:\n', '            path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (str, optional, default=""~/.cache/huggingface/datasets""): Directory to cache data.\n', '            keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n', '            **kwargs: Keyword arguments to be passed to :meth:`pandas.read_csv`.\n', '\n', '        Returns:\n', '            :class:`DatasetDict`\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:from_json,DatasetDict:from_json,method,4,12,12,160,13.33,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",766,"['        """"""Create DatasetDict from JSON Lines file(s).\n', '\n', '        Args:\n', '            path_or_paths (path-like or list of path-like): Path(s) of the JSON Lines file(s).\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (str, optional, default=""~/.cache/huggingface/datasets""): Directory to cache data.\n', '            keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n', '            **kwargs: Keyword arguments to be passed to :class:`JsonConfig`.\n', '\n', '        Returns:\n', '            :class:`DatasetDict`\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:from_parquet,DatasetDict:from_parquet,method,4,13,13,191,14.69,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', 'columns', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', ' Optional[List[str]] ', None, None]","[None, None, ' None', ' None', ' False', ' None', None, None]",793,"['        """"""Create DatasetDict from Parquet file(s).\n', '\n', '        Args:\n', '            path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (str, optional, default=""~/.cache/huggingface/datasets""): Directory to cache data.\n', '            keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n', '            columns (:obj:`List[str]`, optional): If not None, only these columns will be read from the file.\n', ""                A column name may be a prefix of a nested field, e.g. 'a' will select\n"", ""                'a.b', 'a.c', and 'a.d.e'.\n"", '            **kwargs: Keyword arguments to be passed to :class:`ParquetConfig`.\n', '\n', '        Returns:\n', '            :class:`DatasetDict`\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:from_text,DatasetDict:from_text,method,4,12,12,160,13.33,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",829,"['        """"""Create DatasetDict from text file(s).\n', '\n', '        Args:\n', '            path_or_paths (dict of path-like): Path(s) of the text file(s).\n', '            features (:class:`Features`, optional): Dataset features.\n', '            cache_dir (str, optional, default=""~/.cache/huggingface/datasets""): Directory to cache data.\n', '            keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n', '            **kwargs: Keyword arguments to be passed to :class:`TextConfig`.\n', '\n', '        Returns:\n', '            :class:`DatasetDict`\n', '        """"""\n']"
parser/test3/dataset_dict.py:DatasetDict:prepare_for_task,DatasetDict:prepare_for_task,method,3,9,9,110,12.22,0,0,"['self', 'task', 'TaskTemplate]']","[None, ' Union[str', None]","[None, None, None]",856,[]
parser/test3/dataset_dict.py:DatasetDict:align_labels_with_mapping,DatasetDict:align_labels_with_mapping,method,6,14,14,158,11.29,1,0,"['self', 'label2id', 'label_column']","[None, ' Dict', ' str']","[None, None, None]",861,[]
parser/test3/features.py:_arrow_to_datasets_dtype,_arrow_to_datasets_dtype,function,22,117,69,1406,12.02,0,2,['arrow_type'],[' pa.DataType'],[None],43,"['    """"""\n', '    _arrow_to_datasets_dtype takes a pyarrow.DataType and converts it to a datasets string dtype.\n', '    In effect, `dt == string_to_arrow(_arrow_to_datasets_dtype(dt))`\n', '    """"""\n']"
parser/test3/features.py:string_to_arrow,string_to_arrow,function,17,97,73,1219,12.57,0,3,['datasets_dtype'],[' str'],[None],95,"['    """"""\n', '    string_to_arrow takes a datasets string dtype and converts it to a pyarrow.DataType.\n', '\n', '    In effect, `dt == string_to_arrow(_arrow_to_datasets_dtype(dt))`\n', '\n', '    This is necessary because the datasets.Value() primitive type is constructed using a string dtype\n', '\n', '    Value(dtype=str)\n', '\n', '    But Features.type (via `get_nested_type()` expects to resolve Features into a pyarrow Schema,\n', '        which means that each Value() must be able to resolve into a corresponding pyarrow.DataType, which is the\n', '        purpose of this function.\n', '    """"""\n']"
parser/test3/features.py:_cast_to_python_objects,_cast_to_python_objects,function,34,164,78,1357,8.27,2,10,['obj'],[' Any'],[None],145,"['    """"""\n', '    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\n', '    It works recursively.\n', '\n', '    To avoid iterating over possibly long lists, it first checks if the first element that is not None has to be casted.\n', ""    If the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\n"", '    This trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\n', '\n', '    Args:\n', '        obj: the object (nested struct) to cast\n', '\n', '    Returns:\n', '        casted_obj: the casted object\n', '        has_changed (bool): True if the object has been changed, False if it is identical\n', '    """"""\n']"
parser/test3/features.py:cast_to_python_objects,cast_to_python_objects,function,2,2,2,37,18.5,0,0,['obj'],[' Any'],[None],210,"['    """"""\n', '    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\n', '    It works recursively.\n', '\n', '    To avoid iterating over possibly long lists, it first checks if the first element that is not None has to be casted.\n', ""    If the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\n"", '    This trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\n', '\n', '    Args:\n', '        obj: the object (nested struct) to cast\n', '\n', '    Returns:\n', '        casted_obj: the casted object\n', '    """"""\n']"
parser/test3/features.py:_is_zero_copy_only,_is_zero_copy_only,function,2,5,5,52,10.4,0,0,['pa_type'],[' pa.DataType'],[None],381,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']"
parser/test3/features.py:pandas_types_mapper,pandas_types_mapper,function,3,5,5,92,18.4,0,1,['dtype'],[None],[None],544,[]
parser/test3/features.py:get_nested_type,get_nested_type,function,18,95,67,749,7.88,0,2,['schema'],[' FeatureType'],[None],826,"['    """"""\n', '    get_nested_type() converts a datasets.FeatureType into a pyarrow.DataType, and acts as the inverse of\n', '        generate_from_arrow_type().\n', '\n', '    It performs double-duty as the implementation of Features.type and handles the conversion of\n', '        datasets.Feature->pa.struct\n', '    """"""\n']"
parser/test3/features.py:encode_nested_example,encode_nested_example,function,14,111,62,958,8.63,2,4,"['schema', 'obj']","[None, None]","[None, None]",858,"['    """"""Encode a nested example.\n', '    This is used since some features (in particular ClassLabel) have some logic during encoding.\n', '    """"""\n']"
parser/test3/features.py:generate_from_dict,generate_from_dict,function,9,51,34,435,8.53,0,3,['obj'],[' Any'],[None],897,"['    """"""Regenerate the nested feature object from a deserialized dict.\n', ""    We use the '_type' fields to get the dataclass name to load.\n"", '\n', '    generate_from_dict is the recursive helper for Features.from_dict, and allows for a convenient constructor syntax\n', '    to define features from deserialized JSON dictionaries. This function is used in particular when deserializing\n', '    a :class:`DatasetInfo` that was dumped to a JSON object. This acts as an analogue to\n', ""    :meth:`Features.from_arrow_schema` and handles the recursive field-by-field instantiation, but doesn't require any\n"", '    mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive dtypes\n', '    that :class:`Value` automatically performs.\n', '    """"""\n']"
parser/test3/features.py:generate_from_arrow_type,generate_from_arrow_type,function,15,79,60,915,11.58,0,2,['pa_type'],[' pa.DataType'],[None],923,"['    """"""\n', '    generate_from_arrow_type accepts an arrow DataType and returns a datasets FeatureType to be used as the type for\n', '        a single field.\n', '\n', '    This is the high-level arrow->datasets type conversion and is inverted by get_nested_type().\n', '\n', '    This operates at the individual *field* level, whereas Features.from_arrow_schema() operates at the\n', '        full schema level and holds the methods that represent the bijection from Features<->pyarrow.Schema\n', '    """"""\n']"
parser/test3/features.py:Value,Value,class,20,57,40,556,9.75,0,3,[],[],[],229,[]
parser/test3/features.py:_ArrayXD,_ArrayXD,class,11,22,18,263,11.95,0,1,[],[],[],281,[]
parser/test3/features.py:Array2D,Array2D,class,8,12,12,100,8.33,0,0,[],[],[],296,[]
parser/test3/features.py:Array3D,Array3D,class,8,12,12,100,8.33,0,0,[],[],[],305,[]
parser/test3/features.py:Array4D,Array4D,class,8,12,12,100,8.33,0,0,[],[],[],314,[]
parser/test3/features.py:Array5D,Array5D,class,8,12,12,100,8.33,0,0,[],[],[],323,[]
parser/test3/features.py:_ArrayXDExtensionType,_ArrayXDExtensionType,class,22,81,65,737,9.1,1,0,[],[],[],331,[]
parser/test3/features.py:Array2DExtensionType,Array2DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],365,[]
parser/test3/features.py:Array3DExtensionType,Array3DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],369,[]
parser/test3/features.py:Array4DExtensionType,Array4DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],373,[]
parser/test3/features.py:Array5DExtensionType,Array5DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],377,[]
parser/test3/features.py:ArrayExtensionArray,ArrayExtensionArray,class,20,41,31,593,14.46,1,0,[],[],[],395,[]
parser/test3/features.py:PandasArrayExtensionDtype,PandasArrayExtensionDtype,class,18,61,41,725,11.89,0,1,[],[],[],418,[]
parser/test3/features.py:PandasArrayExtensionArray,PandasArrayExtensionArray,class,49,263,166,2462,9.36,1,8,[],[],[],453,[]
parser/test3/features.py:ClassLabel,ClassLabel,class,62,381,195,2924,7.67,3,17,[],[],[],550,[]
parser/test3/features.py:Translation,Translation,class,14,25,24,242,9.68,0,0,[],[],[],683,[]
parser/test3/features.py:TranslationVariableLanguages,TranslationVariableLanguages,class,33,98,75,1053,10.74,1,2,[],[],[],720,[]
parser/test3/features.py:Sequence,Sequence,class,12,19,18,159,8.37,0,0,[],[],[],796,[]
parser/test3/features.py:Features,Features,class,43,239,127,2194,9.18,3,9,[],[],[],953,[]
parser/test3/features.py:Value:__post_init__,Value:__post_init__,method,3,20,12,161,8.05,0,2,['self'],[None],[None],260,[]
parser/test3/features.py:Value:__call__,Value:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],267,[]
parser/test3/features.py:Value:encode_example,Value:encode_example,method,6,15,11,187,12.47,0,1,"['self', 'value']","[None, None]","[None, None]",270,[]
parser/test3/features.py:_ArrayXD:__post_init__,_ArrayXD:__post_init__,method,2,2,2,28,14.0,0,0,['self'],[None],[None],260,[]
parser/test3/features.py:_ArrayXD:__call__,_ArrayXD:__call__,method,3,6,5,95,15.83,0,0,['self'],[None],[None],267,[]
parser/test3/features.py:_ArrayXD:encode_example,_ArrayXD:encode_example,method,4,7,6,64,9.14,0,1,"['self', 'value']","[None, None]","[None, None]",270,[]
parser/test3/features.py:_ArrayXDExtensionType:__init__,_ArrayXDExtensionType:__init__,method,7,43,38,342,7.95,0,0,"['self', 'shape', 'dtype']","[None, ' tuple', ' str']","[None, None, None]",334,[]
parser/test3/features.py:_ArrayXDExtensionType:__reduce__,_ArrayXDExtensionType:__reduce__,method,2,6,6,53,8.83,0,0,['self'],[None],[None],344,[]
parser/test3/features.py:_ArrayXDExtensionType:__arrow_ext_class__,_ArrayXDExtensionType:__arrow_ext_class__,method,2,2,2,25,12.5,0,0,['self'],[None],[None],350,[]
parser/test3/features.py:_ArrayXDExtensionType:_generate_dtype,_ArrayXDExtensionType:_generate_dtype,method,6,10,8,90,9.0,1,0,"['self', 'dtype']","[None, None]","[None, None]",353,[]
parser/test3/features.py:_ArrayXDExtensionType:to_pandas_dtype,_ArrayXDExtensionType:to_pandas_dtype,method,2,2,2,48,24.0,0,0,['self'],[None],[None],361,[]
parser/test3/features.py:ArrayExtensionArray:__array__,ArrayExtensionArray:__array__,method,4,4,4,103,25.75,0,0,['self'],[None],[None],396,[]
parser/test3/features.py:ArrayExtensionArray:__getitem__,ArrayExtensionArray:__getitem__,method,2,2,2,21,10.5,0,0,"['self', 'i']","[None, None]","[None, None]",400,[]
parser/test3/features.py:ArrayExtensionArray:to_numpy,ArrayExtensionArray:to_numpy,method,12,21,18,251,11.95,1,0,"['self', 'zero_copy_only']","[None, None]","[None, 'True']",403,[]
parser/test3/features.py:ArrayExtensionArray:to_pylist,ArrayExtensionArray:to_pylist,method,4,4,4,112,28.0,0,0,['self'],[None],[None],413,[]
parser/test3/features.py:PandasArrayExtensionDtype:__init__,PandasArrayExtensionDtype:__init__,method,2,2,2,27,13.5,0,0,"['self', 'value_type', 'np.dtype]']","[None, ' Union[""PandasArrayExtensionDtype""', None]","[None, None, None]",421,[]
parser/test3/features.py:PandasArrayExtensionDtype:__from_arrow__,PandasArrayExtensionDtype:__from_arrow__,method,8,16,15,278,17.38,0,1,"['self', 'array']","[None, None]","[None, None]",424,[]
parser/test3/features.py:PandasArrayExtensionDtype:construct_array_type,PandasArrayExtensionDtype:construct_array_type,method,2,2,2,31,15.5,0,0,['cls'],[None],[None],433,[]
parser/test3/features.py:PandasArrayExtensionDtype:type,PandasArrayExtensionDtype:type,method,2,2,2,16,8.0,0,0,['self'],[None],[None],437,[]
parser/test3/features.py:PandasArrayExtensionDtype:kind,PandasArrayExtensionDtype:kind,method,1,2,2,9,4.5,0,0,['self'],[None],[None],441,[]
parser/test3/features.py:PandasArrayExtensionDtype:name,PandasArrayExtensionDtype:name,method,1,2,2,33,16.5,0,0,['self'],[None],[None],445,[]
parser/test3/features.py:PandasArrayExtensionDtype:value_type,PandasArrayExtensionDtype:value_type,method,2,2,2,22,11.0,0,0,['self'],[None],[None],449,[]
parser/test3/features.py:PandasArrayExtensionArray:__init__,PandasArrayExtensionArray:__init__,method,6,9,9,92,10.22,0,1,"['self', 'data', 'copy']","[None, ' np.ndarray', ' bool ']","[None, None, ' False']",454,[]
parser/test3/features.py:PandasArrayExtensionArray:__array__,PandasArrayExtensionArray:__array__,method,11,23,19,189,8.22,1,2,"['self', 'dtype']","[None, None]","[None, 'None']",458,"['        """"""\n', '        Convert to NumPy Array.\n', '        Note that Pandas expects a 1D array when dtype is set to object.\n', '        But for other dtypes, the returned shape is the same as the one of ``data``.\n', '\n', '        More info about pandas 1D requirement for PandasExtensionArray here:\n', '        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.extensions.ExtensionArray.html#pandas.api.extensions.ExtensionArray\n', '\n', '        """"""\n']"
parser/test3/features.py:PandasArrayExtensionArray:copy,PandasArrayExtensionArray:copy,method,2,3,3,53,17.67,0,0,"['self', 'deep']","[None, ' bool ']","[None, ' False']",478,[]
parser/test3/features.py:PandasArrayExtensionArray:_from_sequence,PandasArrayExtensionArray:_from_sequence,method,4,13,13,103,7.92,0,0,"['cls', 'scalars', 'dtype', 'copy']","[None, None, ' Optional[PandasArrayExtensionDtype] ', ' bool ']","[None, None, ' None', ' False']",482,[]
parser/test3/features.py:PandasArrayExtensionArray:_concat_same_type,PandasArrayExtensionArray:_concat_same_type,method,4,9,9,69,7.67,0,0,"['cls', 'to_concat']","[None, ' Sequence_[""PandasArrayExtensionArray""]']","[None, None]",489,[]
parser/test3/features.py:PandasArrayExtensionArray:dtype,PandasArrayExtensionArray:dtype,method,2,2,2,17,8.5,0,0,['self'],[None],[None],494,[]
parser/test3/features.py:PandasArrayExtensionArray:nbytes,PandasArrayExtensionArray:nbytes,method,2,2,2,23,11.5,0,0,['self'],[None],[None],498,[]
parser/test3/features.py:PandasArrayExtensionArray:isna,PandasArrayExtensionArray:isna,method,2,6,6,54,9.0,0,0,['self'],[None],[None],501,[]
parser/test3/features.py:PandasArrayExtensionArray:__setitem__,PandasArrayExtensionArray:__setitem__,method,1,2,2,26,13.0,0,0,"['self', 'key', 'slice', 'np.ndarray]', 'value']","[None, ' Union[int', None, None, ' Any']","[None, None, None, None, None]",504,[]
parser/test3/features.py:PandasArrayExtensionArray:__getitem__,PandasArrayExtensionArray:__getitem__,method,4,8,7,107,13.38,0,1,"['self', 'item', 'slice', 'np.ndarray]']","[None, ' Union[int', None, None]","[None, None, None, None]",507,[]
parser/test3/features.py:PandasArrayExtensionArray:take,PandasArrayExtensionArray:take,method,19,76,63,682,8.97,0,3,"['self', 'indices', 'allow_fill', 'fill_value']","[None, ' Sequence_[int]', ' bool ', ' bool ']","[None, None, ' False', ' None']",512,[]
parser/test3/features.py:PandasArrayExtensionArray:__len__,PandasArrayExtensionArray:__len__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],535,[]
parser/test3/features.py:PandasArrayExtensionArray:__eq__,PandasArrayExtensionArray:__eq__,method,3,14,14,161,11.5,0,1,"['self', 'other']","[None, None]","[None, None]",538,[]
parser/test3/features.py:ClassLabel:__post_init__,ClassLabel:__post_init__,method,12,110,64,839,7.63,1,5,['self'],[None],[None],260,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']"
parser/test3/features.py:ClassLabel:__call__,ClassLabel:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],267,[]
parser/test3/features.py:ClassLabel:str2int,ClassLabel:str2int,method,22,79,58,590,7.47,1,5,"['self', 'values', 'Iterable]']","[None, ' Union[str', None]","[None, None, None]",609,"['        """"""Conversion class name string => integer.""""""\n']"
parser/test3/features.py:ClassLabel:int2str,ClassLabel:int2str,method,18,71,52,449,6.32,1,4,"['self', 'values', 'Iterable]']","[None, ' Union[int', None]","[None, None, None]",637,"['        """"""Conversion integer => class name string.""""""\n']"
parser/test3/features.py:ClassLabel:encode_example,ClassLabel:encode_example,method,6,50,43,375,7.5,0,3,"['self', 'example_data']","[None, None]","[None, None]",658,[]
parser/test3/features.py:ClassLabel:_load_names_from_file,ClassLabel:_load_names_from_file,method,3,18,18,130,7.22,0,0,['names_filepath'],[None],[None],677,[]
parser/test3/features.py:Translation:__call__,Translation:__call__,method,2,7,7,66,9.43,0,0,['self'],[None],[None],267,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']"
parser/test3/features.py:TranslationVariableLanguages:__post_init__,TranslationVariableLanguages:__post_init__,method,2,12,8,141,11.75,0,0,['self'],[None],[None],260,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']"
parser/test3/features.py:TranslationVariableLanguages:__call__,TranslationVariableLanguages:__call__,method,2,5,5,87,17.4,0,0,['self'],[None],[None],267,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']"
parser/test3/features.py:TranslationVariableLanguages:encode_example,TranslationVariableLanguages:encode_example,method,15,54,46,521,9.65,1,2,"['self', 'translation_dict']","[None, None]","[None, None]",771,[]
parser/test3/features.py:Features:type,Features:type,method,2,2,2,27,13.5,0,0,['self'],[None],[None],955,"['        """"""\n', '        Features field types.\n', '\n', '        Returns:\n', '            :obj:`pyarrow.DataType`\n', '        """"""\n']"
parser/test3/features.py:Features:from_arrow_schema,Features:from_arrow_schema,method,4,9,9,89,9.89,0,0,"['cls', 'pa_schema']","[None, ' pa.Schema']","[None, None]",965,"['        """"""\n', '        Construct Features from Arrow Schema.\n', '\n', '        Args:\n', '            pa_schema (:obj:`pyarrow.Schema`): Arrow Schema.\n', '\n', '        Returns:\n', '            :class:`Features`\n', '        """"""\n']"
parser/test3/features.py:Features:from_dict,Features:from_dict,method,4,4,4,44,11.0,0,0,"['cls', 'dic']","[None, None]","[None, None]",979,"['        """"""\n', '        Construct Features from dict.\n', '\n', '        Regenerate the nested feature object from a deserialized dict.\n', ""        We use the '_type' key to infer the dataclass name of the feature FieldType.\n"", '\n', '        It allows for a convenient constructor syntax\n', '        to define features from deserialized JSON dictionaries. This function is used in particular when deserializing\n', '        a :class:`DatasetInfo` that was dumped to a JSON object. This acts as an analogue to\n', ""        :meth:`Features.from_arrow_schema` and handles the recursive field-by-field instantiation, but doesn't require\n"", '        any mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive\n', '        dtypes that :class:`Value` automatically performs.\n', '\n', '        Args:\n', '            dic (:obj:`dict[str, Any]`): Python dictionary.\n', '\n', '        Returns:\n', '            :class:`Features`\n', '\n', '        Examples:\n', ""            >>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})\n"", ""            {'_type': Value(dtype='string', id=None)}\n"", '        """"""\n']"
parser/test3/features.py:Features:encode_example,Features:encode_example,method,4,5,5,81,16.2,0,0,"['self', 'example']","[None, None]","[None, None]",1006,"['        """"""\n', '        Encode example into a format for Arrow.\n', '\n', '        Args:\n', '            example (:obj:`dict[str, Any]`): Data in a Dataset row.\n', '\n', '        Returns:\n', '            :obj:`dict[str, Any]`\n', '        """"""\n']"
parser/test3/features.py:Features:encode_batch,Features:encode_batch,method,8,31,27,291,9.39,1,1,"['self', 'batch']","[None, None]","[None, None]",1019,"['        """"""\n', '        Encode batch into a format for Arrow.\n', '\n', '        Args:\n', '            batch (:obj:`dict[str, list[Any]]`): Data in a Dataset batch.\n', '\n', '        Returns:\n', '            :obj:`dict[str, list[Any]]`\n', '        """"""\n']"
parser/test3/features.py:Features:copy,Features:copy,method,2,2,2,25,12.5,0,0,['self'],[None],[None],1037,"['        """"""\n', '        Make a deep copy of Features.\n', '\n', '        Returns:\n', '            :class:`Features`\n', '        """"""\n']"
parser/test3/features.py:Features:reorder_fields_as,Features:reorder_fields_as,method,22,154,76,1340,8.7,2,8,"['self', 'other']","[None, ' ""Features""']","[None, None]",1046,"['        """"""\n', '        Reorder Features fields to match the field order of other Features.\n', '\n', '        The order of the fields is important since it matters for the underlying arrow data.\n', '        Re-ordering the fields allows to make the underlying arrow data type match.\n', '\n', '        Args:\n', '            other (:class:`Features`): The other Features to align with.\n', '\n', '        Returns:\n', '            :class:`Features`\n', '\n', '        Examples:\n', '\n', '            >>> from datasets import Features, Sequence, Value\n', ""            >>> # let's say we have to features with a different order of nested fields (for a and b for example)\n"", '            >>> f1 = Features({""root"": Sequence({""a"": Value(""string""), ""b"": Value(""string"")})})\n', '            >>> f2 = Features({""root"": {""b"": Sequence(Value(""string"")), ""a"": Sequence(Value(""string""))}})\n', '            >>> assert f1.type != f2.type\n', '            >>> # re-ordering keeps the base structure (here Sequence is defined at the root level), but make the fields order match\n', '            >>> f1.reorder_fields_as(f2)\n', ""            {'root': Sequence(feature={'b': Value(dtype='string', id=None), 'a': Value(dtype='string', id=None)}, length=-1, id=None)}\n"", '            >>> assert f1.reorder_fields_as(f2).type == f2.type\n', '        """"""\n']"
parser/test3/fingerprint.py:maybe_register_dataset_for_temp_dir_deletion,maybe_register_dataset_for_temp_dir_deletion,function,8,23,18,363,15.78,1,3,['dataset'],[None],[None],72,"['    """"""\n', '    This function registers the datasets that have cache files in _TEMP_DIR_FOR_TEMP_CACHE_FILES in order\n', '    to properly delete them before deleting the temporary directory.\n', '    The temporary directory _TEMP_DIR_FOR_TEMP_CACHE_FILES is used when caching is disabled.\n', '    """"""\n']"
parser/test3/fingerprint.py:get_datasets_with_cache_file_in_temp_dir,get_datasets_with_cache_file_in_temp_dir,function,1,9,9,93,10.33,0,0,[],[],[],91,[]
parser/test3/fingerprint.py:set_caching_enabled,set_caching_enabled,function,2,4,4,53,13.25,0,0,['boolean'],[' bool'],[None],95,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use :func:`datasets.Dataset.save_to_disk` to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect :func:`datasets.load_dataset`. If you want to regenerate a dataset from scratch you should use\n"", '    the ``download_mode`` parameter in :func:`datasets.load_dataset`.\n', '    """"""\n']"
parser/test3/fingerprint.py:is_caching_enabled,is_caching_enabled,function,3,4,4,51,12.75,0,0,[],[],[],116,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use :func:`datasets.Dataset.save_to_disk` to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect :func:`datasets.load_dataset`. If you want to regenerate a dataset from scratch you should use\n"", '    the ``download_mode`` parameter in :func:`datasets.load_dataset`.\n', '    """"""\n']"
parser/test3/fingerprint.py:get_temporary_cache_files_directory,get_temporary_cache_files_directory,function,8,17,16,289,17.0,1,1,[],[],[],137,"['    """"""Return a directory that is deleted when session closes.""""""\n']"
parser/test3/fingerprint.py:hashregister,hashregister,function,6,12,10,75,6.25,1,0,['*types'],[None],[None],157,[]
parser/test3/fingerprint.py:_hash_pa_table,_hash_pa_table,function,5,24,20,332,13.83,0,1,"['hasher', 'value']","[None, None]","[None, None]",209,[]
parser/test3/fingerprint.py:_hash_dataset_info,_hash_dataset_info,function,2,3,3,81,27.0,0,0,"['hasher', 'value']","[None, None]","[None, None]",221,[]
parser/test3/fingerprint.py:generate_fingerprint,generate_fingerprint,function,12,21,18,255,12.14,2,1,['dataset'],[None],[None],233,[]
parser/test3/fingerprint.py:generate_random_fingerprint,generate_random_fingerprint,function,2,2,2,49,24.5,0,0,['nbits'],[None],['64'],247,[]
parser/test3/fingerprint.py:update_fingerprint,update_fingerprint,function,23,284,106,2120,7.46,1,4,"['fingerprint', 'transform', 'transform_args']","[None, None, None]","[None, None, None]",251,[]
parser/test3/fingerprint.py:fingerprint_transform,fingerprint_transform,function,72,292,170,2659,9.11,6,11,"['inplace', 'use_kwargs', 'ignore_kwargs', 'fingerprint_names', 'randomized_function', '']","[' bool', ' Optional[List[str]] ', ' Optional[List[str]] ', ' Optional[List[str]] ', ' bool ', None]","[None, ' None', ' None', ' None', ' False', None]",301,"['    """"""\n', '    Wrapper for dataset transforms to update the dataset fingerprint using ``update_fingerprint``\n', '\n', '    Args:\n', '        inplace (``bool``):  If inplace is True, the fingerprint of the dataset is updated inplace.\n', '            Otherwise, a parameter ""new_fingerprint"" is passed to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset.\n', '        use_kwargs (Optional ``List[str]``): optional white list of argument names to take into account\n', '            to update the fingerprint to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset. By default all the arguments are used.\n', '        ignore_kwargs (Optional ``List[str]``): optional black list of argument names to take into account\n', '            to update the fingerprint. Note that ignore_kwargs prevails on use_kwargs.\n', '        fingerprint_names (Optional ``List[str]``, defaults to [""new_fingerprint""]):\n', '            If the dataset transforms is not inplace and returns a DatasetDict, then it can require\n', '            several fingerprints (one per dataset in the DatasetDict). By specifying fingerprint_names,\n', '            one fingerprint named after each element of fingerprint_names is going to be passed.\n', '        randomized_function (``bool``, defaults to False): If the dataset transform is random and has\n', '            optional parameters ""seed"" and ""generator"", then you can set randomized_function to True.\n', '            This way, even if users set ""seed"" and ""generator"" to None, then the fingerprint is\n', ""            going to be randomly generated depending on numpy's current state. In this case, the\n"", '            generator is set to np.random.default_rng(np.random.get_state()[1][0]).\n', '    """"""\n']"
parser/test3/fingerprint.py:_TempDirWithCustomCleanup,_TempDirWithCustomCleanup,class,17,28,25,487,17.39,0,2,[],[],[],48,[]
parser/test3/fingerprint.py:Hasher,Hasher,class,27,75,49,723,9.64,1,2,[],[],[],166,[]
parser/test3/fingerprint.py:_TempDirWithCustomCleanup:__init__,_TempDirWithCustomCleanup:__init__,method,10,11,11,201,18.27,0,0,"['self', 'cleanup_func', '*cleanup_func_args', '**cleanup_func_kwargs']","[None, None, None, None]","[None, 'None', None, None]",55,[]
parser/test3/fingerprint.py:_TempDirWithCustomCleanup:_cleanup,_TempDirWithCustomCleanup:_cleanup,method,3,5,5,126,25.2,0,1,['self'],[None],[None],62,[]
parser/test3/fingerprint.py:_TempDirWithCustomCleanup:cleanup,_TempDirWithCustomCleanup:cleanup,method,2,3,3,43,14.33,0,1,['self'],[None],[None],67,[]
parser/test3/fingerprint.py:Hasher:__init__,Hasher:__init__,method,2,2,2,21,10.5,0,0,['self'],[None],[None],171,[]
parser/test3/fingerprint.py:Hasher:hash_bytes,Hasher:hash_bytes,method,8,16,16,109,6.81,1,1,"['cls', 'value', 'List[bytes]]']","[None, ' Union[bytes', None]","[None, None, None]",175,[]
parser/test3/fingerprint.py:Hasher:hash_default,Hasher:hash_default,method,2,2,2,34,17.0,0,0,"['cls', 'value']","[None, ' Any']","[None, None]",183,[]
parser/test3/fingerprint.py:Hasher:hash,Hasher:hash,method,8,16,16,109,6.81,1,1,"['cls', 'value', 'List[bytes]]']","[None, ' Union[bytes', None]","[None, None, None]",187,"['    """"""\n', '    Wrapper for dataset transforms to update the dataset fingerprint using ``update_fingerprint``\n', '\n', '    Args:\n', '        inplace (``bool``):  If inplace is True, the fingerprint of the dataset is updated inplace.\n', '            Otherwise, a parameter ""new_fingerprint"" is passed to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset.\n', '        use_kwargs (Optional ``List[str]``): optional white list of argument names to take into account\n', '            to update the fingerprint to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset. By default all the arguments are used.\n', '        ignore_kwargs (Optional ``List[str]``): optional black list of argument names to take into account\n', '            to update the fingerprint. Note that ignore_kwargs prevails on use_kwargs.\n', '        fingerprint_names (Optional ``List[str]``, defaults to [""new_fingerprint""]):\n', '            If the dataset transforms is not inplace and returns a DatasetDict, then it can require\n', '            several fingerprints (one per dataset in the DatasetDict). By specifying fingerprint_names,\n', '            one fingerprint named after each element of fingerprint_names is going to be passed.\n', '        randomized_function (``bool``, defaults to False): If the dataset transform is random and has\n', '            optional parameters ""seed"" and ""generator"", then you can set randomized_function to True.\n', '            This way, even if users set ""seed"" and ""generator"" to None, then the fingerprint is\n', ""            going to be randomly generated depending on numpy's current state. In this case, the\n"", '            generator is set to np.random.default_rng(np.random.get_state()[1][0]).\n', '    """"""\n']"
parser/test3/fingerprint.py:Hasher:update,Hasher:update,method,5,6,6,168,28.0,0,0,"['self', 'value']","[None, ' Any']","[None, None]",193,[]
parser/test3/fingerprint.py:Hasher:hexdigest,Hasher:hexdigest,method,2,2,2,24,12.0,0,0,['self'],[None],[None],199,[]
parser/test3/hf_api.py:ObjectInfo,ObjectInfo,class,37,88,68,760,8.64,1,0,[],[],[],29,[]
parser/test3/hf_api.py:HfApi,HfApi,class,22,103,52,838,8.14,4,7,[],[],[],64,[]
parser/test3/hf_api.py:ObjectInfo:__init__,ObjectInfo:__init__,method,31,44,36,293,6.66,1,0,"['self', 'id', 'key', 'lastModified', 'description', 'citation', 'size', 'etag', 'siblings', 'author', '**kwargs', '']","[None, ' str', ' str', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' List[Dict] ', ' str ', None, None]","[None, None, None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', None, None]",34,[]
parser/test3/hf_api.py:ObjectInfo:__repr__,ObjectInfo:__repr__,method,4,12,12,211,17.58,0,0,['self'],[None],[None],59,[]
parser/test3/hf_api.py:HfApi:__init__,HfApi:__init__,method,3,9,8,53,5.89,0,1,"['self', 'endpoint']","[None, None]","[None, 'None']",67,"['        """"""Create Api using a specific endpoint and also the file types (\'datasets\' or \'metrics\')""""""\n']"
parser/test3/hf_api.py:HfApi:dataset_list,HfApi:dataset_list,method,11,37,23,258,6.97,2,3,"['self', 'with_community_datasets', 'id_only']","[None, None, None]","[None, 'True', 'False']",71,"['        """"""\n', '        Get the public list of all the datasets on huggingface, including the community datasets\n', '        """"""\n']"
parser/test3/hf_api.py:HfApi:metric_list,HfApi:metric_list,method,12,37,24,250,6.76,2,3,"['self', 'with_community_metrics', 'id_only']","[None, None, None]","[None, 'True', 'False']",86,"['        """"""\n', '        Get the public list of all the metrics on huggingface, including the community metrics\n', '        """"""\n']"
parser/test3/info.py:SupervisedKeysData,SupervisedKeysData,class,3,6,5,26,4.33,0,0,[],[],[],53,[]
parser/test3/info.py:DownloadChecksumsEntryData,DownloadChecksumsEntryData,class,3,6,5,23,3.83,0,0,[],[],[],59,[]
parser/test3/info.py:MissingCachedSizesConfigError,MissingCachedSizesConfigError,class,0,0,0,0,0.0,0,0,[],[],[],64,[]
parser/test3/info.py:NonMatchingCachedSizesError,NonMatchingCachedSizesError,class,0,0,0,0,0.0,0,0,[],[],[],68,[]
parser/test3/info.py:PostProcessedInfo,PostProcessedInfo,class,13,44,35,421,9.57,0,1,[],[],[],73,[]
parser/test3/info.py:DatasetInfo,DatasetInfo,class,93,426,217,4852,11.39,5,20,[],[],[],89,[]
parser/test3/info.py:DatasetInfosDict,DatasetInfosDict,class,20,77,60,1024,13.3,1,1,[],[],[],283,[]
parser/test3/info.py:MetricInfo,MetricInfo,class,41,165,116,1619,9.81,1,3,[],[],[],308,[]
parser/test3/info.py:PostProcessedInfo:__post_init__,PostProcessedInfo:__post_init__,method,4,11,9,113,10.27,0,1,['self'],[None],[None],77,[]
parser/test3/info.py:PostProcessedInfo:from_dict,PostProcessedInfo:from_dict,method,3,18,14,127,7.06,0,0,"['cls', 'post_processed_info_dict']","[None, ' dict']","[None, None]",83,[]
parser/test3/info.py:DatasetInfo:__post_init__,DatasetInfo:__post_init__,method,24,146,67,1699,11.64,2,14,['self'],[None],[None],77,[]
parser/test3/info.py:DatasetInfo:_license_path,DatasetInfo:_license_path,method,2,3,3,60,20.0,0,0,"['self', 'dataset_info_dir']","[None, None]","[None, None]",184,[]
parser/test3/info.py:DatasetInfo:write_to_directory,DatasetInfo:write_to_directory,method,4,14,9,193,13.79,0,0,"['self', 'dataset_info_dir']","[None, None]","[None, None]",187,"['        """"""Write `DatasetInfo` as JSON to `dataset_info_dir`.\n', '\n', '        Also save the license separately in LICENCE.\n', '        """"""\n']"
parser/test3/info.py:DatasetInfo:_dump_info,DatasetInfo:_dump_info,method,1,1,1,52,52.0,0,0,"['self', 'file']","[None, None]","[None, None]",198,"['        """"""Dump info in `file` file-like object open in bytes mode (to support remote files)""""""\n']"
parser/test3/info.py:DatasetInfo:_dump_license,DatasetInfo:_dump_license,method,1,1,1,40,40.0,0,0,"['self', 'file']","[None, None]","[None, None]",202,"['        """"""Dump license in `file` file-like object open in bytes mode (to support remote files)""""""\n']"
parser/test3/info.py:DatasetInfo:from_merge,DatasetInfo:from_merge,method,19,94,54,1010,10.74,2,4,"['cls', 'dataset_infos']","[None, ' List[""DatasetInfo""]']","[None, None]",207,[]
parser/test3/info.py:DatasetInfo:from_directory,DatasetInfo:from_directory,method,9,26,25,329,12.65,0,1,"['cls', 'dataset_info_dir']","[None, ' str']","[None, None]",244,"['        """"""Create DatasetInfo from the JSON file in `dataset_info_dir`.\n', '\n', '        This function updates all the dynamically generated fields (num_examples,\n', '        hash, time of creation,...) of the DatasetInfo.\n', '\n', '        This will overwrite all previous metadata.\n', '\n', '        Args:\n', '            dataset_info_dir (`str`): The directory containing the metadata file. This\n', '                should be the root directory of a specific dataset version.\n', '        """"""\n']"
parser/test3/info.py:DatasetInfo:from_dict,DatasetInfo:from_dict,method,3,18,14,120,6.67,0,0,"['cls', 'dataset_info_dict']","[None, ' dict']","[None, None]",265,[]
parser/test3/info.py:DatasetInfo:update,DatasetInfo:update,method,6,21,20,143,6.81,1,1,"['self', 'other_dataset_info', 'ignore_none']","[None, ' ""DatasetInfo""', None]","[None, None, 'True']",269,[]
parser/test3/info.py:DatasetInfo:copy,DatasetInfo:copy,method,2,8,8,73,9.12,0,0,['self'],[None],[None],279,[]
parser/test3/info.py:DatasetInfosDict:write_to_directory,DatasetInfosDict:write_to_directory,method,10,45,39,575,12.78,0,1,"['self', 'dataset_infos_dir', 'overwrite']","[None, None, None]","[None, None, 'False']",284,[]
parser/test3/info.py:DatasetInfosDict:from_directory,DatasetInfosDict:from_directory,method,11,24,24,330,13.75,1,0,"['cls', 'dataset_infos_dir']","[None, None]","[None, None]",297,[]
parser/test3/info.py:MetricInfo:__post_init__,MetricInfo:__post_init__,method,6,50,44,320,6.4,1,2,['self'],[None],[None],77,"['    """"""Information about a dataset.\n', '\n', '    `DatasetInfo` documents datasets, including its name, version, and features.\n', '    See the constructor arguments and properties for a full list.\n', '\n', '    Note: Not all fields are known on construction and may be updated later.\n', '\n', '    Attributes:\n', '        description (str): A description of the dataset.\n', '        citation (str): A BibTeX citation of the dataset.\n', '        homepage (str): A URL to the official homepage for the dataset.\n', ""        license (str): The dataset's license. It can be the name of the license or a paragraph containing the terms of the license.\n"", ""        features (Features, optional): The features used to specify the dataset's column types.\n"", '        post_processed (PostProcessedInfo, optional): Information regarding the resources of a possible post-processing of a dataset. For example, it can contain the information of an index.\n', '        supervised_keys (SupervisedKeysData, optional): Specifies the input feature and the label for supervised learning if applicable for the dataset (legacy from TFDS).\n', '        builder_name (str, optional): The name of the :class:`GeneratorBasedBuilder` subclass used to create the dataset. Usually matched to the corresponding script name. It is also the snake_case version of the dataset builder class name.\n', '        config_name (str, optional): The name of the configuration derived from :class:`BuilderConfig`\n', '        version (str or Version, optional): The version of the dataset.\n', '        splits (dict, optional): The mapping between split name and metadata.\n', ""        download_checksums (dict, optional): The mapping between the URL to download the dataset's checksums and corresponding metadata.\n"", '        download_size (int, optional): The size of the files to download to generate the dataset, in bytes.\n', '        post_processing_size (int, optional): Size of the dataset in bytes after post-processing, if any.\n', '        dataset_size (int, optional): The combined size in bytes of the Arrow tables for all splits.\n', '        size_in_bytes (int, optional): The combined size in bytes of all files associated with the dataset (downloaded files + Arrow files).\n', ""        task_templates (List[TaskTemplate], optional): The task templates to prepare the dataset for during training and evaluation. Each template casts the dataset's :class:`Features` to standardized column names and types as detailed in :py:mod:`datasets.tasks`.\n"", '        **config_kwargs: Keyword arguments to be passed to the :class:`BuilderConfig` and used in the :class:`DatasetBuilder`.\n', '    """"""\n']"
parser/test3/info.py:MetricInfo:write_to_directory,MetricInfo:write_to_directory,method,4,17,11,229,13.47,0,0,"['self', 'metric_info_dir']","[None, None]","[None, None]",344,"['        """"""Write `MetricInfo` as JSON to `metric_info_dir`.\n', '        Also save the license separately in LICENCE.\n', '        """"""\n']"
parser/test3/info.py:MetricInfo:from_directory,MetricInfo:from_directory,method,9,26,25,320,12.31,0,1,"['cls', 'metric_info_dir']","[None, None]","[None, None]",355,"['        """"""Create MetricInfo from the JSON file in `metric_info_dir`.\n', '\n', '        Args:\n', '            metric_info_dir: `str` The directory containing the metadata file. This\n', '                should be the root directory of a specific dataset version.\n', '        """"""\n']"
parser/test3/info.py:MetricInfo:from_dict,MetricInfo:from_dict,method,3,18,14,119,6.61,0,0,"['cls', 'metric_info_dict']","[None, ' dict']","[None, None]",371,[]
parser/test3/inspect.py:list_datasets,list_datasets,function,4,6,6,113,18.83,0,0,"['with_community_datasets', 'with_details']","[None, None]","['True', 'False']",30,"['    """"""List all the datasets scripts available on HuggingFace AWS bucket.\n', '\n', '    Args:\n', '        with_community_datasets (``bool``, optional, default ``True``): Include the community provided datasets.\n', '        with_details (``bool``, optional, default ``False``): Return the full details on the datasets instead of only the short name.\n', '    """"""\n']"
parser/test3/inspect.py:list_metrics,list_metrics,function,4,6,6,110,18.33,0,0,"['with_community_metrics', 'with_details']","[None, None]","['True', 'False']",41,"['    """"""List all the metrics script available on HuggingFace AWS bucket\n', '\n', '    Args:\n', '        with_community_metrics (Optional ``bool``): Include the community provided metrics (default: ``True``)\n', '        with_details (Optional ``bool``): Return the full details on the metrics instead of only the short name (default: ``False``)\n', '    """"""\n']"
parser/test3/inspect.py:inspect_dataset,inspect_dataset,function,37,166,84,1254,7.55,0,0,"['path', 'local_path', 'download_config', '**download_kwargs']","[' str', ' str', ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",52,[]
parser/test3/inspect.py:get_dataset_config_names,get_dataset_config_names,function,24,70,50,542,7.74,0,0,['path'],[' str'],[None],115,"['    """"""Get the list of available config names for a particular dataset.\n', '\n', '    Args:\n', '        path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``\n"", '            - a dataset identifier on HuggingFace AWS bucket (list all available datasets and ids with ``datasets.list_datasets()``)\n', ""                e.g. ``'squad'``, ``'glue'`` or ``'openai/webtext'``\n"", '    """"""\n']"
parser/test3/iterable_dataset.py:_infer_features_from_batch,_infer_features_from_batch,function,6,16,15,227,14.19,0,1,"['batch', 'list]', 'try_features']","[' Dict[str', None, ' Optional[Features] ']","[None, None, ' None']",16,[]
parser/test3/iterable_dataset.py:_examples_to_batch,_examples_to_batch,function,6,16,14,134,8.38,1,0,"['examples', 'Any]]']","[' List[Dict[str', None]","[None, None]",26,[]
parser/test3/iterable_dataset.py:_batch_to_examples,_batch_to_examples,function,7,14,12,112,8.0,2,0,"['batch', 'list]']","[' Dict[str', None]","[None, None]",34,"['    """"""Convert a batch (dict of examples) to examples list""""""\n']"
parser/test3/iterable_dataset.py:_shuffle_kwargs,_shuffle_kwargs,function,8,20,16,201,10.05,1,1,"['rng', 'kwargs']","[' np.random.Generator', ' dict']","[None, None]",60,[]
parser/test3/iterable_dataset.py:_generate_examples_from_tables_wrapper,_generate_examples_from_tables_wrapper,function,12,21,19,235,11.19,2,0,['generate_tables_fn'],[None],[None],281,[]
parser/test3/iterable_dataset.py:iterable_dataset,iterable_dataset,function,8,27,25,299,11.07,0,1,"['ex_iterable', 'info', 'split', 'format_type', 'shuffling', '']","[' Iterable', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[str] ', ' Optional[ShuffingConfig] ', None]","[None, ' None', ' None', ' None', ' None', None]",472,[]
parser/test3/iterable_dataset.py:_BaseExamplesIterable,_BaseExamplesIterable,class,4,19,13,208,10.95,0,0,[],[],[],41,[]
parser/test3/iterable_dataset.py:ExamplesIterable,ExamplesIterable,class,14,48,40,521,10.85,1,0,[],[],[],72,[]
parser/test3/iterable_dataset.py:ShardShuffledExamplesIterable,ShardShuffledExamplesIterable,class,12,27,24,348,12.89,1,0,[],[],[],90,[]
parser/test3/iterable_dataset.py:CyclingMultiSourcesExamplesIterable,CyclingMultiSourcesExamplesIterable,class,28,77,60,727,9.44,2,1,[],[],[],103,[]
parser/test3/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable,RandomlyCyclingMultiSourcesExamplesIterable,class,35,121,92,1187,9.81,4,2,[],[],[],127,[]
parser/test3/iterable_dataset.py:MappedExamplesIterable,MappedExamplesIterable,class,28,87,73,931,10.7,1,1,[],[],[],165,[]
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable,BufferShuffledExamplesIterable,class,38,109,84,1085,9.95,2,1,[],[],[],205,[]
parser/test3/iterable_dataset.py:SkipExamplesIterable,SkipExamplesIterable,class,15,38,33,361,9.5,1,0,[],[],[],244,[]
parser/test3/iterable_dataset.py:TakeExamplesIterable,TakeExamplesIterable,class,12,31,26,307,9.9,0,0,[],[],[],264,[]
parser/test3/iterable_dataset.py:ShuffingConfig,ShuffingConfig,class,2,3,3,23,7.67,0,0,[],[],[],293,[]
parser/test3/iterable_dataset.py:IterableDataset,IterableDataset,class,48,204,116,2535,12.43,1,4,[],[],[],297,[]
parser/test3/iterable_dataset.py:_BaseExamplesIterable:__iter__,_BaseExamplesIterable:__iter__,method,1,2,2,26,13.0,0,0,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:_BaseExamplesIterable:shuffle_data_sources,_BaseExamplesIterable:shuffle_data_sources,method,1,2,2,26,13.0,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",48,"['        """"""\n', '        Either shuffle the shards/sources of the dataset, or propagate the shuffling to the underlying iterable.\n', '        If the order of the shards must stay fixed (when using .skip or .take for example), then this method returns self.\n', '        """"""\n']"
parser/test3/iterable_dataset.py:_BaseExamplesIterable:n_shards,_BaseExamplesIterable:n_shards,method,1,2,2,26,13.0,0,0,['self'],[None],[None],56,[]
parser/test3/iterable_dataset.py:ExamplesIterable:__init__,ExamplesIterable:__init__,method,4,4,4,65,16.25,0,0,"['self', 'generate_examples_fn', 'kwargs']","[None, ' Callable', ' dict']","[None, None, None]",73,[]
parser/test3/iterable_dataset.py:ExamplesIterable:__iter__,ExamplesIterable:__iter__,method,3,8,6,74,9.25,1,0,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:ExamplesIterable:shuffle_data_sources,ExamplesIterable:shuffle_data_sources,method,2,4,4,79,19.75,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",81,[]
parser/test3/iterable_dataset.py:ExamplesIterable:n_shards,ExamplesIterable:n_shards,method,3,13,13,116,8.92,0,0,['self'],[None],[None],56,[]
parser/test3/iterable_dataset.py:ShardShuffledExamplesIterable:__init__,ShardShuffledExamplesIterable:__init__,method,3,4,4,60,15.0,0,0,"['self', 'generate_examples_fn', 'kwargs', 'seed']","[None, ' Callable', ' dict', ' Optional[int]']","[None, None, None, None]",91,[]
parser/test3/iterable_dataset.py:ShardShuffledExamplesIterable:__iter__,ShardShuffledExamplesIterable:__iter__,method,7,13,11,188,14.46,1,0,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:__init__,CyclingMultiSourcesExamplesIterable:__init__,method,2,2,2,30,15.0,0,0,"['self', 'ex_iterables']","[None, ' List[_BaseExamplesIterable]']","[None, None]",104,[]
parser/test3/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:__iter__,CyclingMultiSourcesExamplesIterable:__iter__,method,19,44,38,286,6.5,2,1,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:shuffle_data_sources,CyclingMultiSourcesExamplesIterable:shuffle_data_sources,method,3,8,8,142,17.75,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",117,"['        """"""Shuffle each underlying examples iterable.""""""\n']"
parser/test3/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:n_shards,CyclingMultiSourcesExamplesIterable:n_shards,method,2,6,6,64,10.67,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:__init__,RandomlyCyclingMultiSourcesExamplesIterable:__init__,method,5,5,5,78,15.6,0,0,"['self', 'ex_iterables', 'seed', 'probabilities']","[None, None, ' Optional[int] ', ' Optional[List[float]] ']","[None, None, ' None', ' None']",128,[]
parser/test3/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:_iter_random_indices,RandomlyCyclingMultiSourcesExamplesIterable:_iter_random_indices,method,2,27,19,186,6.89,2,1,"['rng', 'num_sources', 'random_batch_size', 'p', '']","[' np.random.Generator', ' int', None, ' Optional[List[float]] ', None]","[None, None, '1000', ' None', None]",134,"['        """"""Get an infinite iterator that randomly samples the index of the source to pick examples from.""""""\n']"
parser/test3/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:__iter__,RandomlyCyclingMultiSourcesExamplesIterable:__iter__,method,21,48,42,361,7.52,2,1,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:shuffle_data_sources,RandomlyCyclingMultiSourcesExamplesIterable:shuffle_data_sources,method,3,10,10,193,19.3,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",159,"['        """"""Shuffle the data sources of each wrapped examples iterable.""""""\n']"
parser/test3/iterable_dataset.py:MappedExamplesIterable:__init__,MappedExamplesIterable:__init__,method,8,8,8,99,12.38,0,0,"['self', 'ex_iterable', 'function', 'batched', 'batch_size']","[None, ' _BaseExamplesIterable', ' Callable', ' bool ', ' int ']","[None, None, None, ' False', ' 1000']",166,[]
parser/test3/iterable_dataset.py:MappedExamplesIterable:__iter__,MappedExamplesIterable:__iter__,method,15,44,36,432,9.82,1,1,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:MappedExamplesIterable:shuffle_data_sources,MappedExamplesIterable:shuffle_data_sources,method,2,6,6,125,20.83,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",194,"['        """"""Shuffle the wrapped examples iterable.""""""\n']"
parser/test3/iterable_dataset.py:MappedExamplesIterable:n_shards,MappedExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable:__init__,BufferShuffledExamplesIterable:__init__,method,6,6,6,72,12.0,0,0,"['self', 'ex_iterable', 'buffer_size', 'seed']","[None, ' _BaseExamplesIterable', ' int', ' Optional[int]']","[None, None, None, None]",206,[]
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable:_iter_random_indices,BufferShuffledExamplesIterable:_iter_random_indices,method,1,11,11,84,7.64,1,0,"['rng', 'buffer_size', 'random_batch_size']","[' np.random.Generator', ' int', None]","[None, None, '1000']",212,[]
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable:__iter__,BufferShuffledExamplesIterable:__iter__,method,27,54,44,429,7.94,1,1,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable:shuffle_data_sources,BufferShuffledExamplesIterable:shuffle_data_sources,method,2,6,6,122,20.33,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",233,"['        """"""Shuffle the wrapped examples iterable as well as the shuffling buffer.""""""\n']"
parser/test3/iterable_dataset.py:BufferShuffledExamplesIterable:n_shards,BufferShuffledExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:SkipExamplesIterable:__init__,SkipExamplesIterable:__init__,method,4,4,4,37,9.25,0,0,"['self', 'ex_iterable', 'n']","[None, ' _BaseExamplesIterable', ' int']","[None, None, None]",245,[]
parser/test3/iterable_dataset.py:SkipExamplesIterable:__iter__,SkipExamplesIterable:__iter__,method,5,11,10,94,8.55,1,0,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:SkipExamplesIterable:shuffle_data_sources,SkipExamplesIterable:shuffle_data_sources,method,1,2,2,10,5.0,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",255,"['        """"""Doesn\'t shuffle the wrapped examples iterable since it would skip exampels from other shards instead.""""""\n']"
parser/test3/iterable_dataset.py:SkipExamplesIterable:n_shards,SkipExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:TakeExamplesIterable:__init__,TakeExamplesIterable:__init__,method,4,4,4,37,9.25,0,0,"['self', 'ex_iterable', 'n']","[None, ' _BaseExamplesIterable', ' int']","[None, None, None]",245,[]
parser/test3/iterable_dataset.py:TakeExamplesIterable:__iter__,TakeExamplesIterable:__iter__,method,2,4,4,40,10.0,0,0,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:TakeExamplesIterable:shuffle_data_sources,TakeExamplesIterable:shuffle_data_sources,method,1,2,2,10,5.0,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",272,"['        """"""Doesn\'t shuffle the wrapped examples iterable since it would take examples from other shards instead.""""""\n']"
parser/test3/iterable_dataset.py:TakeExamplesIterable:n_shards,TakeExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:IterableDataset:__init__,IterableDataset:__init__,method,10,20,19,202,10.1,0,0,"['self', 'ex_iterable', 'info', 'split', 'format_type', 'shuffling', '']","[None, ' _BaseExamplesIterable', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[str] ', ' Optional[ShuffingConfig] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",166,"['        """"""Shuffle the wrapped examples iterable.""""""\n']"
parser/test3/iterable_dataset.py:IterableDataset:_head,IterableDataset:_head,method,2,8,8,61,7.62,0,0,"['self', 'n']","[None, None]","[None, '5']",316,[]
parser/test3/iterable_dataset.py:IterableDataset:_effective_seed,IterableDataset:_effective_seed,method,4,15,11,113,7.53,0,2,['self'],[None],[None],320,[]
parser/test3/iterable_dataset.py:IterableDataset:n_shards,IterableDataset:n_shards,method,2,2,2,32,16.0,0,0,['self'],[None],[None],56,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']"
parser/test3/iterable_dataset.py:IterableDataset:_iter,IterableDataset:_iter,method,5,10,8,148,14.8,0,1,['self'],[None],[None],330,[]
parser/test3/iterable_dataset.py:IterableDataset:__iter__,IterableDataset:__iter__,method,5,12,10,108,9.0,1,1,['self'],[None],[None],44,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']"
parser/test3/iterable_dataset.py:IterableDataset:with_format,IterableDataset:with_format,method,5,8,8,167,20.88,0,0,"['self', 'type', '']","[None, ' Optional[str] ', None]","[None, ' None', None]",345,"['        """"""\n', '        Return a dataset with the specified format.\n', '        This method only supports the ""torch"" format for now.\n', '\n', '        Args:\n', '\n', '            type (:obj:`str`, optional, default None): if set to ""torch"", the returned dataset\n', '                will be a subclass of torch.utils.data.IterableDataset to be used in a DataLoader\n', '        """"""\n']"
parser/test3/iterable_dataset.py:IterableDataset:map,IterableDataset:map,method,7,19,19,315,16.58,0,0,"['self', 'function', 'batched', 'batch_size']","[None, ' Callable', ' bool ', ' int ']","[None, None, ' False', ' 1000']",370,"['        """"""\n', '        Return a dataset with the specified map function. The function is applied on-the-fly on the examples when iterating over the dataset.\n', '\n', '        You can specify whether the function should be batched or not with the ``batched`` parameter:\n', '\n', '        - If batched is False, then the function takes 1 example in and should return 1 example.\n', '          An example is a dictionary, e.g. {""text"": ""Hello there !""}\n', '        - If batched is True and batch_size is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n', '          A batch is a dictionary, e.g. a batch of 1 example is {""text"": [""Hello there !""]}\n', '        - If batched is True and batch_size is ``n`` > 1, then the function takes a batch of ``n`` examples as input and can return a batch with ``n`` examples, or with an arbitrary number of examples.\n', '          Note that the last batch may have less than ``n`` examples.\n', '          A batch is a dictionary, e.g. a batch of ``n`` examples is {""text"": [""Hello there !""] * n}\n', '\n', '        Args:\n', '            function (:obj:`Callable`, optional, default None): if not None, this function is applied\n', '                on-the-fly on the examples when you iterate on the dataset.\n', '            batched (:obj:`bool`, default `False`): Provide batch of examples to `function`.\n', '            batch_size (:obj:`int`, optional, default ``1000``): Number of examples per batch provided to `function` if `batched=True`.\n', '\n', '        """"""\n']"
parser/test3/iterable_dataset.py:IterableDataset:shuffle,IterableDataset:shuffle,method,9,14,14,283,20.21,0,0,"['self', 'buffer_size', 'seed']","[None, None, None]","[None, None, 'None']",404,"['        """"""\n', '        Randomly shuffles the elements of this dataset.\n', '\n', '        This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer,\n', '        replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or\n', '        equal to the full size of the dataset is required.\n', '\n', '        For instance, if your dataset contains 10,000 elements but ``buffer_size`` is set to 1,000, then shuffle will\n', '        initially select a random element from only the first 1,000 elements in the buffer. Once an element is\n', '        selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n', '        maintaining the 1,000 element buffer.\n', '\n', '        If the dataset is made of several shards, it also does shuffle the order of the shards.\n', '        However if the order has been fixed by using :func:`datasets.IterableDataset.skip` or :func:`datasets.IterableDataset.take`\n', '        then the order of the shards is kept unchanged.\n', '\n', '        Args:\n', '            buffer_size (:obj:`int`): size of the buffer.\n', '            seed (:obj:`int`, optional, default None): random seed that will be used to create the distribution.\n', '        """"""\n']"
parser/test3/iterable_dataset.py:IterableDataset:set_epoch,IterableDataset:set_epoch,method,2,2,2,17,8.5,0,0,"['self', 'epoch']","[None, ' int']","[None, None]",436,[]
parser/test3/iterable_dataset.py:IterableDataset:skip,IterableDataset:skip,method,7,11,11,228,20.73,0,0,"['self', 'n']","[None, None]","[None, None]",439,"['        """"""\n', '        Create a new IterableDataset that skips the first ``n`` elements.\n', '\n', '        Args:\n', '            n (:obj:`int`): number of elements to skip.\n', '        """"""\n']"
parser/test3/iterable_dataset.py:IterableDataset:take,IterableDataset:take,method,7,11,11,228,20.73,0,0,"['self', 'n']","[None, None]","[None, None]",455,"['        """"""\n', '        Create a new IterableDataset with only the first ``n`` elements.\n', '\n', '        Args:\n', '            n (:obj:`int`): number of elements to take.\n', '        """"""\n']"
parser/test3/keyhash.py:_as_bytes,_as_bytes,function,7,21,16,241,11.48,0,1,"['hash_data', 'int', 'bytes]']","[' Union[str', None, None]","[None, None, None]",38,"['    """"""\n', '    Returns the input hash_data in its bytes form\n', '\n', '    Args:\n', '    hash_data: the hash salt/key to be converted to bytes\n', '    """"""\n']"
parser/test3/keyhash.py:InvalidKeyError,InvalidKeyError,class,9,30,29,268,8.93,0,0,[],[],[],61,[]
parser/test3/keyhash.py:DuplicatedKeysError,DuplicatedKeysError,class,5,24,24,225,9.38,0,0,[],[],[],71,[]
parser/test3/keyhash.py:KeyHasher,KeyHasher,class,10,22,21,229,10.41,0,0,[],[],[],81,[]
parser/test3/keyhash.py:InvalidKeyError:__init__,InvalidKeyError:__init__,method,8,27,26,239,8.85,0,0,"['self', 'hash_data']","[None, None]","[None, None]",64,[]
parser/test3/keyhash.py:DuplicatedKeysError:__init__,DuplicatedKeysError:__init__,method,4,21,21,202,9.62,0,0,"['self', 'key']","[None, None]","[None, None]",74,[]
parser/test3/keyhash.py:KeyHasher:__init__,KeyHasher:__init__,method,2,2,2,49,24.5,0,0,"['self', 'hash_salt']","[None, ' str']","[None, None]",84,[]
parser/test3/keyhash.py:KeyHasher:hash,KeyHasher:hash,method,6,8,8,101,12.62,0,0,"['self', 'key', 'int', 'bytes]']","[None, ' Union[str', None, None]","[None, None, None, None]",87,"['        """"""Returns 128-bits unique hash of input key\n', '\n', '        Args:\n', '        key: the input key to be hashed (should be str, int or bytes)\n', '\n', '        Returns: 128-bit int hash key""""""\n']"
parser/test3/load.py:init_dynamic_modules,init_dynamic_modules,function,9,18,17,322,17.89,0,1,"['name', 'hf_modules_cache', 'str]] ']","[' str ', ' Optional[Union[Path', None]","[' config.MODULE_NAME_FOR_DYNAMIC_MODULES', None, ' None']",68,"['    """"""\n', '    Create a module with name `name` in which you can add dynamic modules\n', '    such as metrics or datasets. The module can be imported using its name.\n', '    The module is created in the HF_MODULE_CACHE directory by default (~/.cache/huggingface/modules) but it can\n', '    be overriden by specifying a path to another directory in `hf_modules_cache`.\n', '    """"""\n']"
parser/test3/load.py:import_main_class,import_main_class,function,14,28,23,294,10.5,1,3,"['module_path', 'dataset']","[None, None]","[None, 'True']",86,"['    """"""Import a module at module_path and return its main class:\n', '    - a DatasetBuilder if dataset is True\n', '    - a Metric if dataset is False\n', '    """"""\n']"
parser/test3/load.py:files_to_hash,files_to_hash,function,13,25,22,322,12.88,2,1,['file_paths'],[' List[str]'],[None],110,"['    """"""\n', '    Convert a list of scripts or text files provided in file_paths into a hashed filename in a repeatable way.\n', '    """"""\n']"
parser/test3/load.py:convert_github_url,convert_github_url,function,17,68,59,616,9.06,0,2,['url_path'],[' str'],[None],130,"['    """"""Convert a link to a file on a github repo in a link to the raw github object.""""""\n']"
parser/test3/load.py:get_imports,get_imports,function,147,471,254,3227,6.85,1,3,['file_path'],[' str'],[None],150,[]
parser/test3/load.py:load_dataset_builder,load_dataset_builder,function,11,39,36,598,15.33,0,1,"['path', 'name', 'data_dir', 'data_files', 'List] ', 'cache_dir', 'features', 'download_config', 'download_mode', 'script_version', 'Version]] ', 'use_auth_token', 'str]] ', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[str] ', ' Union[Dict', None, ' Optional[str] ', ' Optional[Features] ', ' Optional[DownloadConfig] ', ' Optional[GenerateMode] ', ' Optional[Union[str', None, ' Optional[Union[bool', None, None, None]","[None, ' None', ' None', None, ' None', ' None', ' None', ' None', ' None', None, ' None', None, ' None', None, None]",629,"['    """"""Load a builder for the dataset. A dataset builder can be used to inspect general information that is required to build a dataset (cache directory, config, dataset info, etc.)\n', '    without downloading the dataset itself.\n', '\n', ""    This method will download and import the dataset loading script from ``path`` if it's not already cached inside the library.\n"", '\n', '    Args:\n', '\n', '        path (:obj:`str`): Path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""              e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``.\n"", '            - a dataset identifier in the HuggingFace Datasets Hub (list all available datasets and ids with ``datasets.list_datasets()``)\n', ""              e.g. ``'squad'``, ``'glue'`` or ``'openai/webtext'``.\n"", '        name (:obj:`str`, optional): Defining the name of the dataset configuration.\n', '        data_dir (:obj:`str`, optional): Defining the data_dir of the dataset configuration.\n', '        data_files (:obj:`str`, optional): Defining the data_files of the dataset configuration.\n', '        cache_dir (:obj:`str`, optional): Directory to read/write data. Defaults to ""~/datasets"".\n', '        features (:class:`Features`, optional): Set the features type to use for this dataset.\n', '        download_config (:class:`~utils.DownloadConfig`, optional): Specific download configuration parameters.\n', '        download_mode (:class:`GenerateMode`, optional): Select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS\n', '        script_version (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load:\n', '\n', '            - For canonical datasets in the `huggingface/datasets` library like ""squad"", the default version of the module is the local version fo the lib.\n', '              You can specify a different version from your local version of the lib (e.g. ""master"" or ""1.2.0"") but it might cause compatibility issues.\n', '            - For community provided datasets like ""lhoestq/squad"" that have their own git repository on the Datasets Hub, the default version ""main"" corresponds to the ""main"" branch.\n', '              You can specify a different version that the default ""main"" by using a commit sha or a git tag of the dataset repository.\n', '        use_auth_token (``str`` or ``bool``, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If True, will get token from `""~/.huggingface""`.\n', '\n', '    Returns:\n', '        :class:`DatasetBuilder`\n', '\n', '    """"""\n']"
parser/test3/load.py:load_dataset,load_dataset,function,12,93,71,1179,12.68,0,1,"['path', 'name', 'data_dir', 'data_files', 'List] ', 'cache_dir', 'features', 'download_config', 'download_mode', 'script_version', 'Version]] ', 'use_auth_token', 'str]] ', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[str] ', ' Union[Dict', None, ' Optional[str] ', ' Optional[Features] ', ' Optional[DownloadConfig] ', ' Optional[GenerateMode] ', ' Optional[Union[str', None, ' Optional[Union[bool', None, None, None]","[None, ' None', ' None', None, ' None', ' None', ' None', ' None', ' None', None, ' None', None, ' None', None, None]",710,[]
parser/test3/load.py:load_from_disk,load_from_disk,function,13,48,35,691,14.4,0,3,"['dataset_path', 'fs', 'keep_in_memory']","[' str', None, ' Optional[bool] ']","[None, 'None', ' None']",878,"['    """"""\n', '    Loads a dataset that was previously saved using ``dataset.save_to_disk(dataset_path)`` from a dataset directory, or from a filesystem using either :class:`datasets.filesystems.S3FileSystem` or any implementation of ``fsspec.spec.AbstractFileSystem``.\n', '\n', '    Args:\n', '        dataset_path (:obj:`str`): Path (e.g. ``""dataset/train""``) or remote uri (e.g.\n', '            ``""s3://my-bucket/dataset/train""``) of the Dataset or DatasetDict directory where the dataset will be\n', '            loaded from.\n', '        fs (:class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n', '            Instance of of the remote filesystem used to download the files from.\n', '        keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the dataset\n', '            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n', '            nonzero. See more details in the :ref:`load_dataset_enhancing_performance` section.\n', '\n', '    Returns:\n', '        ``datasets.Dataset`` or ``datasets.DatasetDict``\n', '            if `dataset_path` is a path of a dataset directory: the dataset requested,\n', '            if `dataset_path` is a path of a dataset dict directory: a ``datasets.DatasetDict`` with each split.\n', '            keep_in_memory (``bool``, default False): Whether to copy the data in-memory.\n', '    """"""\n']"
parser/test3/metric.py:FileFreeLock,FileFreeLock,class,11,32,27,369,11.53,0,0,[],[],[],43,[]
parser/test3/metric.py:MetricInfoMixin,MetricInfoMixin,class,28,95,40,1033,10.87,0,0,[],[],[],65,[]
parser/test3/metric.py:Metric,Metric,class,155,940,448,9945,10.58,4,26,[],[],[],127,[]
parser/test3/metric.py:FileFreeLock:__init__,FileFreeLock:__init__,method,3,5,5,76,15.2,0,0,"['self', 'lock_file', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",46,[]
parser/test3/metric.py:FileFreeLock:_acquire,FileFreeLock:_acquire,method,5,16,15,186,11.62,0,0,['self'],[None],[None],50,[]
parser/test3/metric.py:FileFreeLock:_release,FileFreeLock:_release,method,1,2,2,23,11.5,0,0,['self'],[None],[None],61,[]
parser/test3/metric.py:MetricInfoMixin:__init__,MetricInfoMixin:__init__,method,2,2,2,22,11.0,0,0,"['self', 'info']","[None, ' MetricInfo']","[None, None]",70,[]
parser/test3/metric.py:MetricInfoMixin:info,MetricInfoMixin:info,method,2,2,2,23,11.5,0,0,['self'],[None],[None],74,"['        """""":class:`datasets.MetricInfo` object containing all the metadata in the metric.""""""\n']"
parser/test3/metric.py:MetricInfoMixin:name,MetricInfoMixin:name,method,2,2,2,35,17.5,0,0,['self'],[None],[None],79,[]
parser/test3/metric.py:MetricInfoMixin:experiment_id,MetricInfoMixin:experiment_id,method,2,2,2,37,18.5,0,0,['self'],[None],[None],83,[]
parser/test3/metric.py:MetricInfoMixin:description,MetricInfoMixin:description,method,2,2,2,35,17.5,0,0,['self'],[None],[None],87,[]
parser/test3/metric.py:MetricInfoMixin:citation,MetricInfoMixin:citation,method,2,2,2,32,16.0,0,0,['self'],[None],[None],91,[]
parser/test3/metric.py:MetricInfoMixin:features,MetricInfoMixin:features,method,2,2,2,32,16.0,0,0,['self'],[None],[None],95,[]
parser/test3/metric.py:MetricInfoMixin:inputs_description,MetricInfoMixin:inputs_description,method,2,2,2,42,21.0,0,0,['self'],[None],[None],99,[]
parser/test3/metric.py:MetricInfoMixin:homepage,MetricInfoMixin:homepage,method,2,2,2,32,16.0,0,0,['self'],[None],[None],103,[]
parser/test3/metric.py:MetricInfoMixin:license,MetricInfoMixin:license,method,2,2,2,31,15.5,0,0,['self'],[None],[None],107,[]
parser/test3/metric.py:MetricInfoMixin:codebase_urls,MetricInfoMixin:codebase_urls,method,2,2,2,37,18.5,0,0,['self'],[None],[None],111,[]
parser/test3/metric.py:MetricInfoMixin:reference_urls,MetricInfoMixin:reference_urls,method,2,2,2,38,19.0,0,0,['self'],[None],[None],115,[]
parser/test3/metric.py:MetricInfoMixin:streamable,MetricInfoMixin:streamable,method,2,2,2,34,17.0,0,0,['self'],[None],[None],119,[]
parser/test3/metric.py:MetricInfoMixin:format,MetricInfoMixin:format,method,2,2,2,30,15.0,0,0,['self'],[None],[None],123,[]
parser/test3/metric.py:Metric:__init__,Metric:__init__,method,48,132,98,1487,11.27,0,0,"['self', 'config_name', 'keep_in_memory', 'cache_dir', 'num_process', 'process_id', 'seed', 'experiment_id', 'max_concurrent_cache_files', 'timeout', 'float] ', '**kwargs', '']","[None, ' Optional[str] ', ' bool ', ' Optional[str] ', ' int ', ' int ', ' Optional[int] ', ' Optional[str] ', ' int ', ' Union[int', None, None, None]","[None, ' None', ' False', ' None', ' 1', ' 0', ' None', ' None', ' 10000', None, ' 100', None, None]",147,[]
parser/test3/metric.py:Metric:__len__,Metric:__len__,method,2,8,8,46,5.75,0,1,['self'],[None],[None],211,"['        """"""Return the number of examples (predictions or predictions/references pair)\n', ""        currently stored in the metric's cache.\n"", '        """"""\n']"
parser/test3/metric.py:Metric:__repr__,Metric:__repr__,method,1,14,13,137,9.79,0,0,['self'],[None],[None],217,[]
parser/test3/metric.py:Metric:_build_data_dir,Metric:_build_data_dir,method,5,10,8,178,17.8,0,0,['self'],[None],[None],224,"['        """"""Path of this metric in cache_dir:\n', '        Will be:\n', '            self._data_dir_root/self.name/self.config_name/self.hash (if not none)/\n', '        If any of these element is missing or if ``with_version=False`` the corresponding subfolders are dropped.\n', '        """"""\n']"
parser/test3/metric.py:Metric:_create_cache_file,Metric:_create_cache_file,method,16,102,81,969,9.5,1,2,"['self', 'timeout']","[None, None]","[None, '1']",235,"['        """"""Create a new cache file. If the default cache file is used, we generated a new hash.""""""\n']"
parser/test3/metric.py:Metric:_get_all_cache_files,Metric:_get_all_cache_files,method,22,86,72,752,8.74,1,3,['self'],[None],[None],268,"['        """"""Get a lock on all the cache files in a distributed setup.\n', '        We wait for timeout second to let all the distributed node finish their tasks (default is 100 seconds).\n', '        """"""\n']"
parser/test3/metric.py:Metric:_check_all_processes_locks,Metric:_check_all_processes_locks,method,8,37,35,471,12.73,1,0,['self'],[None],[None],301,[]
parser/test3/metric.py:Metric:_check_rendez_vous,Metric:_check_rendez_vous,method,13,47,39,686,14.6,0,0,['self'],[None],[None],317,[]
parser/test3/metric.py:Metric:_finalize,Metric:_finalize,method,20,76,62,763,10.04,0,3,['self'],[None],[None],337,"['        """"""Close all the writing process and load/gather the data\n', '        from all the nodes if main node or all_process is True.\n', '        """"""\n']"
parser/test3/metric.py:Metric:compute,Metric:compute,method,24,61,43,713,11.69,1,3,"['self', '*', 'predictions', 'references', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",371,"['        """"""Compute the metrics.\n', '\n', '        Usage of positional arguments is not allowed to prevent mistakes.\n', '\n', '        Args:\n', '            predictions (list/array/tensor, optional): Predictions.\n', '            references (list/array/tensor, optional): References.\n', '            **kwargs (optional): Keyword arguments that will be forwarded to the metrics :meth:`_compute`\n', '                method (see details in the docstring).\n', '\n', '        Return:\n', '            dict or None\n', '\n', '            - Dictionary with the metrics if this metric is run on the main process (``process_id == 0``).\n', '            - None if the metric is not run on the main process (``process_id != 0``).\n', '        """"""\n']"
parser/test3/metric.py:Metric:add_batch,Metric:add_batch,method,7,36,34,388,10.78,0,1,"['self', '*', 'predictions', 'references']","[None, None, None, None]","[None, None, 'None', 'None']",423,"['        """"""Add a batch of predictions and references for the metric\'s stack.\n', '\n', '        Args:\n', '            predictions (list/array/tensor, optional): Predictions.\n', '            references (list/array/tensor, optional): References.\n', '        """"""\n']"
parser/test3/metric.py:Metric:add,Metric:add,method,11,77,49,822,10.68,0,2,"['self', '*', 'predictions', 'references']","[None, None, None, None]","[None, None, 'None', 'None']",444,[]
parser/test3/metric.py:Metric:_init_writer,Metric:_init_writer,method,24,120,83,1198,9.98,0,6,"['self', 'timeout']","[None, None]","[None, '1']",465,[]
parser/test3/metric.py:Metric:_info,Metric:_info,method,1,2,2,24,12.0,0,0,['self'],[None],[None],504,"['        """"""Construct the MetricInfo object. See `MetricInfo` for details.\n', '\n', '        Warning: This function is only called once and the result is cached for all\n', '        following .info() calls.\n', '\n', '        Returns:\n', '            info: (MetricInfo) The metrics information\n', '        """"""\n']"
parser/test3/metric.py:Metric:download_and_prepare,Metric:download_and_prepare,method,8,22,17,327,14.86,0,2,"['self', 'download_config', 'dl_manager', '']","[None, ' Optional[DownloadConfig] ', ' Optional[DownloadManager] ', None]","[None, ' None', ' None', None]",515,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        Args:\n', '            download_config (:class:`DownloadConfig`, optional): Specific download configuration parameters.\n', '            dl_manager (:class:`DownloadManager`, optional): Specific download manager to use.\n', '        """"""\n']"
parser/test3/metric.py:Metric:_download_and_prepare,Metric:_download_and_prepare,method,1,2,2,10,5.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",538,"['        """"""Downloads and prepares resources for the metric.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `download_and_prepare`. It should download all required resources for the metric.\n', '\n', '        Args:\n', '            dl_manager (:class:`DownloadManager`): `DownloadManager` used to download and cache data.\n', '        """"""\n']"
parser/test3/metric.py:Metric:_compute,Metric:_compute,method,1,2,2,24,12.0,0,0,"['self', '*', 'predictions', 'references', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",549,"['        """"""This method defines the common API for all the metrics in the library""""""\n']"
parser/test3/metric.py:Metric:__del__,Metric:__del__,method,6,42,24,307,7.31,0,4,['self'],[None],[None],553,[]
parser/test3/naming.py:camelcase_to_snakecase,camelcase_to_snakecase,function,5,8,6,114,14.25,0,0,['name'],[None],[None],33,"['    """"""Convert camel-case string to snake-case.""""""\n']"
parser/test3/naming.py:snakecase_to_camelcase,snakecase_to_camelcase,function,3,17,12,167,9.82,0,0,['name'],[None],[None],40,"['    """"""Convert snake-case string to camel-case string.""""""\n']"
parser/test3/naming.py:filename_prefix_for_name,filename_prefix_for_name,function,4,17,16,123,7.24,0,1,['name'],[None],[None],47,[]
parser/test3/naming.py:filename_prefix_for_split,filename_prefix_for_split,function,4,33,28,244,7.39,0,2,"['name', 'split']","[None, None]","[None, None]",53,[]
parser/test3/naming.py:filepattern_for_dataset_split,filepattern_for_dataset_split,function,6,17,14,161,9.47,0,1,"['dataset_name', 'split', 'data_dir', 'filetype_suffix']","[None, None, None, None]","[None, None, None, 'None']",61,[]
parser/test3/naming.py:filename_for_dataset_split,filename_for_dataset_split,function,4,12,10,114,9.5,0,1,"['dataset_name', 'split', 'filetype_suffix']","[None, None, None]","[None, None, 'None']",69,[]
parser/test3/naming.py:filepath_for_dataset_split,filepath_for_dataset_split,function,5,11,10,167,15.18,0,0,"['dataset_name', 'split', 'data_dir', 'filetype_suffix']","[None, None, None, None]","[None, None, None, 'None']",76,[]
parser/test3/search.py:MissingIndex,MissingIndex,class,0,1,1,4,4.0,0,0,[],[],[],34,[]
parser/test3/search.py:BaseIndex,BaseIndex,class,16,52,39,489,9.4,1,0,[],[],[],49,[]
parser/test3/search.py:ElasticSearchIndex,ElasticSearchIndex,class,51,259,183,2228,8.6,3,3,[],[],[],87,[]
parser/test3/search.py:FaissIndex,FaissIndex,class,66,383,202,3601,9.4,1,17,[],[],[],190,[]
parser/test3/search.py:IndexableMixin,IndexableMixin,class,57,430,211,4553,10.59,0,4,[],[],[],345,[]
parser/test3/search.py:BaseIndex:search,BaseIndex:search,method,13,27,27,287,10.63,1,0,"['self', 'query', 'k']","[None, None, ' int ']","[None, None, ' 10']",52,"['        """"""\n', '        To implement.\n', '        This method has to return the scores and the indices of the retrieved examples given a certain query.\n', '        """"""\n']"
parser/test3/search.py:BaseIndex:search_batch,BaseIndex:search_batch,method,11,17,17,200,11.76,1,0,"['self', 'queries', 'k']","[None, None, ' int ']","[None, None, ' 10']",59,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            queries (`Union[List[str], np.ndarray]`): The queries as a list of strings if `column` is a text index or as a numpy array if `column` is a vector index.\n', '            k (`int`): The number of examples to retrieve per query.\n', '\n', '        Ouput:\n', '            total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_indices (`List[List[int]]`): The indices of the retrieved examples per query.\n', '        """"""\n']"
parser/test3/search.py:BaseIndex:save,BaseIndex:save,method,1,2,2,24,12.0,0,0,"['self', 'file', 'PurePath]']","[None, ' Union[str', None]","[None, None, None]",77,"['        """"""Serialize the index on disk""""""\n']"
parser/test3/search.py:BaseIndex:load,BaseIndex:load,method,1,2,2,24,12.0,0,0,"['cls', 'file', 'PurePath]']","[None, ' Union[str', None]","[None, None, None]",82,"['        """"""Deserialize the index from disk""""""\n']"
parser/test3/search.py:ElasticSearchIndex:__init__,ElasticSearchIndex:__init__,method,23,118,83,815,6.91,0,1,"['self', 'host', 'port', 'es_client', 'es_index_name', 'es_index_config', '']","[None, ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[str] ', ' Optional[dict] ', None]","[None, ' None', ' None', ' None', ' None', ' None', None]",97,[]
parser/test3/search.py:ElasticSearchIndex:add_documents,ElasticSearchIndex:add_documents,method,24,80,65,827,10.34,3,2,"['self', 'documents', '""Dataset""]', 'column']","[None, ' Union[List[str]', None, ' Optional[str] ']","[None, None, None, ' None']",135,"['        """"""\n', '        Add documents to the index.\n', '        If the documents are inside a certain column, you can specify it using the `column` argument.\n', '        """"""\n']"
parser/test3/search.py:ElasticSearchIndex:search,ElasticSearchIndex:search,method,6,27,24,265,9.81,0,0,"['self', 'query', 'k']","[None, ' str', None]","[None, None, '10']",171,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            query (`str`): The query as a string.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            scores (`List[List[float]`): The retrieval scores of the retrieved examples.\n', '            indices (`List[List[int]]`): The indices of the retrieved examples.\n', '        """"""\n']"
parser/test3/search.py:FaissIndex:__init__,FaissIndex:__init__,method,10,56,47,372,6.64,0,1,"['self', 'device', 'string_factory', 'metric_type', 'custom_index', '']","[None, ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', None]","[None, ' None', ' None', ' None', ' None', None]",97,"['        """"""\n', '        Add documents to the index.\n', '        If the documents are inside a certain column, you can specify it using the `column` argument.\n', '        """"""\n']"
parser/test3/search.py:FaissIndex:add_vectors,FaissIndex:add_vectors,method,36,170,88,1671,9.83,1,12,"['self', 'vectors', '""Dataset""]', 'column', 'batch_size', 'train_size', 'faiss_verbose', '']","[None, ' Union[np.array', None, ' Optional[str] ', ' int ', ' Optional[int] ', ' Optional[bool] ', None]","[None, None, None, ' None', ' 1000', ' None', ' None', None]",224,"['        """"""\n', '        Add vectors to the index.\n', '        If the arrays are inside a certain column, you can specify it using the `column` argument.\n', '        """"""\n']"
parser/test3/search.py:FaissIndex:search,FaissIndex:search,method,11,48,35,542,11.29,0,2,"['self', 'query', 'k']","[None, ' np.array', None]","[None, None, '10']",281,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            query (`np.array`): The query as a numpy array.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            scores (`List[List[float]`): The retrieval scores of the retrieved examples.\n', '            indices (`List[List[int]]`): The indices of the retrieved examples.\n', '        """"""\n']"
parser/test3/search.py:FaissIndex:search_batch,FaissIndex:search_batch,method,8,16,16,203,12.69,0,1,"['self', 'queries', 'k']","[None, ' np.array', None]","[None, None, '10']",299,"['        """"""Find the nearest examples indices to the queries.\n', '\n', '        Args:\n', '            queries (`np.array`): The queries as a numpy array.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_indices (`List[List[int]]`): The indices of the retrieved examples per query.\n', '        """"""\n']"
parser/test3/search.py:FaissIndex:save,FaissIndex:save,method,10,21,19,173,8.24,0,1,"['self', 'file', 'PurePath]']","[None, ' Union[str', None]","[None, None, None]",77,"['        """"""Serialize the index on disk""""""\n']"
parser/test3/search.py:FaissIndex:load,FaissIndex:load,method,15,28,25,317,11.32,0,1,"['cls', 'file', 'PurePath]', 'device', '']","[None, ' Union[str', None, ' Optional[int] ', None]","[None, None, None, ' None', None]",328,"['        """"""Deserialize the FaissIndex from disk""""""\n']"
parser/test3/search.py:IndexableMixin:__init__,IndexableMixin:__init__,method,3,4,4,36,9.0,0,0,['self'],[None],[None],348,[]
parser/test3/search.py:IndexableMixin:__len__,IndexableMixin:__len__,method,1,2,2,24,12.0,0,0,['self'],[None],[None],351,[]
parser/test3/search.py:IndexableMixin:__getitem__,IndexableMixin:__getitem__,method,1,2,2,24,12.0,0,0,"['self', 'key']","[None, None]","[None, None]",354,[]
parser/test3/search.py:IndexableMixin:is_index_initialized,IndexableMixin:is_index_initialized,method,3,4,4,31,7.75,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",357,[]
parser/test3/search.py:IndexableMixin:_check_index_is_initialized,IndexableMixin:_check_index_is_initialized,method,2,23,22,194,8.43,0,1,"['self', 'index_name']","[None, ' str']","[None, None]",360,[]
parser/test3/search.py:IndexableMixin:list_indexes,IndexableMixin:list_indexes,method,1,2,2,25,12.5,0,0,['self'],[None],[None],366,"['        """"""List the colindex_nameumns/identifiers of all the attached indexes.""""""\n']"
parser/test3/search.py:IndexableMixin:get_index,IndexableMixin:get_index,method,3,3,3,76,25.33,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",370,"['        """"""List the index_name/identifiers of all the attached indexes.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): Index name.\n', '\n', '        Returns:\n', '            :class:`BaseIndex`\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:add_faiss_index,IndexableMixin:add_faiss_index,method,7,47,39,587,12.49,0,1,"['self', 'column', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', 'train_size', 'faiss_verbose', '']","[None, ' str', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' Optional[int] ', ' bool ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', ' False', None]",382,[]
parser/test3/search.py:IndexableMixin:add_faiss_index_from_external_arrays,IndexableMixin:add_faiss_index_from_external_arrays,method,4,13,12,260,20.0,0,0,"['self', 'external_arrays', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', 'train_size', 'faiss_verbose', '']","[None, ' np.array', ' str', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' Optional[int] ', ' bool ', None]","[None, None, None, ' None', ' None', ' None', ' None', ' None', ' False', None]",417,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        The index is created using the vectors of `external_arrays`.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n', '        You can find more information about Faiss here:\n', '        - For `string factory`: https://github.com/facebookresearch/faiss/wiki/The-index-factory\n', '\n', '        Args:\n', '            external_arrays (:obj:`np.array`): If you want to use arrays from outside the lib for the index, you can set `external_arrays`.\n', '                It will use `external_arrays` to create the Faiss index instead of the arrays in the given `column`.\n', '            index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n', '            device (Optional :obj:`int`): If not None, this is the index of the GPU to use. By default it uses the CPU.\n', '            string_factory (Optional :obj:`str`): This is passed to the index factory of Faiss to create the index. Default index class is IndexFlatIP.\n', '            metric_type (Optional :obj:`int`): Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n', '            custom_index (Optional :obj:`faiss.Index`): Custom Faiss index that you already have instantiated and configured for your needs.\n', '            train_size (Optional :obj:`int`): If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (:obj:`bool`, defaults to False): Enable the verbosity of the Faiss index.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:save_faiss_index,IndexableMixin:save_faiss_index,method,6,24,21,229,9.54,0,1,"['self', 'index_name', 'file', 'PurePath]']","[None, ' str', ' Union[str', None]","[None, None, None, None]",451,"['        """"""Save a FaissIndex on disk.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n', '            file (:obj:`str`): The path to the serialized faiss index on disk.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:load_faiss_index,IndexableMixin:load_faiss_index,method,8,41,37,335,8.17,0,0,"['self', 'index_name', 'file', 'PurePath]', 'device', '']","[None, ' str', ' Union[str', None, ' Optional[int] ', None]","[None, None, None, None, ' None', None]",464,"['        """"""Load a FaissIndex from disk.\n', '\n', '        If you want to do additional configurations, you can have access to the faiss index object by doing\n', '        `.get_index(index_name).faiss_index` to make it fit your needs.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to\n', '                call `.get_nearest` or `.search`.\n', '            file (:obj:`str`): The path to the serialized faiss index on disk.\n', '            device (Optional :obj:`int`): If not None, this is the index of the GPU to use. By default it uses the CPU.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:add_elasticsearch_index,IndexableMixin:add_elasticsearch_index,method,0,13,13,124,9.54,0,0,"['self', 'column', 'index_name', 'host', 'port', 'es_client', 'es_index_name', 'es_index_config', '']","[None, ' str', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[str] ', ' Optional[dict] ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', None]",490,"['        """"""Add a text index using ElasticSearch for fast retrieval.\n', '\n', '        Args:\n', '            column (:obj:`str`): The column of the documents to add to the index.\n', '            index_name (Optional :obj:`str`): The index_name/identifier of the index. This is the index name that is used to call `.get_nearest` or `.search`.\n', '                By defaul it corresponds to `column`.\n', '            host (Optional :obj:`str`, defaults to localhost):\n', '                host of where ElasticSearch is running\n', '            port (Optional :obj:`str`, defaults to 9200):\n', '                port of where ElasticSearch is running\n', '            es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n', '                The elasticsearch client used to create the index if host and port are None.\n', '            es_index_name (Optional :obj:`str`): The elasticsearch index name used to create the index.\n', '            es_index_config (Optional :obj:`dict`):\n', '                The configuration of the elasticsearch index.\n', '                Default config is:\n', '\n', '        Config::\n', '\n', '            {\n', '                ""settings"": {\n', '                    ""number_of_shards"": 1,\n', '                    ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                },\n', '                ""mappings"": {\n', '                    ""properties"": {\n', '                        ""text"": {\n', '                            ""type"": ""text"",\n', '                            ""analyzer"": ""standard"",\n', '                            ""similarity"": ""BM25""\n', '                        },\n', '                    }\n', '                },\n', '            }\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:load_elasticsearch_index,IndexableMixin:load_elasticsearch_index,method,2,8,8,147,18.38,0,0,"['self', 'index_name', 'es_index_name', 'host', 'port', 'es_client', 'es_index_config', '']","[None, ' str', ' str', ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[dict] ', None]","[None, None, None, ' None', ' None', ' None', ' None', None]",542,"['        """"""Load an existing text index using ElasticSearch for fast retrieval.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index. This is the index name that is used to call `.get_nearest` or `.search`.\n', '            es_index_name (:obj:`str`): The name of elasticsearch index to load.\n', '            host (Optional :obj:`str`, defaults to localhost):\n', '                host of where ElasticSearch is running\n', '            port (Optional :obj:`str`, defaults to 9200):\n', '                port of where ElasticSearch is running\n', '            es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n', '                The elasticsearch client used to create the index if host and port are None.\n', '            es_index_config (Optional :obj:`dict`):\n', '                The configuration of the elasticsearch index.\n', '                Default config is::\n', '\n', '                    {\n', '                        ""settings"": {\n', '                            ""number_of_shards"": 1,\n', '                            ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                        },\n', '                        ""mappings"": {\n', '                            ""properties"": {\n', '                                ""text"": {\n', '                                    ""type"": ""text"",\n', '                                    ""analyzer"": ""standard"",\n', '                                    ""similarity"": ""BM25""\n', '                                },\n', '                            }\n', '                        },\n', '                    }\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:drop_index,IndexableMixin:drop_index,method,2,2,2,28,14.0,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",586,"['        """"""Drop the index with the specified column.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:search,IndexableMixin:search,method,5,20,18,296,14.8,0,0,"['self', 'index_name', 'query', 'np.array]', 'k']","[None, ' str', ' Union[str', None, ' int ']","[None, None, None, None, ' 10']",594,"['        """"""Find the nearest examples indices in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The name/identifier of the index.\n', '            query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (:obj:`int`): The number of examples to retrieve.\n', '\n', '        Returns:\n', '            scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples.\n', '            indices (:obj:`List[List[int]]`): The indices of the retrieved examples.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:search_batch,IndexableMixin:search_batch,method,3,4,4,100,25.0,0,0,"['self', 'index_name', 'queries', 'np.array]', 'k']","[None, ' str', ' Union[List[str]', None, ' int ']","[None, None, None, None, ' 10']",609,"['        """"""Find the nearest examples indices in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index.\n', '            queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (:obj:`int`): The number of examples to retrieve per query.\n', '\n', '        Returns:\n', '            total_scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_indices (:obj:`List[List[int]]`): The indices of the retrieved examples per query.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:get_nearest_examples,IndexableMixin:get_nearest_examples,method,7,27,25,251,9.3,0,0,"['self', 'index_name', 'query', 'np.array]', 'k']","[None, ' str', ' Union[str', None, ' int ']","[None, None, None, None, ' 10']",624,"['        """"""Find the nearest examples in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index.\n', '            query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (:obj:`int`): The number of examples to retrieve.\n', '\n', '        Returns:\n', '            scores (:obj:`List[float]`): The retrieval scores of the retrieved examples.\n', '            examples (:obj:`dict`): The retrieved examples.\n', '        """"""\n']"
parser/test3/search.py:IndexableMixin:get_nearest_examples_batch,IndexableMixin:get_nearest_examples_batch,method,6,22,17,219,9.95,0,0,"['self', 'index_name', 'queries', 'np.array]', 'k']","[None, ' str', ' Union[List[str]', None, ' int ']","[None, None, None, None, ' 10']",642,"['        """"""Find the nearest examples in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (:obj:`str`): The index_name/identifier of the index.\n', '            queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (:obj:`int`): The number of examples to retrieve per query.\n', '\n', '        Returns:\n', '            total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_examples (`List[dict]`): The retrieved examples per query.\n', '        """"""\n']"
parser/test3/splits.py:SplitInfo,SplitInfo,class,11,23,21,264,11.48,0,0,[],[],[],32,[]
parser/test3/splits.py:SubSplitInfo,SubSplitInfo,class,7,12,9,179,14.92,0,0,[],[],[],51,[]
parser/test3/splits.py:SplitBase,SplitBase,class,30,220,139,1585,7.2,2,6,[],[],[],73,[]
parser/test3/splits.py:PercentSliceMeta,PercentSliceMeta,class,5,19,18,175,9.21,0,1,[],[],[],244,[]
parser/test3/splits.py:PercentSlice,PercentSlice,class,0,1,1,4,4.0,0,0,[],[],[],251,[]
parser/test3/splits.py:_SplitMerged,_SplitMerged,class,12,24,20,357,14.88,0,0,[],[],[],266,[]
parser/test3/splits.py:_SubSplit,_SubSplit,class,12,46,35,555,12.07,0,1,[],[],[],282,[]
parser/test3/splits.py:NamedSplit,NamedSplit,class,0,1,1,1,1.0,0,0,[],[],[],304,[]
parser/test3/splits.py:NamedSplitAll,NamedSplitAll,class,7,20,17,266,13.3,0,0,[],[],[],373,[]
parser/test3/splits.py:Split,Split,class,8,18,18,182,10.11,0,0,[],[],[],388,[]
parser/test3/splits.py:SplitReadInstruction,SplitReadInstruction,class,22,78,61,984,12.62,1,2,[],[],[],428,[]
parser/test3/splits.py:SplitDict,SplitDict,class,33,143,98,1581,11.06,1,7,[],[],[],483,[]
parser/test3/splits.py:SplitGenerator,SplitGenerator,class,11,31,30,271,8.74,0,0,[],[],[],550,[]
parser/test3/splits.py:SplitInfo:file_instructions,SplitInfo:file_instructions,method,4,8,8,147,18.38,0,0,['self'],[None],[None],39,"['        """"""Returns the list of dict(filename, take, skip).""""""\n']"
parser/test3/splits.py:SubSplitInfo:num_examples,SubSplitInfo:num_examples,method,2,2,2,36,18.0,0,0,['self'],[None],[None],63,"['        """"""Returns the number of example in the subsplit.""""""\n']"
parser/test3/splits.py:SubSplitInfo:file_instructions,SubSplitInfo:file_instructions,method,2,2,2,41,20.5,0,0,['self'],[None],[None],39,"['        """"""Returns the list of dict(filename, take, skip).""""""\n']"
parser/test3/splits.py:SplitBase:get_read_instruction,SplitBase:get_read_instruction,method,1,3,3,42,14.0,0,0,"['self', 'split_dict']","[None, None]","[None, None]",108,"['        """"""Parse the descriptor tree and compile all read instructions together.\n', '\n', '        Args:\n', '            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n', '\n', '        Returns:\n', '            split_read_instruction: `SplitReadInstruction`\n', '        """"""\n']"
parser/test3/splits.py:SplitBase:__eq__,SplitBase:__eq__,method,3,14,14,126,9.0,0,1,"['self', 'other']","[None, None]","[None, None]",119,"['        """"""Equality: datasets.Split.TRAIN == \'train\'.""""""\n']"
parser/test3/splits.py:SplitBase:__ne__,SplitBase:__ne__,method,2,3,3,27,9.0,0,0,"['self', 'other']","[None, None]","[None, None]",125,"['        """"""InEquality: datasets.Split.TRAIN != \'test\'.""""""\n']"
parser/test3/splits.py:SplitBase:__add__,SplitBase:__add__,method,2,3,3,30,10.0,0,0,"['self', 'other']","[None, None]","[None, None]",129,"['        """"""Merging: datasets.Split.TRAIN + datasets.Split.TEST.""""""\n']"
parser/test3/splits.py:SplitBase:subsplit,SplitBase:subsplit,method,1,9,9,56,6.22,0,0,"['self', 'arg', 'k', 'percent', 'weighted=None)', 'k', 'percent', 'weighted)) != 1']","[None, None, None, None, '', None, None, '']","[None, 'None', 'None', 'None', 'None):  # pylint: disable=redefined-outer-namebool(x) for x in (arg', None, None, ' 1:']",133,"['        """"""Divides this split into subsplits.\n', '\n', '        There are 3 ways to define subsplits, which correspond to the 3\n', '        arguments `k` (get `k` even subsplits), `percent` (get a slice of the\n', '        dataset with `datasets.percent`), and `weighted` (get subsplits with proportions\n', '        specified by `weighted`).\n', '\n', '        Examples:\n', '\n', '        ```\n', '        # 50% train, 50% test\n', '        train, test = split.subsplit(k=2)\n', '        # 50% train, 25% test, 25% validation\n', '        train, test, validation = split.subsplit(weighted=[2, 1, 1])\n', '        # Extract last 20%\n', '        subsplit = split.subsplit(datasets.percent[-20:])\n', '        ```\n', '\n', '        Warning: k and weighted will be converted into percent which mean that\n', '        values below the percent will be rounded up or down. The final split may be\n', '        bigger to deal with remainders. For instance:\n', '\n', '        ```\n', '        train, test, valid = split.subsplit(k=3)  # 33%, 33%, 34%\n', '        s1, s2, s3, s4 = split.subsplit(weighted=[2, 2, 1, 1])  # 33%, 33%, 16%, 18%\n', '        ```\n', '\n', '        Args:\n', '            arg: If no kwargs are given, `arg` will be interpreted as one of\n', '                `k`, `percent`, or `weighted` depending on the type.\n', '                For example:\n', '                ```\n', '                split.subsplit(10)  # Equivalent to split.subsplit(k=10)\n', '                split.subsplit(datasets.percent[:-20])  # percent=datasets.percent[:-20]\n', '                split.subsplit([1, 1, 2])  # weighted=[1, 1, 2]\n', '                ```\n', '            k: `int` If set, subdivide the split into `k` equal parts.\n', '            percent: `datasets.percent slice`, return a single subsplit corresponding to\n', '                a slice of the original split. For example:\n', '                `split.subsplit(datasets.percent[-20:])  # Last 20% of the dataset`.\n', '            weighted: `list[int]`, return a list of subsplits whose proportions match\n', '                the normalized sum of the list. For example:\n', '                `split.subsplit(weighted=[1, 1, 2])  # 25%, 25%, 50%`.\n', '\n', '        Returns:\n', '            A subsplit or list of subsplits extracted from this split object.\n', '        """"""\n']"
parser/test3/splits.py:PercentSliceMeta:__getitem__,PercentSliceMeta:__getitem__,method,4,16,15,142,8.88,0,1,"['cls', 'slice_value']","[None, None]","[None, None]",245,[]
parser/test3/splits.py:_SplitMerged:__init__,_SplitMerged:__init__,method,4,4,4,39,9.75,0,0,"['self', 'split1', 'split2']","[None, None, None]","[None, None, None]",269,[]
parser/test3/splits.py:_SplitMerged:get_read_instruction,_SplitMerged:get_read_instruction,method,5,7,5,169,24.14,0,0,"['self', 'split_dict']","[None, None]","[None, None]",108,"['        """"""Parse the descriptor tree and compile all read instructions together.\n', '\n', '        Args:\n', '            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n', '\n', '        Returns:\n', '            split_read_instruction: `SplitReadInstruction`\n', '        """"""\n']"
parser/test3/splits.py:_SplitMerged:__repr__,_SplitMerged:__repr__,method,1,4,4,53,13.25,0,0,['self'],[None],[None],278,[]
parser/test3/splits.py:_SubSplit:__init__,_SubSplit:__init__,method,4,4,4,47,11.75,0,0,"['self', 'split', 'slice_value']","[None, None, None]","[None, None, None]",285,[]
parser/test3/splits.py:_SubSplit:get_read_instruction,_SubSplit:get_read_instruction,method,2,2,2,69,34.5,0,0,"['self', 'split_dict']","[None, None]","[None, None]",108,"['        """"""Parse the descriptor tree and compile all read instructions together.\n', '\n', '        Args:\n', '            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n', '\n', '        Returns:\n', '            split_read_instruction: `SplitReadInstruction`\n', '        """"""\n']"
parser/test3/splits.py:_SubSplit:__repr__,_SubSplit:__repr__,method,4,31,23,339,10.94,0,1,['self'],[None],[None],278,"['    """"""Represent a sub split of a split descriptor.""""""\n']"
parser/test3/splits.py:NamedSplitAll:__init__,NamedSplitAll:__init__,method,1,2,2,41,20.5,0,0,['self'],[None],[None],376,[]
parser/test3/splits.py:NamedSplitAll:__repr__,NamedSplitAll:__repr__,method,1,2,2,23,11.5,0,0,['self'],[None],[None],278,"['    """"""Represent a sub split of a split descriptor.""""""\n']"
parser/test3/splits.py:NamedSplitAll:get_read_instruction,NamedSplitAll:get_read_instruction,method,3,9,9,120,13.33,0,0,"['self', 'split_dict']","[None, None]","[None, None]",108,"['        """"""Parse the descriptor tree and compile all read instructions together.\n', '\n', '        Args:\n', '            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n', '\n', '        Returns:\n', '            split_read_instruction: `SplitReadInstruction`\n', '        """"""\n']"
parser/test3/splits.py:Split:__new__,Split:__new__,method,2,7,7,54,7.71,0,0,"['cls', 'name']","[None, None]","[None, None]",413,"['        """"""Create a custom split with datasets.Split(\'custom_name\').""""""\n']"
parser/test3/splits.py:SplitReadInstruction:__init__,SplitReadInstruction:__init__,method,4,16,16,177,11.06,0,1,"['self', 'split_info']","[None, None]","[None, 'None']",444,[]
parser/test3/splits.py:SplitReadInstruction:add,SplitReadInstruction:add,method,2,2,2,55,27.5,0,0,"['self', 'sliced_split']","[None, None]","[None, None]",450,"['        """"""Add a SlicedSplitInfo the read instructions.""""""\n']"
parser/test3/splits.py:SplitReadInstruction:__add__,SplitReadInstruction:__add__,method,4,12,8,223,18.58,0,0,"['self', 'other']","[None, None]","[None, None]",129,"['        """"""Merging: datasets.Split.TRAIN + datasets.Split.TEST.""""""\n']"
parser/test3/splits.py:SplitReadInstruction:__getitem__,SplitReadInstruction:__getitem__,method,10,29,27,297,10.24,1,1,"['self', 'slice_value']","[None, None]","[None, None]",467,"['        """"""Sub-splits.""""""\n']"
parser/test3/splits.py:SplitReadInstruction:get_list_sliced_split_info,SplitReadInstruction:get_list_sliced_split_info,method,1,5,5,71,14.2,0,0,['self'],[None],[None],479,[]
parser/test3/splits.py:SplitDict:__init__,SplitDict:__init__,method,3,5,5,77,15.4,0,0,"['self', '*args', 'dataset_name', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",486,[]
parser/test3/splits.py:SplitDict:__getitem__,SplitDict:__getitem__,method,7,16,15,213,13.31,0,1,"['self', 'key', 'str]']","[None, ' Union[SplitBase', None]","[None, None, None]",490,[]
parser/test3/splits.py:SplitDict:__setitem__,SplitDict:__setitem__,method,4,24,21,210,8.75,0,2,"['self', 'key', 'str]', 'value']","[None, ' Union[SplitBase', None, ' SplitInfo']","[None, None, None, None]",503,[]
parser/test3/splits.py:SplitDict:add,SplitDict:add,method,5,14,14,193,13.79,0,1,"['self', 'split_info']","[None, ' SplitInfo']","[None, None]",510,"['        """"""Add the split info.""""""\n']"
parser/test3/splits.py:SplitDict:total_num_examples,SplitDict:total_num_examples,method,2,6,6,44,7.33,0,0,['self'],[None],[None],518,"['        """"""Return the total number of examples.""""""\n']"
parser/test3/splits.py:SplitDict:from_split_dict,SplitDict:from_split_dict,method,10,29,21,337,11.62,1,3,"['cls', 'split_infos', 'Dict]', 'dataset_name']","[None, ' Union[List', None, ' Optional[str] ']","[None, None, None, ' None']",523,"['        """"""Returns a new SplitDict initialized from a Dict or List of `split_infos`.""""""\n']"
parser/test3/splits.py:SplitDict:to_split_dict,SplitDict:to_split_dict,method,2,9,9,55,6.11,0,0,['self'],[None],[None],540,"['        """"""Returns a list of SplitInfo protos that we have.""""""\n']"
parser/test3/splits.py:SplitDict:copy,SplitDict:copy,method,2,3,3,71,23.67,0,0,['self'],[None],[None],545,[]
parser/test3/splits.py:SplitGenerator:__post_init__,SplitGenerator:__post_init__,method,5,21,20,155,7.38,0,0,['self'],[None],[None],569,[]
parser/test3/streaming.py:extend_module_for_streaming,extend_module_for_streaming,function,4,15,12,249,16.6,0,1,"['module_path', 'use_auth_token', 'bool]] ']","[None, ' Optional[Union[str', None]","[None, None, ' None']",75,"['    """"""\n', '    Extend the `open` and `os.path.join` functions of the module to support data streaming.\n', '    They rare replaced by `xopen` and `xjoin` defined to work with the StreamingDownloadManager.\n', '\n', '    We use fsspec to extend `open` to be able to read remote files.\n', '    To join paths and naviguate in remote compressed archives, we use the ""::"" separator.\n', '    """"""\n']"
parser/test3/streaming.py:_PatchedModuleObj,_PatchedModuleObj,class,6,21,18,160,7.62,1,2,[],[],[],12,[]
parser/test3/streaming.py:patch_submodule,patch_submodule,class,28,62,53,637,10.27,1,0,[],[],[],22,[]
parser/test3/streaming.py:_PatchedModuleObj:__init__,_PatchedModuleObj:__init__,method,5,18,15,134,7.44,1,2,"['self', 'module']","[None, None]","[None, None]",15,[]
parser/test3/streaming.py:patch_submodule:__init__,patch_submodule:__init__,method,10,12,12,115,9.58,0,0,"['self', 'obj', 'target', 'new']","[None, None, ' str', None]","[None, None, None, None]",41,[]
parser/test3/streaming.py:patch_submodule:__enter__,patch_submodule:__enter__,method,8,20,17,202,10.1,1,0,['self'],[None],[None],48,[]
parser/test3/streaming.py:patch_submodule:__exit__,patch_submodule:__exit__,method,1,3,3,40,13.33,0,0,"['self', '*exc_info']","[None, None]","[None, None]",56,[]
parser/test3/streaming.py:patch_submodule:start,patch_submodule:start,method,2,2,2,50,25.0,0,0,['self'],[None],[None],59,"['        """"""Activate a patch.""""""\n']"
parser/test3/streaming.py:patch_submodule:stop,patch_submodule:stop,method,4,8,7,89,11.12,0,0,['self'],[None],[None],64,"['        """"""Stop an active patch.""""""\n']"
parser/test3/table.py:inject_arrow_table_documentation,inject_arrow_table_documentation,function,6,11,10,138,12.55,0,0,['arrow_table_method'],[None],[None],19,[]
parser/test3/table.py:_in_memory_arrow_table_from_file,_in_memory_arrow_table_from_file,function,7,8,7,142,17.75,0,0,['filename'],[' str'],[None],28,[]
parser/test3/table.py:_in_memory_arrow_table_from_buffer,_in_memory_arrow_table_from_buffer,function,7,8,7,114,14.25,0,0,['buffer'],[' pa.Buffer'],[None],35,[]
parser/test3/table.py:_memory_mapped_arrow_table_from_file,_memory_mapped_arrow_table_from_file,function,7,8,7,148,18.5,0,0,['filename'],[' str'],[None],42,[]
parser/test3/table.py:_write_table_to_file,_write_table_to_file,function,12,23,20,244,10.61,1,0,"['table', 'filename']","[' pa.Table', ' str']","[None, None]",49,[]
parser/test3/table.py:_deepcopy,_deepcopy,function,10,17,15,139,8.18,1,0,"['x', 'memo']","[None, ' dict']","[None, None]",59,"['    """"""deepcopy a regular class instance""""""\n']"
parser/test3/table.py:_interpolation_search,_interpolation_search,function,12,56,40,228,4.07,1,1,"['arr', 'x']","[' List[int]', ' int']","[None, None]",69,"['    """"""\n', '    Return the position i of a sorted array so that arr[i] <= x < arr[i+1]\n', '\n', '    Args:\n', '        arr (:obj:`List[int]`): non-empty sorted list of integers\n', '        x (:obj:`int`): query\n', '\n', '    Returns:\n', '        `int`: the position i so that arr[i] <= x < arr[i+1]\n', '\n', '    Raises:\n', '        `IndexError`: if the array is empty or if the query is outside the array values\n', '    """"""\n']"
parser/test3/table.py:concat_tables,concat_tables,function,4,10,9,108,10.8,0,1,"['tables', 'axis']","[' List[Table]', ' int ']","[None, ' 0']",833,"['    """"""\n', '    Concatenate tables.\n', '\n', '    Args:\n', '        tables (list of :class:`Table`): List of tables to be concatenated.\n', '        axis (``{0, 1}``, default ``0``, meaning over rows):\n', '            Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n', '            (horizontally).\n', '\n', '            .. versionadded:: 1.6.0\n', '\n', '    Returns:\n', '        :obj:`datasets.table.Table` that is the concatenated table:\n', '            If the number of input tables is > 1, then the returned table is a :obj:`datasets.table.ConcatenationTable`.\n', ""            Otherwise if there's only one table, it is returned as is.\n"", '    """"""\n']"
parser/test3/table.py:list_table_cache_files,list_table_cache_files,function,7,26,20,245,9.42,2,1,['table'],[' Table'],[None],856,"['    """"""\n', '    Get the cache files that are loaded by the table.\n', '    Cache file are used when parts of the table come from the disk via memory mapping.\n', '\n', '    Returns:\n', '        :obj:`List[str]`: a list of paths to the cache files loaded by the table\n', '    """"""\n']"
parser/test3/table.py:cast_with_sliced_list_support,cast_with_sliced_list_support,function,25,81,58,792,9.78,0,2,"['pa_table', 'schema']","[' pa.Table', ' pa.Schema']","[None, None]",876,"['    """"""Same as pyarrow.Table.cast, except it works for sliced list arrays""""""\n']"
parser/test3/table.py:IndexedTableMixin,IndexedTableMixin,class,32,117,87,1177,10.06,1,2,[],[],[],95,[]
parser/test3/table.py:Table,Table,class,79,323,169,4139,12.81,0,3,[],[],[],141,[]
parser/test3/table.py:TableBlock,TableBlock,class,0,1,1,4,4.0,0,0,[],[],[],317,[]
parser/test3/table.py:InMemoryTable,InMemoryTable,class,23,142,65,2480,17.46,0,0,[],[],[],327,[]
parser/test3/table.py:MemoryMappedTable,MemoryMappedTable,class,40,274,129,3931,14.35,1,2,[],[],[],422,[]
parser/test3/table.py:ConcatenationTable,ConcatenationTable,class,114,737,320,7328,9.94,23,15,[],[],[],551,[]
parser/test3/table.py:IndexedTableMixin:__init__,IndexedTableMixin:__init__,method,7,13,13,141,10.85,0,0,"['self', 'table']","[None, ' pa.Table']","[None, None]",96,[]
parser/test3/table.py:IndexedTableMixin:fast_gather,IndexedTableMixin:fast_gather,method,8,26,26,272,10.46,1,0,"['self', 'indices', 'np.ndarray]']","[None, ' Union[List[int]', None]","[None, None, None]",101,"['        """"""\n', '        Create a pa.Table by gathering the records at the records at the specified indices. Should be faster\n', '        than pa.concat_tables(table.fast_slice(int(i) % table.num_rows, 1) for i in indices) since NumPy can compute\n', '        the binary searches in parallel, highly optimized C\n', '        """"""\n']"
parser/test3/table.py:IndexedTableMixin:fast_slice,IndexedTableMixin:fast_slice,method,17,61,42,608,9.97,0,2,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",117,"['        """"""\n', '        Slice the Table using interpolation search.\n', ""        The behavior is the same as :obj:`pyarrow.Table.slice` but it's significantly faster.\n"", '\n', '        Interpolation search is used to find the start and end indexes of the batches we want to keep.\n', '        The batches to keep are then concatenated to form the sliced Table.\n', '        """"""\n']"
parser/test3/table.py:Table:__init__,Table:__init__,method,3,3,3,40,13.33,0,0,"['self', 'table']","[None, ' pa.Table']","[None, None]",96,"['        """"""\n', '        Create a pa.Table by gathering the records at the records at the specified indices. Should be faster\n', '        than pa.concat_tables(table.fast_slice(int(i) % table.num_rows, 1) for i in indices) since NumPy can compute\n', '        the binary searches in parallel, highly optimized C\n', '        """"""\n']"
parser/test3/table.py:Table:__deepcopy__,Table:__deepcopy__,method,3,7,7,102,14.57,0,0,"['self', 'memo']","[None, ' dict']","[None, None]",157,[]
parser/test3/table.py:Table:__getstate__,Table:__getstate__,method,11,40,38,381,9.53,0,1,['self'],[None],[None],166,[]
parser/test3/table.py:Table:__setstate__,Table:__setstate__,method,7,29,24,278,9.59,0,1,"['self', 'state']","[None, None]","[None, None]",181,[]
parser/test3/table.py:Table:validate,Table:validate,method,2,3,3,41,13.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",193,[]
parser/test3/table.py:Table:equals,Table:equals,method,7,27,20,162,6.0,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",197,[]
parser/test3/table.py:Table:to_batches,Table:to_batches,method,2,3,3,43,14.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",203,[]
parser/test3/table.py:Table:to_pydict,Table:to_pydict,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",207,[]
parser/test3/table.py:Table:to_pandas,Table:to_pandas,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",211,[]
parser/test3/table.py:Table:to_string,Table:to_string,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",214,[]
parser/test3/table.py:Table:field,Table:field,method,2,3,3,38,12.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",218,[]
parser/test3/table.py:Table:column,Table:column,method,2,3,3,39,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",222,[]
parser/test3/table.py:Table:itercolumns,Table:itercolumns,method,2,3,3,44,14.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",226,[]
parser/test3/table.py:Table:schema,Table:schema,method,2,2,2,23,11.5,0,0,['self'],[None],[None],230,[]
parser/test3/table.py:Table:columns,Table:columns,method,2,2,2,24,12.0,0,0,['self'],[None],[None],234,[]
parser/test3/table.py:Table:num_columns,Table:num_columns,method,2,2,2,28,14.0,0,0,['self'],[None],[None],238,[]
parser/test3/table.py:Table:num_rows,Table:num_rows,method,2,2,2,25,12.5,0,0,['self'],[None],[None],242,[]
parser/test3/table.py:Table:shape,Table:shape,method,2,2,2,22,11.0,0,0,['self'],[None],[None],246,[]
parser/test3/table.py:Table:nbytes,Table:nbytes,method,2,2,2,23,11.5,0,0,['self'],[None],[None],250,[]
parser/test3/table.py:Table:column_names,Table:column_names,method,2,2,2,29,14.5,0,0,['self'],[None],[None],254,[]
parser/test3/table.py:Table:__eq__,Table:__eq__,method,2,2,2,24,12.0,0,0,"['self', 'other']","[None, None]","[None, None]",257,[]
parser/test3/table.py:Table:__getitem__,Table:__getitem__,method,2,2,2,19,9.5,0,0,"['self', 'i']","[None, None]","[None, None]",260,[]
parser/test3/table.py:Table:__len__,Table:__len__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],263,[]
parser/test3/table.py:Table:__repr__,Table:__repr__,method,2,3,3,76,25.33,0,0,['self'],[None],[None],266,[]
parser/test3/table.py:Table:__str__,Table:__str__,method,2,3,3,75,25.0,0,0,['self'],[None],[None],269,[]
parser/test3/table.py:Table:slice,Table:slice,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",273,[]
parser/test3/table.py:Table:filter,Table:filter,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",277,[]
parser/test3/table.py:Table:flatten,Table:flatten,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",281,[]
parser/test3/table.py:Table:combine_chunks,Table:combine_chunks,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",285,[]
parser/test3/table.py:Table:cast,Table:cast,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",289,[]
parser/test3/table.py:Table:add_column,Table:add_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",293,[]
parser/test3/table.py:Table:append_column,Table:append_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",297,[]
parser/test3/table.py:Table:remove_column,Table:remove_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",301,[]
parser/test3/table.py:Table:set_column,Table:set_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",305,[]
parser/test3/table.py:Table:rename_columns,Table:rename_columns,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",309,[]
parser/test3/table.py:Table:drop,Table:drop,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",313,[]
parser/test3/table.py:InMemoryTable:from_file,InMemoryTable:from_file,method,4,4,4,65,16.25,0,0,"['cls', 'filename']","[None, ' str']","[None, None]",343,[]
parser/test3/table.py:InMemoryTable:from_buffer,InMemoryTable:from_buffer,method,4,4,4,65,16.25,0,0,"['cls', 'buffer']","[None, ' pa.Buffer']","[None, None]",348,[]
parser/test3/table.py:InMemoryTable:from_pandas,InMemoryTable:from_pandas,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",354,[]
parser/test3/table.py:InMemoryTable:from_arrays,InMemoryTable:from_arrays,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",359,[]
parser/test3/table.py:InMemoryTable:from_pydict,InMemoryTable:from_pydict,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",364,[]
parser/test3/table.py:InMemoryTable:from_batches,InMemoryTable:from_batches,method,2,3,3,48,16.0,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",369,[]
parser/test3/table.py:InMemoryTable:slice,InMemoryTable:slice,method,2,3,3,65,21.67,0,0,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",373,[]
parser/test3/table.py:InMemoryTable:filter,InMemoryTable:filter,method,2,3,3,54,18.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",277,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:flatten,InMemoryTable:flatten,method,2,3,3,55,18.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",281,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:combine_chunks,InMemoryTable:combine_chunks,method,2,3,3,62,20.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",285,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:cast,InMemoryTable:cast,method,2,3,3,52,17.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",289,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:add_column,InMemoryTable:add_column,method,2,3,3,58,19.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",293,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:append_column,InMemoryTable:append_column,method,2,3,3,61,20.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",297,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:remove_column,InMemoryTable:remove_column,method,2,3,3,61,20.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",301,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:set_column,InMemoryTable:set_column,method,2,3,3,58,19.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",305,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:rename_columns,InMemoryTable:rename_columns,method,2,3,3,62,20.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",309,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:InMemoryTable:drop,InMemoryTable:drop,method,2,3,3,52,17.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",313,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:__init__,MemoryMappedTable:__init__,method,6,13,12,96,7.38,0,1,"['self', 'table', 'path', 'replays']","[None, ' pa.Table', ' str', ' Optional[List[Replay]] ']","[None, None, None, ' None']",443,[]
parser/test3/table.py:MemoryMappedTable:from_file,MemoryMappedTable:from_file,method,5,9,8,126,14.0,0,0,"['cls', 'filename', 'replays']","[None, ' str', None]","[None, None, 'None']",449,[]
parser/test3/table.py:MemoryMappedTable:__getstate__,MemoryMappedTable:__getstate__,method,1,5,5,47,9.4,0,0,['self'],[None],[None],166,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:__setstate__,MemoryMappedTable:__setstate__,method,7,13,12,198,15.23,0,0,"['self', 'state']","[None, None]","[None, None]",181,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:_apply_replays,MemoryMappedTable:_apply_replays,method,7,17,16,103,6.06,1,1,"['table', 'replays']","[' pa.Table', ' Optional[List[Replay]] ']","[None, ' None']",465,[]
parser/test3/table.py:MemoryMappedTable:_append_replay,MemoryMappedTable:_append_replay,method,4,5,4,72,14.4,0,0,"['self', 'replay']","[None, ' Replay']","[None, None]",471,[]
parser/test3/table.py:MemoryMappedTable:slice,MemoryMappedTable:slice,method,5,12,12,159,13.25,0,0,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",373,"['    """"""\n', ""    The table is said memory mapped when it doesn't use the user's RAM but loads the data\n"", '    from the disk instead.\n', '\n', ""    Pickling it doesn't copy the data into memory.\n"", '    Instead, only the path to the memory mapped arrow file is pickled, as well as the list\n', '    of transforms to ""replay"" when reloading the table from the disk.\n', '\n', '    Its implementation requires to store an history of all the transforms that were applied\n', '    to the underlying pyarrow Table, so that they can be ""replayed"" when reloading the Table\n', '    from the disk.\n', '\n', '    This is different from the InMemoryTable table, for which pickling does copy all the\n', '    data in memory.\n', '\n', '    InMemoryTable must be used when data fit in memory, while MemoryMapped are reserved for\n', '    data bigger than memory or when you want the memory footprint of your application to\n', '    stay low.\n', '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:filter,MemoryMappedTable:filter,method,5,11,11,172,15.64,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",277,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:flatten,MemoryMappedTable:flatten,method,5,11,11,174,15.82,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",281,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:combine_chunks,MemoryMappedTable:combine_chunks,method,5,11,11,188,17.09,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",285,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:cast,MemoryMappedTable:cast,method,5,11,11,168,15.27,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",289,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:add_column,MemoryMappedTable:add_column,method,5,11,11,180,16.36,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",293,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:append_column,MemoryMappedTable:append_column,method,5,11,11,186,16.91,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",297,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:remove_column,MemoryMappedTable:remove_column,method,5,11,11,186,16.91,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",301,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:set_column,MemoryMappedTable:set_column,method,5,11,11,180,16.36,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",305,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:rename_columns,MemoryMappedTable:rename_columns,method,5,11,11,188,17.09,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",309,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:MemoryMappedTable:drop,MemoryMappedTable:drop,method,5,11,11,168,15.27,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",313,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:ConcatenationTable:__init__,ConcatenationTable:__init__,method,7,33,31,244,7.39,2,1,"['self', 'table', 'blocks']","[None, ' pa.Table', ' List[List[TableBlock]]']","[None, None, None]",574,[]
parser/test3/table.py:ConcatenationTable:__getstate__,ConcatenationTable:__getstate__,method,1,3,3,28,9.33,0,0,['self'],[None],[None],166,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:ConcatenationTable:__setstate__,ConcatenationTable:__setstate__,method,5,7,7,138,19.71,0,0,"['self', 'state']","[None, None]","[None, None]",181,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:ConcatenationTable:_concat_blocks,ConcatenationTable:_concat_blocks,method,17,70,48,520,7.43,3,3,"['blocks', 'pa.Table]]', 'axis']","[' List[Union[TableBlock', None, ' int ']","[None, None, ' 0']",596,[]
parser/test3/table.py:ConcatenationTable:_concat_blocks_horizontally_and_vertically,ConcatenationTable:_concat_blocks_horizontally_and_vertically,method,9,18,18,294,16.33,1,1,"['cls', 'blocks']","[None, ' List[List[TableBlock]]']","[None, None]",620,[]
parser/test3/table.py:ConcatenationTable:_merge_blocks,ConcatenationTable:_merge_blocks,method,10,56,38,497,8.88,1,3,"['cls', 'blocks', 'axis']","[None, ' TableBlockContainer', ' Optional[int] ']","[None, None, ' None']",630,[]
parser/test3/table.py:ConcatenationTable:_consolidate_blocks,ConcatenationTable:_consolidate_blocks,method,4,14,11,160,11.43,0,1,"['cls', 'blocks']","[None, ' TableBlockContainer']","[None, None]",646,[]
parser/test3/table.py:ConcatenationTable:from_blocks,ConcatenationTable:from_blocks,method,10,31,24,335,10.81,1,1,"['cls', 'blocks']","[None, ' TableBlockContainer']","[None, None]",655,[]
parser/test3/table.py:ConcatenationTable:from_tables,ConcatenationTable:from_tables,method,25,108,74,963,8.92,3,3,"['cls', 'tables', 'Table]]', 'axis']","[None, ' List[Union[pa.Table', None, ' int ']","[None, None, None, ' 0']",669,"['        """"""Create ConcatenationTable from list of tables.\n', '\n', '        Args:\n', '            tables (list of :class:`Table` or list of :obj:`pyarrow.Table`): List of tables.\n', '            axis: (``{0, 1}``, default ``0``, meaning over rows):\n', '            Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n', '            (horizontally).\n', '\n', '            .. versionadded:: 1.6.0\n', '        """"""\n']"
parser/test3/table.py:ConcatenationTable:_slices,ConcatenationTable:_slices,method,4,14,12,90,6.43,1,0,['self'],[None],[None],720,[]
parser/test3/table.py:ConcatenationTable:slice,ConcatenationTable:slice,method,15,60,39,439,7.32,1,2,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",373,"['    """"""\n', ""    The table is said memory mapped when it doesn't use the user's RAM but loads the data\n"", '    from the disk instead.\n', '\n', ""    Pickling it doesn't copy the data into memory.\n"", '    Instead, only the path to the memory mapped arrow file is pickled, as well as the list\n', '    of transforms to ""replay"" when reloading the table from the disk.\n', '\n', '    Its implementation requires to store an history of all the transforms that were applied\n', '    to the underlying pyarrow Table, so that they can be ""replayed"" when reloading the Table\n', '    from the disk.\n', '\n', '    This is different from the InMemoryTable table, for which pickling does copy all the\n', '    data in memory.\n', '\n', '    InMemoryTable must be used when data fit in memory, while MemoryMapped are reserved for\n', '    data bigger than memory or when you want the memory footprint of your application to\n', '    stay low.\n', '    """"""\n']"
parser/test3/table.py:ConcatenationTable:filter,ConcatenationTable:filter,method,8,26,23,247,9.5,1,0,"['self', 'mask', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",747,[]
parser/test3/table.py:ConcatenationTable:flatten,ConcatenationTable:flatten,method,8,18,16,168,9.33,1,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",281,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:ConcatenationTable:combine_chunks,ConcatenationTable:combine_chunks,method,8,18,16,182,10.11,1,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",285,"['    """"""\n', '    TableBlock is the allowed class inside a ConcanetationTable.\n', '    Only MemoryMappedTable and InMemoryTable are TableBlock.\n', ""    This is because we don't want a ConcanetationTable made out of other ConcanetationTables.\n"", '    """"""\n']"
parser/test3/table.py:ConcatenationTable:cast,ConcatenationTable:cast,method,18,42,33,438,10.43,3,0,"['self', 'target_schema', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",772,[]
parser/test3/table.py:ConcatenationTable:add_column,ConcatenationTable:add_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",293,[]
parser/test3/table.py:ConcatenationTable:append_column,ConcatenationTable:append_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",297,[]
parser/test3/table.py:ConcatenationTable:remove_column,ConcatenationTable:remove_column,method,11,32,27,273,8.53,2,0,"['self', 'i', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",796,[]
parser/test3/table.py:ConcatenationTable:set_column,ConcatenationTable:set_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",305,[]
parser/test3/table.py:ConcatenationTable:rename_columns,ConcatenationTable:rename_columns,method,9,29,24,274,9.45,1,0,"['self', 'names', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",814,[]
parser/test3/table.py:ConcatenationTable:drop,ConcatenationTable:drop,method,8,26,20,191,7.35,1,0,"['self', 'columns', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",825,[]
parser/test3/commands/datasets_cli.py:main,main,function,18,29,29,564,19.45,0,1,[],[],[],12,[]
parser/test3/commands/dummy_data.py:test_command_factory,test_command_factory,function,2,12,12,190,15.83,0,0,['args'],[None],[None],26,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager,DummyDataGeneratorDownloadManager,class,100,481,279,5285,10.99,7,15,[],[],[],40,[]
parser/test3/commands/dummy_data.py:DummyDataCommand,DummyDataCommand,class,121,997,481,9598,9.63,4,11,[],[],[],215,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:__init__,DummyDataGeneratorDownloadManager:__init__,method,5,8,8,141,17.62,0,0,"['self', 'mock_download_manager', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",41,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:download,DummyDataGeneratorDownloadManager:download,method,7,27,15,567,21.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",47,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:download_and_extract,DummyDataGeneratorDownloadManager:download_and_extract,method,6,12,10,270,22.5,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",54,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:auto_generate_dummy_data_folder,DummyDataGeneratorDownloadManager:auto_generate_dummy_data_folder,method,12,67,54,854,12.75,1,1,"['self', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'encoding', '']","[None, ' int ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' 5', ' None', ' None', ' None', ' None', None]",61,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:_create_dummy_data,DummyDataGeneratorDownloadManager:_create_dummy_data,method,58,255,160,2376,9.32,5,10,"['self', 'src_path', 'dst_path', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'encoding', '']","[None, ' str', ' str', ' int', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, None, ' None', ' None', ' None', ' None', None]",103,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:_create_xml_dummy_data,DummyDataGeneratorDownloadManager:_create_xml_dummy_data,method,17,42,35,404,9.62,1,4,"['src_path', 'dst_path', 'xml_tag', 'n_lines', 'encoding']","[None, None, None, None, None]","[None, None, None, '5', 'DEFAULT_ENCODING']",188,[]
parser/test3/commands/dummy_data.py:DummyDataGeneratorDownloadManager:compress_autogenerated_dummy_data,DummyDataGeneratorDownloadManager:compress_autogenerated_dummy_data,method,7,19,19,293,15.42,0,0,"['self', 'path_to_dataset']","[None, None]","[None, None]",206,[]
parser/test3/commands/dummy_data.py:DummyDataCommand:register_subcommand,DummyDataCommand:register_subcommand,method,4,181,105,1560,8.62,0,0,['parser'],[' ArgumentParser'],[None],217,[]
parser/test3/commands/dummy_data.py:DummyDataCommand:__init__,DummyDataCommand:__init__,method,22,31,29,521,16.81,0,1,"['self', 'path_to_dataset', 'auto_generate', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'keep_uncompressed', 'cache_dir', 'encoding', '']","[None, ' str', ' bool', ' int', ' Optional[str]', ' Optional[str]', ' Optional[str]', ' bool', ' Optional[str]', ' Optional[str]', None]","[None, None, None, None, None, None, None, None, None, None, None]",261,[]
parser/test3/commands/dummy_data.py:DummyDataCommand:run,DummyDataCommand:run,method,28,87,68,1208,13.89,1,4,['self'],[None],[None],288,[]
parser/test3/commands/dummy_data.py:DummyDataCommand:_autogenerate_dummy_data,DummyDataCommand:_autogenerate_dummy_data,method,34,169,115,2065,12.22,1,3,"['self', 'dataset_builder', 'mock_dl_manager', 'keep_uncompressed']","[None, None, None, None]","[None, None, None, None]",332,[]
parser/test3/commands/dummy_data.py:DummyDataCommand:_print_dummy_data_instructions,DummyDataCommand:_print_dummy_data_instructions,method,40,490,211,3761,7.68,2,3,"['self', 'dataset_builder', 'mock_dl_manager']","[None, None, None]","[None, None, None]",390,[]
parser/test3/commands/env.py:info_command_factory,info_command_factory,function,2,2,2,26,13.0,0,0,['_'],[None],[None],10,[]
parser/test3/commands/env.py:EnvironmentCommand,EnvironmentCommand,class,12,53,46,569,10.74,0,0,[],[],[],14,[]
parser/test3/commands/env.py:EnvironmentCommand:register_subcommand,EnvironmentCommand:register_subcommand,method,3,8,8,139,17.38,0,0,['parser'],[' ArgumentParser'],[None],16,[]
parser/test3/commands/env.py:EnvironmentCommand:run,EnvironmentCommand:run,method,6,25,22,249,9.96,0,0,['self'],[None],[None],20,[]
parser/test3/commands/env.py:EnvironmentCommand:format_dict,EnvironmentCommand:format_dict,method,1,11,11,71,6.45,0,0,['d'],[None],[None],34,[]
parser/test3/commands/run_beam.py:run_beam_command_factory,run_beam_command_factory,function,2,12,12,192,16.0,0,0,['args'],[None],[None],14,[]
parser/test3/commands/run_beam.py:RunBeamCommand,RunBeamCommand,class,64,284,208,3579,12.6,2,5,[],[],[],28,[]
parser/test3/commands/run_beam.py:RunBeamCommand:register_subcommand,RunBeamCommand:register_subcommand,method,4,96,69,1192,12.42,0,0,['parser'],[' ArgumentParser'],[None],30,[]
parser/test3/commands/run_beam.py:RunBeamCommand:__init__,RunBeamCommand:__init__,method,18,18,18,283,15.72,0,0,"['self', 'dataset', 'name', 'cache_dir', 'beam_pipeline_options', 'data_dir', 'all_configs', 'save_infos', 'ignore_verifications', 'force_redownload', '']","[None, ' str', ' str', ' str', ' str', ' str', ' bool', ' bool', ' bool', ' bool', None]","[None, None, None, None, None, None, None, None, None, None, None]",60,[]
parser/test3/commands/run_beam.py:RunBeamCommand:run,RunBeamCommand:run,method,45,142,111,1841,12.96,2,5,['self'],[None],[None],82,[]
parser/test3/commands/test.py:test_command_factory,test_command_factory,function,2,14,14,210,15.0,0,0,['args'],[None],[None],19,[]
parser/test3/commands/test.py:TestCommand,TestCommand,class,69,413,265,4505,10.91,2,14,[],[],[],35,[]
parser/test3/commands/test.py:TestCommand:register_subcommand,TestCommand:register_subcommand,method,4,120,82,1313,10.94,0,0,['parser'],[' ArgumentParser'],[None],37,[]
parser/test3/commands/test.py:TestCommand:__init__,TestCommand:__init__,method,25,72,61,599,8.32,0,1,"['self', 'dataset', 'name', 'cache_dir', 'data_dir', 'all_configs', 'save_infos', 'ignore_verifications', 'force_redownload', 'clear_cache', 'proc_rank', 'num_proc', '']","[None, ' str', ' str', ' str', ' str', ' bool', ' bool', ' bool', ' bool', ' bool', ' int', ' int', None]","[None, None, None, None, None, None, None, None, None, None, None, None, None]",78,[]
parser/test3/commands/test.py:TestCommand:run,TestCommand:run,method,48,189,133,2310,12.22,2,13,['self'],[None],[None],111,[]
parser/test3/commands/__init__.py:BaseDatasetsCLICommand,BaseDatasetsCLICommand,class,3,12,9,160,13.33,0,0,[],[],[],5,[]
parser/test3/commands/__init__.py:BaseDatasetsCLICommand:register_subcommand,BaseDatasetsCLICommand:register_subcommand,method,1,2,2,26,13.0,0,0,['parser'],[' ArgumentParser'],[None],8,[]
parser/test3/commands/__init__.py:BaseDatasetsCLICommand:run,BaseDatasetsCLICommand:run,method,1,2,2,26,13.0,0,0,['self'],[None],[None],12,[]
parser/test3/filesystems/s3filesystem.py:S3FileSystem,S3FileSystem,class,2,4,4,67,16.75,0,0,[],[],[],4,[]
parser/test3/filesystems/__init__.py:extract_path_from_uri,extract_path_from_uri,function,3,8,7,83,10.38,0,1,['dataset_path'],[' str'],[None],18,"['    """"""\n', '    preprocesses `dataset_path` and removes remote filesystem (e.g. removing ``s3://``)\n', '\n', '    Args:\n', '        dataset_path (``str``): path (e.g. ``dataset/train``) or remote uri (e.g. ``s3://my-bucket/dataset/train``) of the dataset directory\n', '    """"""\n']"
parser/test3/filesystems/__init__.py:is_remote_filesystem,is_remote_filesystem,function,4,13,12,65,5.0,0,1,['fs'],[' fsspec.spec.AbstractFileSystem'],[None],30,"['    """"""\n', '    Validates if filesystem has remote protocol.\n', '\n', '    Args:\n', ""        fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystems.S3FileSystem`\n"", '    """"""\n']"
parser/test3/formatting/formatting.py:_is_range_contiguous,_is_range_contiguous,function,5,6,6,39,6.5,0,0,['key'],[' range'],[None],34,[]
parser/test3/formatting/formatting.py:_raise_bad_key_type,_raise_bad_key_type,function,1,19,18,104,5.47,0,0,['key'],[' Any'],[None],38,[]
parser/test3/formatting/formatting.py:_query_table_with_indices_mapping,_query_table_with_indices_mapping,function,13,69,46,670,9.71,0,6,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'indices']","[' Table', ' Union[int', None, None, None, None, ' Table']","[None, None, None, None, None, None, None]",44,"['    """"""\n', '    Query a pyarrow Table to extract the subtable that correspond to the given key.\n', '    The :obj:`indices` parameter corresponds to the indices mapping in case we cant to take into\n', '    account a shuffling or an indices selection for example.\n', '    The indices table must contain one column named ""indices"" of type uint64.\n', '    """"""\n']"
parser/test3/formatting/formatting.py:_query_table,_query_table,function,18,140,70,1303,9.31,0,13,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'indices']","[' Table', ' Union[int', None, None, None, None, ' Table']","[None, None, None, None, None, None, None]",74,[]
parser/test3/formatting/formatting.py:_unnest,_unnest,function,5,8,8,49,6.12,1,0,"['py_dict', 'List[T]]']","[' Dict[str', None]","[None, None]",116,"['    """"""Return the first element of a batch (dict) as a row (dict)""""""\n']"
parser/test3/formatting/formatting.py:_check_valid_column_key,_check_valid_column_key,function,3,19,15,109,5.74,0,1,"['key', 'columns']","[' str', ' List[str]']","[None, None]",303,[]
parser/test3/formatting/formatting.py:_check_valid_index_key,_check_valid_index_key,function,5,55,38,459,8.35,0,4,"['key', 'slice', 'range', 'Iterable]', 'size']","[' Union[int', None, None, None, ' int']","[None, None, None, None, None]",308,[]
parser/test3/formatting/formatting.py:key_to_query_type,key_to_query_type,function,3,18,13,157,8.72,0,1,"['key', 'slice', 'range', 'str', 'Iterable]']","[' Union[int', None, None, None, None]","[None, None, None, None, None]",327,[]
parser/test3/formatting/formatting.py:query_table,query_table,function,12,40,30,395,9.88,0,4,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'indices', '']","[' Table', ' Union[int', None, None, None, None, ' Optional[Table] ', None]","[None, None, None, None, None, None, ' None', None]",337,"['    """"""\n', '    Query a Table to extract the subtable that correspond to the given key.\n', '\n', '    Args:\n', '        table (``datasets.table.Table``): The input Table to query from\n', '        key (``Union[int, slice, range, str, Iterable]``): The key can be of different types:\n', '            - an integer i: the subtable containing only the i-th row\n', '            - a slice [i:j:k]: the subtable containing the rows that correspond to this slice\n', '            - a range(i, j, k): the subtable containing the rows that correspond to this range\n', '            - a string c: the subtable containing all the rows but only the column c\n', '            - an iterable l: the subtable that is the concatenation of all the i-th rows for all i in the iterable\n', '        indices (Optional ``datasets.table.Table``): If not None, it is used to re-map the given key to the table rows.\n', '            The indices table must contain one column named ""indices"" of type uint64.\n', '            This is used in case of shuffling or rows selection.\n', '\n', '\n', '    Returns:\n', '        ``pyarrow.Table``: the result of the query on the input table\n', '    """"""\n']"
parser/test3/formatting/formatting.py:format_table,format_table,function,20,89,58,985,11.07,0,5,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'formatter', 'format_columns', 'output_all_columns', '']","[' Table', ' Union[int', None, None, None, None, ' Formatter', ' Optional[list] ', None, None]","[None, None, None, None, None, None, None, ' None', 'False', None]",377,"['    """"""\n', '    Format a Table depending on the key that was used and a Formatter object.\n', '\n', '    Args:\n', '        table (``datasets.table.Table``): The input Table to format\n', '        key (``Union[int, slice, range, str, Iterable]``): Depending on the key that was used, the formatter formats\n', '            the table as either a row, a column or a batch.\n', '        formatter (``datasets.formatting.formatting.Formatter``): Any subclass of a Formatter such as\n', '            PythonFormatter, NumpyFormatter, etc.\n', '        format_columns (Optional ``List[str]``): if not None, it defines the columns that will be formatted using the\n', '            given formatter. Other columns are discarded (unless ``output_all_columns`` is True)\n', '        output_all_columns (``bool``, defaults to False). If True, the formatted output is completed using the columns\n', '            that are not in the ``format_columns`` list. For these columns, the PythonFormatter is used.\n', '\n', '\n', '    Returns:\n', '        A row, column or batch formatted object defined by the Formatter:\n', '        - the PythonFormatter returns a dictionary for a row or a batch, and a list for a column.\n', '        - the NumpyFormatter returns a dictionary for a row or a batch, and a np.array for a column.\n', '        - the PandasFormatter returns a pd.DataFrame for a row or a batch, and a pd.Series for a column.\n', '        - the TorchFormatter returns a dictionary for a row or a batch, and a torch.Tensor for a column.\n', '        - the TFFormatter returns a dictionary for a row or a batch, and a tf.Tensor for a column.\n', '    """"""\n']"
parser/test3/formatting/formatting.py:BaseArrowExtractor,BaseArrowExtractor,class,4,24,13,237,9.88,0,0,[],[],[],99,[]
parser/test3/formatting/formatting.py:SimpleArrowExtractor,SimpleArrowExtractor,class,6,24,13,209,8.71,0,0,[],[],[],121,[]
parser/test3/formatting/formatting.py:PythonArrowExtractor,PythonArrowExtractor,class,7,24,13,242,10.08,0,0,[],[],[],132,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor,NumpyArrowExtractor,class,21,66,43,821,12.44,2,1,[],[],[],143,[]
parser/test3/formatting/formatting.py:PandasArrowExtractor,PandasArrowExtractor,class,7,24,13,394,16.42,0,0,[],[],[],170,[]
parser/test3/formatting/formatting.py:Formatter,Formatter,class,18,57,38,676,11.86,0,1,[],[],[],181,[]
parser/test3/formatting/formatting.py:ArrowFormatter,ArrowFormatter,class,5,24,13,330,13.75,0,0,[],[],[],210,[]
parser/test3/formatting/formatting.py:PythonFormatter,PythonFormatter,class,5,24,13,318,13.25,0,0,[],[],[],221,[]
parser/test3/formatting/formatting.py:NumpyFormatter,NumpyFormatter,class,8,29,17,461,15.9,0,0,[],[],[],232,[]
parser/test3/formatting/formatting.py:PandasFormatter,PandasFormatter,class,5,24,13,339,14.12,0,0,[],[],[],246,[]
parser/test3/formatting/formatting.py:CustomFormatter,CustomFormatter,class,15,138,66,1069,7.75,0,2,[],[],[],257,[]
parser/test3/formatting/formatting.py:BaseArrowExtractor:extract_row,BaseArrowExtractor:extract_row,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",106,[]
parser/test3/formatting/formatting.py:BaseArrowExtractor:extract_column,BaseArrowExtractor:extract_column,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",109,[]
parser/test3/formatting/formatting.py:BaseArrowExtractor:extract_batch,BaseArrowExtractor:extract_batch,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",112,[]
parser/test3/formatting/formatting.py:SimpleArrowExtractor:extract_row,SimpleArrowExtractor:extract_row,method,2,2,2,14,7.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",122,[]
parser/test3/formatting/formatting.py:SimpleArrowExtractor:extract_column,SimpleArrowExtractor:extract_column,method,2,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",125,[]
parser/test3/formatting/formatting.py:SimpleArrowExtractor:extract_batch,SimpleArrowExtractor:extract_batch,method,2,2,2,14,7.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",128,[]
parser/test3/formatting/formatting.py:PythonArrowExtractor:extract_row,PythonArrowExtractor:extract_row,method,2,2,2,35,17.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",133,[]
parser/test3/formatting/formatting.py:PythonArrowExtractor:extract_column,PythonArrowExtractor:extract_column,method,2,2,2,36,18.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",136,[]
parser/test3/formatting/formatting.py:PythonArrowExtractor:extract_batch,PythonArrowExtractor:extract_batch,method,2,2,2,26,13.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",139,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor:__init__,NumpyArrowExtractor:__init__,method,2,2,2,36,18.0,0,0,"['self', '**np_array_kwargs']","[None, None]","[None, None]",144,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor:extract_row,NumpyArrowExtractor:extract_row,method,2,2,2,43,21.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",133,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor:extract_column,NumpyArrowExtractor:extract_column,method,2,2,2,68,34.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",150,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor:extract_batch,NumpyArrowExtractor:extract_batch,method,2,7,7,82,11.71,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",139,[]
parser/test3/formatting/formatting.py:NumpyArrowExtractor:_arrow_array_to_numpy,NumpyArrowExtractor:_arrow_array_to_numpy,method,12,26,21,340,13.08,2,1,"['self', 'pa_array']","[None, ' pa.Array']","[None, None]",156,[]
parser/test3/formatting/formatting.py:PandasArrowExtractor:extract_row,PandasArrowExtractor:extract_row,method,2,2,2,74,37.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",171,[]
parser/test3/formatting/formatting.py:PandasArrowExtractor:extract_column,PandasArrowExtractor:extract_column,method,2,2,2,96,48.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",174,[]
parser/test3/formatting/formatting.py:PandasArrowExtractor:extract_batch,PandasArrowExtractor:extract_batch,method,2,2,2,58,29.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",177,[]
parser/test3/formatting/formatting.py:Formatter:__call__,Formatter:__call__,method,5,15,10,172,11.47,0,1,"['self', 'pa_table', 'query_type']","[None, ' pa.Table', ' str']","[None, None, None]",192,[]
parser/test3/formatting/formatting.py:Formatter:format_row,Formatter:format_row,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",200,[]
parser/test3/formatting/formatting.py:Formatter:format_column,Formatter:format_column,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",203,[]
parser/test3/formatting/formatting.py:Formatter:format_batch,Formatter:format_batch,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",206,[]
parser/test3/formatting/formatting.py:ArrowFormatter:format_row,ArrowFormatter:format_row,method,2,2,2,57,28.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",211,[]
parser/test3/formatting/formatting.py:ArrowFormatter:format_column,ArrowFormatter:format_column,method,2,2,2,60,30.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",214,[]
parser/test3/formatting/formatting.py:ArrowFormatter:format_batch,ArrowFormatter:format_batch,method,2,2,2,59,29.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",217,[]
parser/test3/formatting/formatting.py:PythonFormatter:format_row,PythonFormatter:format_row,method,2,2,2,57,28.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",222,[]
parser/test3/formatting/formatting.py:PythonFormatter:format_column,PythonFormatter:format_column,method,2,2,2,60,30.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",225,[]
parser/test3/formatting/formatting.py:PythonFormatter:format_batch,PythonFormatter:format_batch,method,2,2,2,59,29.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",228,[]
parser/test3/formatting/formatting.py:NumpyFormatter:__init__,NumpyFormatter:__init__,method,2,2,2,36,18.0,0,0,"['self', '**np_array_kwargs']","[None, None]","[None, None]",144,[]
parser/test3/formatting/formatting.py:NumpyFormatter:format_row,NumpyFormatter:format_row,method,2,2,2,78,39.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",222,[]
parser/test3/formatting/formatting.py:NumpyFormatter:format_column,NumpyFormatter:format_column,method,2,2,2,81,40.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",239,[]
parser/test3/formatting/formatting.py:NumpyFormatter:format_batch,NumpyFormatter:format_batch,method,2,2,2,80,40.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",228,[]
parser/test3/formatting/formatting.py:PandasFormatter:format_row,PandasFormatter:format_row,method,2,2,2,57,28.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",247,[]
parser/test3/formatting/formatting.py:PandasFormatter:format_column,PandasFormatter:format_column,method,2,2,2,60,30.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",250,[]
parser/test3/formatting/formatting.py:PandasFormatter:format_batch,PandasFormatter:format_batch,method,2,2,2,59,29.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",253,[]
parser/test3/formatting/formatting.py:CustomFormatter:__init__,CustomFormatter:__init__,method,2,2,2,24,12.0,0,0,"['self', 'transform', 'dict]']","[None, ' Callable[[dict]', None]","[None, None, None]",267,[]
parser/test3/formatting/formatting.py:CustomFormatter:format_row,CustomFormatter:format_row,method,5,27,24,199,7.37,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",222,"['    """"""\n', '    A user-defined custom formatter function defined by a ``transform``.\n', '    The transform must take as input a batch of data extracted for an arrow table using the python extractor,\n', '    and return a batch.\n', ""    If the output batch is not a dict, then output_all_columns won't work.\n"", ""    If the ouput batch has several fields, then querying a single column won't work since we don't know which field\n"", '    to return.\n', '    """"""\n']"
parser/test3/formatting/formatting.py:CustomFormatter:format_column,CustomFormatter:format_column,method,6,82,48,557,6.79,0,2,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",203,"['    """"""\n', '    A user-defined custom formatter function defined by a ``transform``.\n', '    The transform must take as input a batch of data extracted for an arrow table using the python extractor,\n', '    and return a batch.\n', ""    If the output batch is not a dict, then output_all_columns won't work.\n"", ""    If the ouput batch has several fields, then querying a single column won't work since we don't know which field\n"", '    to return.\n', '    """"""\n']"
parser/test3/formatting/formatting.py:CustomFormatter:format_batch,CustomFormatter:format_batch,method,4,4,4,87,21.75,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",228,"['    """"""\n', '    A user-defined custom formatter function defined by a ``transform``.\n', '    The transform must take as input a batch of data extracted for an arrow table using the python extractor,\n', '    and return a batch.\n', ""    If the output batch is not a dict, then output_all_columns won't work.\n"", ""    If the ouput batch has several fields, then querying a single column won't work since we don't know which field\n"", '    to return.\n', '    """"""\n']"
parser/test3/formatting/jax_formatter.py:JaxFormatter,JaxFormatter,class,42,119,82,1364,11.46,0,4,[],[],[],30,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:__init__,JaxFormatter:__init__,method,9,12,11,89,7.42,0,0,"['self', '**jnp_array_kwargs']","[None, None]","[None, None]",31,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:_tensorize,JaxFormatter:_tensorize,method,9,30,22,338,11.27,0,2,"['self', 'value']","[None, None]","[None, None]",35,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:_recursive_tensorize,JaxFormatter:_recursive_tensorize,method,17,29,27,273,9.41,0,2,"['self', 'data_struct']","[None, ' dict']","[None, None]",54,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:recursive_tensorize,JaxFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",62,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:format_row,JaxFormatter:format_row,method,4,4,4,90,22.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",65,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:format_column,JaxFormatter:format_column,method,4,4,4,93,23.25,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",69,[]
parser/test3/formatting/jax_formatter.py:JaxFormatter:format_batch,JaxFormatter:format_batch,method,4,4,4,96,24.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",73,[]
parser/test3/formatting/tf_formatter.py:TFFormatter,TFFormatter,class,28,116,80,1321,11.39,0,3,[],[],[],30,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:__init__,TFFormatter:__init__,method,8,12,10,89,7.42,0,0,"['self', '**tf_tensor_kwargs']","[None, None]","[None, None]",31,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:_tensorize,TFFormatter:_tensorize,method,7,22,18,266,12.09,0,1,"['self', 'value']","[None, None]","[None, None]",35,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:_recursive_tensorize,TFFormatter:_recursive_tensorize,method,4,34,31,304,8.94,0,2,"['self', 'data_struct']","[None, ' dict']","[None, None]",46,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:recursive_tensorize,TFFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",58,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:format_row,TFFormatter:format_row,method,4,4,4,90,22.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",61,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:format_column,TFFormatter:format_column,method,4,4,4,93,23.25,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",65,[]
parser/test3/formatting/tf_formatter.py:TFFormatter:format_batch,TFFormatter:format_batch,method,4,4,4,96,24.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",69,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter,TorchFormatter,class,40,107,76,1294,12.09,0,3,[],[],[],30,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:__init__,TorchFormatter:__init__,method,7,10,8,88,8.8,0,0,"['self', '**torch_tensor_kwargs']","[None, None]","[None, None]",31,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:_tensorize,TorchFormatter:_tensorize,method,6,20,16,260,13.0,0,1,"['self', 'value']","[None, None]","[None, None]",35,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:_recursive_tensorize,TorchFormatter:_recursive_tensorize,method,17,29,27,278,9.59,0,2,"['self', 'data_struct']","[None, ' dict']","[None, None]",46,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:recursive_tensorize,TorchFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",54,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:format_row,TorchFormatter:format_row,method,4,4,4,90,22.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",57,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:format_column,TorchFormatter:format_column,method,4,4,4,93,23.25,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",61,[]
parser/test3/formatting/torch_formatter.py:TorchFormatter:format_batch,TorchFormatter:format_batch,method,4,4,4,96,24.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",65,[]
parser/test3/formatting/__init__.py:_register_formatter,_register_formatter,function,9,45,30,448,9.96,1,3,"['formatter_cls', 'format_type', 'aliases']","[' type', ' Optional[str]', ' Optional[List[str]] ']","[None, None, ' None']",42,"['    """"""\n', '    Register a Formatter object using a name and optional aliases.\n', '    This function must be used on a Formatter class.\n', '    """"""\n']"
parser/test3/formatting/__init__.py:_register_unavailable_formatter,_register_unavailable_formatter,function,4,16,14,136,8.5,1,1,"['unavailable_error', 'format_type', 'aliases']","[' Exception', ' Optional[str]', ' Optional[List[str]] ']","[None, None, ' None']",61,"['    """"""\n', '    Register an unavailable Formatter object using a name and optional aliases.\n', '    This function must be used on an Exception object that is raised when trying to get the unavailable formatter.\n', '    """"""\n']"
parser/test3/formatting/__init__.py:get_format_type_from_alias,get_format_type_from_alias,function,4,9,7,102,11.33,0,1,['format_type'],[' Optional[str]'],[None],105,"['    """"""If the given format type is a known alias, then return its main type name. Otherwise return the type with no change.""""""\n']"
parser/test3/formatting/__init__.py:get_formatter,get_formatter,function,8,40,30,382,9.55,0,2,"['format_type', '**format_kwargs']","[' Optional[str]', None]","[None, None]",113,"['    """"""\n', '    Factory function to get a Formatter given its type name and keyword arguments.\n', '    A formatter is an object that extracts and formats data from pyarrow table.\n', '    It defines the formatting for rows, colums and batches.\n', ""    If the formatter for a given type name doesn't exist or is not available, an error is raised.\n"", '    """"""\n']"
parser/test3/io/abc.py:AbstractDatasetReader,AbstractDatasetReader,class,16,45,41,460,10.22,0,1,[],[],[],9,[]
parser/test3/io/abc.py:AbstractDatasetReader:__init__,AbstractDatasetReader:__init__,method,14,19,18,201,10.58,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",10,[]
parser/test3/io/abc.py:AbstractDatasetReader:read,AbstractDatasetReader:read,method,0,1,1,4,4.0,0,0,['self'],[None],[None],27,[]
parser/test3/io/csv.py:CsvDatasetReader,CsvDatasetReader,class,21,69,57,952,13.8,0,1,[],[],[],11,[]
parser/test3/io/csv.py:CsvDatasetWriter,CsvDatasetWriter,class,32,112,89,1173,10.47,1,4,[],[],[],55,[]
parser/test3/io/csv.py:CsvDatasetReader:__init__,CsvDatasetReader:__init__,method,10,23,22,308,13.39,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",12,[]
parser/test3/io/csv.py:CsvDatasetReader:read,CsvDatasetReader:read,method,9,25,20,433,17.32,0,0,['self'],[None],[None],32,[]
parser/test3/io/csv.py:CsvDatasetWriter:__init__,CsvDatasetWriter:__init__,method,8,8,8,109,13.62,0,0,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', '**to_csv_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, None, ' None', None, None]",12,[]
parser/test3/io/csv.py:CsvDatasetWriter:write,CsvDatasetWriter:write,method,9,27,21,364,13.48,0,2,['self'],[None],[None],68,[]
parser/test3/io/csv.py:CsvDatasetWriter:_write,CsvDatasetWriter:_write,method,16,43,37,441,10.26,1,2,"['self', 'file_obj', 'batch_size', 'header', 'encoding', '**to_csv_kwargs']","[None, ' BinaryIO', ' int', ' bool ', ' str ', None]","[None, None, None, ' True', ' ""utf-8""', None]",78,"['        """"""Writes the pyarrow table as CSV to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']"
parser/test3/io/json.py:JsonDatasetReader,JsonDatasetReader,class,24,78,65,1063,13.63,0,1,[],[],[],12,[]
parser/test3/io/json.py:JsonDatasetWriter,JsonDatasetWriter,class,30,104,83,1177,11.32,1,3,[],[],[],60,[]
parser/test3/io/json.py:JsonDatasetReader:__init__,JsonDatasetReader:__init__,method,12,26,25,339,13.04,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'field', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[str] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', None, None]",13,[]
parser/test3/io/json.py:JsonDatasetReader:read,JsonDatasetReader:read,method,10,28,23,487,17.39,0,0,['self'],[None],[None],36,[]
parser/test3/io/json.py:JsonDatasetWriter:__init__,JsonDatasetWriter:__init__,method,8,8,8,111,13.88,0,0,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', '**to_json_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, None, ' None', None, None]",13,[]
parser/test3/io/json.py:JsonDatasetWriter:write,JsonDatasetWriter:write,method,9,27,21,366,13.56,0,2,['self'],[None],[None],73,[]
parser/test3/io/json.py:JsonDatasetWriter:_write,JsonDatasetWriter:_write,method,14,36,33,421,11.69,1,1,"['self', 'file_obj', 'batch_size', 'encoding', 'orient', 'lines', '**to_json_kwargs', '']","[None, ' BinaryIO', ' int', ' str ', None, None, None, None]","[None, None, None, ' ""utf-8""', '""records""', 'True', None, None]",83,"['        """"""Writes the pyarrow table as JSON lines to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']"
parser/test3/io/parquet.py:ParquetDatasetReader,ParquetDatasetReader,class,25,91,77,1172,12.88,0,2,[],[],[],16,[]
parser/test3/io/parquet.py:ParquetDatasetWriter,ParquetDatasetWriter,class,35,118,96,1340,11.36,1,4,[],[],[],66,[]
parser/test3/io/parquet.py:ParquetDatasetReader:__init__,ParquetDatasetReader:__init__,method,14,45,42,528,11.73,0,2,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",17,[]
parser/test3/io/parquet.py:ParquetDatasetReader:read,ParquetDatasetReader:read,method,9,25,20,433,17.32,0,0,['self'],[None],[None],43,[]
parser/test3/io/parquet.py:ParquetDatasetWriter:__init__,ParquetDatasetWriter:__init__,method,10,27,27,284,10.52,0,1,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', '**parquet_writer_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', None, None]","[None, None, None, None, ' None', None, None]",17,[]
parser/test3/io/parquet.py:ParquetDatasetWriter:write,ParquetDatasetWriter:write,method,9,27,21,380,14.07,0,2,['self'],[None],[None],83,[]
parser/test3/io/parquet.py:ParquetDatasetWriter:_write,ParquetDatasetWriter:_write,method,17,38,35,441,11.61,1,1,"['self', 'file_obj', 'batch_size', '**parquet_writer_kwargs']","[None, ' BinaryIO', ' int', None]","[None, None, None, None]",93,"['        """"""Writes the pyarrow table as Parquet to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']"
parser/test3/io/text.py:TextDatasetReader,TextDatasetReader,class,21,69,57,953,13.81,0,1,[],[],[],9,[]
parser/test3/io/text.py:TextDatasetReader:__init__,TextDatasetReader:__init__,method,10,23,22,309,13.43,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' False', None, None]",10,[]
parser/test3/io/text.py:TextDatasetReader:read,TextDatasetReader:read,method,9,25,20,433,17.32,0,0,['self'],[None],[None],30,[]
parser/test3/packaged_modules/__init__.py:hash_python_lines,hash_python_lines,function,10,22,21,220,10.0,1,1,['lines'],[' List[str]'],[None],13,[]
parser/test3/tasks/automatic_speech_recognition.py:AutomaticSpeechRecognition,AutomaticSpeechRecognition,class,10,28,24,425,15.18,0,0,[],[],[],9,[]
parser/test3/tasks/automatic_speech_recognition.py:AutomaticSpeechRecognition:column_mapping,AutomaticSpeechRecognition:column_mapping,method,1,5,5,95,19.0,0,0,['self'],[None],[None],19,[]
parser/test3/tasks/base.py:TaskTemplate,TaskTemplate,class,13,49,38,441,9.0,0,0,[],[],[],13,[]
parser/test3/tasks/base.py:TaskTemplate:features,TaskTemplate:features,method,2,3,3,55,18.33,0,0,['self'],[None],[None],20,[]
parser/test3/tasks/base.py:TaskTemplate:column_mapping,TaskTemplate:column_mapping,method,1,2,2,24,12.0,0,0,['self'],[None],[None],25,[]
parser/test3/tasks/base.py:TaskTemplate:from_dict,TaskTemplate:from_dict,method,3,18,14,116,6.44,0,0,"['cls', 'template_dict']","[' Type[T]', ' dict']","[None, None]",29,[]
parser/test3/tasks/question_answering.py:QuestionAnsweringExtractive,QuestionAnsweringExtractive,class,12,46,39,505,10.98,0,0,[],[],[],9,[]
parser/test3/tasks/question_answering.py:QuestionAnsweringExtractive:column_mapping,QuestionAnsweringExtractive:column_mapping,method,1,7,7,99,14.14,0,0,['self'],[None],[None],28,[]
parser/test3/tasks/summarization.py:Summarization,Summarization,class,10,28,24,325,11.61,0,0,[],[],[],9,[]
parser/test3/tasks/summarization.py:Summarization:column_mapping,Summarization:column_mapping,method,1,5,5,61,12.2,0,0,['self'],[None],[None],18,[]
parser/test3/tasks/text_classification.py:TextClassification,TextClassification,class,22,50,46,626,12.52,0,1,[],[],[],9,[]
parser/test3/tasks/text_classification.py:TextClassification:__post_init__,TextClassification:__post_init__,method,7,15,15,245,16.33,0,1,['self'],[None],[None],19,[]
parser/test3/tasks/text_classification.py:TextClassification:column_mapping,TextClassification:column_mapping,method,3,7,7,62,8.86,0,0,['self'],[None],[None],28,[]
parser/test3/tasks/__init__.py:task_template_from_dict,task_template_from_dict,function,7,21,18,254,12.1,0,1,['task_template_dict'],[' dict'],[None],30,"['    """"""Create one of the supported task templates in :py:mod:`datasets.tasks` from a dictionary.""""""\n']"
parser/test3/utils/beam_utils.py:upload_local_to_remote,upload_local_to_remote,function,12,42,30,467,11.12,1,2,"['local_file_path', 'remote_file_path', 'force_upload']","[None, None, None]","[None, None, 'False']",27,"['    """"""Use the Beam Filesystems to upload to a remote directory on gcs/s3/hdfs...""""""\n']"
parser/test3/utils/beam_utils.py:download_remote_to_local,download_remote_to_local,function,12,42,30,470,11.19,1,2,"['remote_file_path', 'local_file_path', 'force_download']","[None, None, None]","[None, None, 'False']",44,"['    """"""Use the Beam Filesystems to download from a remote directory on gcs/s3/hdfs...""""""\n']"
parser/test3/utils/beam_utils.py:_create_parquet_sink,_create_parquet_sink,function,2,13,13,191,14.69,0,0,"['file_path_prefix', 'schema', 'codec', 'row_group_buffer_size', 'record_batch_size', 'use_deprecated_int96_timestamps', 'file_name_suffix', 'num_shards', 'shard_name_template', 'mime_type', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None]",181,[]
parser/test3/utils/beam_utils.py:BeamPipeline,BeamPipeline,class,4,10,9,124,12.4,0,0,[],[],[],19,[]
parser/test3/utils/beam_utils.py:WriteToParquet,WriteToParquet,class,9,43,38,604,14.05,0,0,[],[],[],61,[]
parser/test3/utils/beam_utils.py:_ParquetSink,_ParquetSink,class,67,168,116,2259,13.45,6,6,[],[],[],207,[]
parser/test3/utils/beam_utils.py:BeamPipeline:is_local,BeamPipeline:is_local,method,3,8,7,105,13.12,0,0,['self'],[None],[None],22,[]
parser/test3/utils/beam_utils.py:WriteToParquet:__init__,WriteToParquet:__init__,method,3,15,15,242,16.13,0,0,"['self', 'file_path_prefix', 'schema', 'row_group_buffer_size', 'record_batch_size', 'codec', 'use_deprecated_int96_timestamps', 'file_name_suffix', 'num_shards', 'shard_name_template', 'mime_type', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, '64 * 1024 * 1024', '1000', '""none""', 'False', '""""', '0', 'None', '""application/x-parquet""', None]",72,"['        """"""Initialize a WriteToParquet transform.\n', '        (from apache_beam.io.parquetio, only ._flush_buffer() is different)\n', '\n', '        Writes parquet files from a :class:`~apache_beam.pvalue.PCollection` of\n', '        records. Each record is a dictionary with keys of a string type that\n', '        represent column names. Schema must be specified like the example below.\n', '\n', '        .. testsetup::\n', '\n', '            from tempfile import NamedTemporaryFile\n', '            import glob\n', '            import os\n', '            import pyarrow\n', '\n', '            filename = NamedTemporaryFile(delete=False).name\n', '\n', '        .. testcode::\n', '\n', '            with beam.Pipeline() as p:\n', ""                records = p | 'Read' >> beam.Create(\n"", ""                        [{'name': 'foo', 'age': 10}, {'name': 'bar', 'age': 20}]\n"", '                )\n', ""                _ = records | 'Write' >> beam.io.WriteToParquet(filename,\n"", '                        pyarrow.schema(\n', ""                                [('name', pyarrow.binary()), ('age', pyarrow.int64())]\n"", '                        )\n', '                )\n', '\n', '        .. testcleanup::\n', '\n', ""            for output in glob.glob('{}*'.format(filename)):\n"", '                os.remove(output)\n', '\n', '        For more information on supported types and schema, please see the pyarrow\n', '        document.\n', '\n', '        Args:\n', '            file_path_prefix: The file path to write to. The files written will begin\n', '                with this prefix, followed by a shard identifier (see num_shards), and\n', '                end in a common extension, if given by file_name_suffix. In most cases,\n', '                only this argument is specified and num_shards, shard_name_template, and\n', '                file_name_suffix use default values.\n', '            schema: The schema to use, as type of ``pyarrow.Schema``.\n', '            row_group_buffer_size: The byte size of the row group buffer. Note that\n', '                this size is for uncompressed data on the memory and normally much\n', '                bigger than the actual row group size written to a file.\n', '            record_batch_size: The number of records in each record batch. Record\n', '                batch is a basic unit used for storing data in the row group buffer.\n', '                A higher record batch size implies low granularity on a row group buffer\n', '                size. For configuring a row group size based on the number of records,\n', '                set ``row_group_buffer_size`` to 1 and use ``record_batch_size`` to\n', '                adjust the value.\n', '            codec: The codec to use for block-level compression. Any string supported\n', '                by the pyarrow specification is accepted.\n', '            use_deprecated_int96_timestamps: Write nanosecond resolution timestamps to\n', '                INT96 Parquet format. Defaults to False.\n', '            file_name_suffix: Suffix for the files written.\n', '            num_shards: The number of files (shards) used for output. If not set, the\n', '                service will decide on the optimal number of shards.\n', '                Constraining the number of shards is likely to reduce\n', '                the performance of a pipeline.  Setting this value is not recommended\n', '                unless you require a specific number of output files.\n', '            shard_name_template: A template string containing placeholders for\n', '                the shard number and shard count. When constructing a filename for a\n', ""                particular shard number, the upper-case letters 'S' and 'N' are\n"", '                replaced with the 0-padded shard number and shard count respectively.\n', ""                This argument can be '' in which case it behaves as if num_shards was\n"", '                set to 1 and only one file will be generated. The default pattern used\n', ""                is '-SSSSS-of-NNNNN' if None is passed as the shard_name_template.\n"", '            mime_type: The MIME type to use for the produced files, if the filesystem\n', '                supports specifying MIME types.\n', '\n', '        Returns:\n', '            A WriteToParquet transform usable for writing.\n', '        """"""\n']"
parser/test3/utils/beam_utils.py:WriteToParquet:expand,WriteToParquet:expand,method,3,4,4,29,7.25,0,0,"['self', 'pcoll']","[None, None]","[None, None]",174,[]
parser/test3/utils/beam_utils.py:WriteToParquet:display_data,WriteToParquet:display_data,method,1,3,3,28,9.33,0,0,['self'],[None],[None],177,[]
parser/test3/utils/beam_utils.py:_ParquetSink:__init__,_ParquetSink:__init__,method,24,32,32,556,17.38,1,0,"['self', 'file_path_prefix', 'schema', 'codec', 'row_group_buffer_size', 'record_batch_size', 'use_deprecated_int96_timestamps', 'file_name_suffix', 'num_shards', 'shard_name_template', 'mime_type', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",72,"['        """"""Initialize a WriteToParquet transform.\n', '        (from apache_beam.io.parquetio, only ._flush_buffer() is different)\n', '\n', '        Writes parquet files from a :class:`~apache_beam.pvalue.PCollection` of\n', '        records. Each record is a dictionary with keys of a string type that\n', '        represent column names. Schema must be specified like the example below.\n', '\n', '        .. testsetup::\n', '\n', '            from tempfile import NamedTemporaryFile\n', '            import glob\n', '            import os\n', '            import pyarrow\n', '\n', '            filename = NamedTemporaryFile(delete=False).name\n', '\n', '        .. testcode::\n', '\n', '            with beam.Pipeline() as p:\n', ""                records = p | 'Read' >> beam.Create(\n"", ""                        [{'name': 'foo', 'age': 10}, {'name': 'bar', 'age': 20}]\n"", '                )\n', ""                _ = records | 'Write' >> beam.io.WriteToParquet(filename,\n"", '                        pyarrow.schema(\n', ""                                [('name', pyarrow.binary()), ('age', pyarrow.int64())]\n"", '                        )\n', '                )\n', '\n', '        .. testcleanup::\n', '\n', ""            for output in glob.glob('{}*'.format(filename)):\n"", '                os.remove(output)\n', '\n', '        For more information on supported types and schema, please see the pyarrow\n', '        document.\n', '\n', '        Args:\n', '            file_path_prefix: The file path to write to. The files written will begin\n', '                with this prefix, followed by a shard identifier (see num_shards), and\n', '                end in a common extension, if given by file_name_suffix. In most cases,\n', '                only this argument is specified and num_shards, shard_name_template, and\n', '                file_name_suffix use default values.\n', '            schema: The schema to use, as type of ``pyarrow.Schema``.\n', '            row_group_buffer_size: The byte size of the row group buffer. Note that\n', '                this size is for uncompressed data on the memory and normally much\n', '                bigger than the actual row group size written to a file.\n', '            record_batch_size: The number of records in each record batch. Record\n', '                batch is a basic unit used for storing data in the row group buffer.\n', '                A higher record batch size implies low granularity on a row group buffer\n', '                size. For configuring a row group size based on the number of records,\n', '                set ``row_group_buffer_size`` to 1 and use ``record_batch_size`` to\n', '                adjust the value.\n', '            codec: The codec to use for block-level compression. Any string supported\n', '                by the pyarrow specification is accepted.\n', '            use_deprecated_int96_timestamps: Write nanosecond resolution timestamps to\n', '                INT96 Parquet format. Defaults to False.\n', '            file_name_suffix: Suffix for the files written.\n', '            num_shards: The number of files (shards) used for output. If not set, the\n', '                service will decide on the optimal number of shards.\n', '                Constraining the number of shards is likely to reduce\n', '                the performance of a pipeline.  Setting this value is not recommended\n', '                unless you require a specific number of output files.\n', '            shard_name_template: A template string containing placeholders for\n', '                the shard number and shard count. When constructing a filename for a\n', ""                particular shard number, the upper-case letters 'S' and 'N' are\n"", '                replaced with the 0-padded shard number and shard count respectively.\n', ""                This argument can be '' in which case it behaves as if num_shards was\n"", '                set to 1 and only one file will be generated. The default pattern used\n', ""                is '-SSSSS-of-NNNNN' if None is passed as the shard_name_template.\n"", '            mime_type: The MIME type to use for the produced files, if the filesystem\n', '                supports specifying MIME types.\n', '\n', '        Returns:\n', '            A WriteToParquet transform usable for writing.\n', '        """"""\n']"
parser/test3/utils/beam_utils.py:_ParquetSink:open,_ParquetSink:open,method,4,10,10,213,21.3,0,0,"['self', 'temp_path']","[None, None]","[None, None]",244,[]
parser/test3/utils/beam_utils.py:_ParquetSink:write_record,_ParquetSink:write_record,method,8,14,13,226,16.14,1,2,"['self', 'writer', 'value']","[None, None, None]","[None, None, None]",253,[]
parser/test3/utils/beam_utils.py:_ParquetSink:close,_ParquetSink:close,method,6,16,12,195,12.19,0,3,"['self', 'writer']","[None, None]","[None, None]",264,[]
parser/test3/utils/beam_utils.py:_ParquetSink:display_data,_ParquetSink:display_data,method,7,11,10,177,16.09,0,0,['self'],[None],[None],177,"['    """"""A sink for parquet files.""""""\n']"
parser/test3/utils/beam_utils.py:_ParquetSink:_write_batches,_ParquetSink:_write_batches,method,5,7,7,132,18.86,0,0,"['self', 'writer']","[None, None]","[None, None]",282,[]
parser/test3/utils/beam_utils.py:_ParquetSink:_flush_buffer,_ParquetSink:_flush_buffer,method,22,47,35,399,8.49,4,1,['self'],[None],[None],288,[]
parser/test3/utils/deprecation_utils.py:deprecated,deprecated,function,12,55,48,594,10.8,0,2,['help_message'],[' Optional[str] '],[' None'],12,"['    """"""Decorator to mark a function as deprecated.\n', '\n', '    Args:\n', '        help_message (`Optional[str]`): An optional message to guide the user on how to\n', '            switch to non-deprecated usage of the library.\n', '    """"""\n']"
parser/test3/utils/doc_utils.py:is_documented_by,is_documented_by,function,5,8,7,120,15.0,0,0,['function_with_docstring'],[' Callable'],[None],4,"['    """"""Decorator to share docstrings across common functions.\n', '\n', '    Args:\n', '        function_with_docstring (`Callable`): Name of the function with the docstring.\n', '    """"""\n']"
parser/test3/utils/download_manager.py:GenerateMode,GenerateMode,class,3,6,6,131,21.83,0,0,[],[],[],42,[]
parser/test3/utils/download_manager.py:DownloadManager,DownloadManager,class,102,338,223,5060,14.97,4,5,[],[],[],66,[]
parser/test3/utils/download_manager.py:DownloadManager:__init__,DownloadManager:__init__,method,17,22,19,277,12.59,0,0,"['self', 'dataset_name', 'data_dir', 'download_config', 'base_path', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[DownloadConfig] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' None', None]",67,"['        """"""Download manager constructor.\n', '\n', '        Args:\n', '            data_dir: can be used to specify a manual directory to get the files from.\n', '            dataset_name: `str`, name of dataset this instance will be used for. If\n', '                provided, downloads will contain which datasets they were used for.\n', '            download_config: `DownloadConfig` to specify the cache directory and other\n', '                download options\n', '            base_path: `str`, base path that is used when relative paths are used to\n', '                download files. This can be a remote url.\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:manual_dir,DownloadManager:manual_dir,method,2,2,2,20,10.0,0,0,['self'],[None],[None],95,[]
parser/test3/utils/download_manager.py:DownloadManager:downloaded_size,DownloadManager:downloaded_size,method,2,6,6,96,16.0,0,0,['self'],[None],[None],99,"['        """"""Returns the total size of downloaded files.""""""\n']"
parser/test3/utils/download_manager.py:DownloadManager:ship_files_with_pipeline,DownloadManager:ship_files_with_pipeline,method,17,53,44,706,13.32,0,1,"['self', 'downloaded_path_or_paths', 'pipeline']","[None, None, None]","[None, None, None]",103,"['        """"""\n', '        Ship the files using Beam FileSystems to the pipeline temp dir.\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:_record_sizes_checksums,DownloadManager:_record_sizes_checksums,method,4,8,8,145,18.12,1,0,"['self', 'url_or_urls', 'downloaded_path_or_paths']","[None, ' NestedDataStructure', ' NestedDataStructure']","[None, None, None]",131,"['        """"""Record size/checksum of downloaded files.""""""\n']"
parser/test3/utils/download_manager.py:DownloadManager:download_custom,DownloadManager:download_custom,method,23,56,43,913,16.3,1,1,"['self', 'url_or_urls', 'custom_download']","[None, None, None]","[None, None, None]",137,"['        """"""\n', '        Download given urls(s) by calling `custom_download`.\n', '\n', '        Args:\n', '            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n', '                url is a `str`.\n', '            custom_download: Callable with signature (src_url: str, dst_path: str) -> Any\n', '                as for example `tf.io.gfile.copy`, that lets you download from google storage\n', '\n', '        Returns:\n', '            downloaded_path(s): `str`, The downloaded paths matching the given input\n', '                url_or_urls.\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:download,DownloadManager:download,method,2,6,6,96,16.0,0,0,['self'],[None],[None],176,"['        """"""Download given url(s).\n', '\n', '        Args:\n', '            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n', '                url is a `str`.\n', '\n', '        Returns:\n', '            downloaded_path(s): `str`, The downloaded paths matching the given input\n', '                url_or_urls.\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:_download,DownloadManager:_download,method,5,8,8,169,21.12,0,1,"['self', 'url_or_filename', 'download_config']","[None, ' str', ' DownloadConfig']","[None, None, None]",216,[]
parser/test3/utils/download_manager.py:DownloadManager:iter_archive,DownloadManager:iter_archive,method,16,46,38,514,11.17,2,1,"['self', 'path']","[None, None]","[None, None]",222,"['        """"""Returns iterator over files within archive.\n', '\n', '        Args:\n', '            path: path to archive.\n', '\n', '        Returns:\n', '            Generator yielding tuple (path_within_archive, file_obj).\n', '            File-Obj are opened in byte mode (io.BufferedReader)\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:extract,DownloadManager:extract,method,13,21,20,462,22.0,0,0,"['self', 'path_or_paths', 'num_proc']","[None, None, None]","[None, None, 'None']",247,"['        """"""Extract given path(s).\n', '\n', '        Args:\n', '            path_or_paths: path or `list`/`dict` of path of file to extract. Each\n', '                path is a `str`.\n', '            num_proc: Use multi-processing if `num_proc` > 1 and the length of\n', '                `path_or_paths` is larger than `num_proc`\n', '\n', '        Returns:\n', '            extracted_path(s): `str`, The extracted paths matching the given input\n', '                path_or_paths.\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:download_and_extract,DownloadManager:download_and_extract,method,2,2,2,46,23.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",273,"['        """"""Download and extract given url_or_urls.\n', '\n', '        Is roughly equivalent to:\n', '\n', '        ```\n', '        extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))\n', '        ```\n', '\n', '        Args:\n', '            url_or_urls: url or `list`/`dict` of urls to download and extract. Each\n', '                url is a `str`.\n', '\n', '        Returns:\n', '            extracted_path(s): `str`, extracted paths of given URL(s).\n', '        """"""\n']"
parser/test3/utils/download_manager.py:DownloadManager:get_recorded_sizes_checksums,DownloadManager:get_recorded_sizes_checksums,method,2,2,2,43,21.5,0,0,['self'],[None],[None],291,[]
parser/test3/utils/filelock.py:logger,logger,function,4,8,6,72,9.0,0,0,[],[],[],80,"['    """"""Returns the logger instance used in this module.""""""\n']"
parser/test3/utils/filelock.py:Timeout,Timeout,class,11,20,17,158,7.9,0,0,[],[],[],89,[]
parser/test3/utils/filelock.py:_Acquire_ReturnProxy,_Acquire_ReturnProxy,class,7,19,15,164,8.63,0,0,[],[],[],117,[]
parser/test3/utils/filelock.py:BaseFileLock,BaseFileLock,class,54,245,139,2322,9.48,1,6,[],[],[],130,[]
parser/test3/utils/filelock.py:WindowsFileLock,WindowsFileLock,class,15,49,32,457,9.33,0,0,[],[],[],349,[]
parser/test3/utils/filelock.py:UnixFileLock,UnixFileLock,class,12,36,27,343,9.53,0,0,[],[],[],392,[]
parser/test3/utils/filelock.py:SoftFileLock,SoftFileLock,class,14,35,26,319,9.11,0,0,[],[],[],425,[]
parser/test3/utils/filelock.py:Timeout:__init__,Timeout:__init__,method,3,4,4,35,8.75,0,0,"['self', 'lock_file']","[None, None]","[None, None]",95,"['        """""" """"""\n']"
parser/test3/utils/filelock.py:Timeout:__str__,Timeout:__str__,method,7,11,10,75,6.82,0,0,['self'],[None],[None],101,[]
parser/test3/utils/filelock.py:_Acquire_ReturnProxy:__init__,_Acquire_ReturnProxy:__init__,method,3,4,4,25,6.25,0,0,"['self', 'lock']","[None, None]","[None, None]",118,[]
parser/test3/utils/filelock.py:_Acquire_ReturnProxy:__enter__,_Acquire_ReturnProxy:__enter__,method,2,2,2,15,7.5,0,0,['self'],[None],[None],122,[]
parser/test3/utils/filelock.py:_Acquire_ReturnProxy:__exit__,_Acquire_ReturnProxy:__exit__,method,2,3,3,30,10.0,0,0,"['self', 'exc_type', 'exc_value', 'traceback']","[None, None, None, None]","[None, None, None, None]",125,[]
parser/test3/utils/filelock.py:BaseFileLock:__init__,BaseFileLock:__init__,method,10,15,15,209,13.93,0,0,"['self', 'lock_file', 'timeout', 'max_filename_length']","[None, None, None, None]","[None, None, '-1', '255']",135,"['        """""" """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:lock_file,BaseFileLock:lock_file,method,2,2,2,21,10.5,0,0,['self'],[None],[None],161,"['        """"""\n', '        The path to the lock file.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:timeout,BaseFileLock:timeout,method,2,2,2,19,9.5,0,0,['self'],[None],[None],168,"['        """"""\n', '        You can set a default timeout for the filelock. It will be used as\n', '        fallback value in the acquire method, if no timeout value (*None*) is\n', '        given.\n', '\n', '        If you want to disable the timeout, set it to a negative value.\n', '\n', '        A timeout of 0 means, that there is exactly one attempt to acquire the\n', '        file lock.\n', '\n', '        .. versionadded:: 2.0.0\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:timeout,BaseFileLock:timeout,method,2,2,2,19,9.5,0,0,['self'],[None],[None],184,"['        """""" """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:_acquire,BaseFileLock:_acquire,method,1,2,2,26,13.0,0,0,['self'],[None],[None],192,"['        """"""\n', '        Platform dependent. If the file lock could be\n', '        acquired, self._lock_file_fd holds the file descriptor\n', '        of the lock file.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:_release,BaseFileLock:_release,method,1,2,2,26,13.0,0,0,['self'],[None],[None],200,"['        """"""\n', '        Releases the lock and sets self._lock_file_fd to None.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:is_locked,BaseFileLock:is_locked,method,2,5,5,33,6.6,0,0,['self'],[None],[None],210,"['        """"""\n', '        True, if the object holds the file lock.\n', '\n', '        .. versionchanged:: 2.0.0\n', '\n', '            This was previously a method and is now a property.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:acquire,BaseFileLock:acquire,method,22,93,63,788,8.47,1,3,"['self', 'timeout', 'poll_intervall']","[None, None, None]","[None, 'None', '0.05']",220,"['        """"""\n', '        Acquires the file lock or fails with a :exc:`Timeout` error.\n', '\n', '        .. code-block:: python\n', '\n', '            # You can use this method in the context manager (recommended)\n', '            with lock.acquire():\n', '                pass\n', '\n', '            # Or use an equivalent try-finally construct:\n', '            lock.acquire()\n', '            try:\n', '                pass\n', '            finally:\n', '                lock.release()\n', '\n', '        :arg float timeout:\n', '            The maximum time waited for the file lock.\n', '            If ``timeout < 0``, there is no timeout and this method will\n', '            block until the lock could be acquired.\n', '            If ``timeout`` is None, the default :attr:`~timeout` is used.\n', '\n', '        :arg float poll_intervall:\n', '            We check once in *poll_intervall* seconds if we can acquire the\n', '            file lock.\n', '\n', '        :raises Timeout:\n', '            if the lock could not be acquired in *timeout* seconds.\n', '\n', '        .. versionchanged:: 2.0.0\n', '\n', '            This method returns now a *proxy* object instead of *self*,\n', '            so that it can be used in a with statement without side effects.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:release,BaseFileLock:release,method,13,37,29,318,8.59,0,2,"['self', 'force']","[None, None]","[None, 'False']",293,"['        """"""\n', '        Releases the file lock.\n', '\n', '        Please note, that the lock is only completly released, if the lock\n', '        counter is 0.\n', '\n', '        Also note, that the lock file itself is not automatically deleted.\n', '\n', '        :arg bool force:\n', '            If true, the lock counter is ignored and the lock is released in\n', '            every case.\n', '        """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:__enter__,BaseFileLock:__enter__,method,2,3,3,25,8.33,0,0,['self'],[None],[None],122,"['    """"""\n', '    Implements the base class of a file lock.\n', '    """"""\n']"
parser/test3/utils/filelock.py:BaseFileLock:__exit__,BaseFileLock:__exit__,method,2,3,3,25,8.33,0,0,"['self', 'exc_type', 'exc_value', 'traceback']","[None, None, None, None]","[None, None, None, None]",125,[]
parser/test3/utils/filelock.py:BaseFileLock:__del__,BaseFileLock:__del__,method,2,3,3,35,11.67,0,0,['self'],[None],[None],330,[]
parser/test3/utils/filelock.py:BaseFileLock:hash_filename_if_too_long,BaseFileLock:hash_filename_if_too_long,method,12,28,23,286,10.21,0,1,"['self', 'path', 'max_length']","[None, ' str', ' int']","[None, None, None]",334,[]
parser/test3/utils/filelock.py:WindowsFileLock:_acquire,WindowsFileLock:_acquire,method,12,30,25,260,8.67,0,0,['self'],[None],[None],192,"['        """"""\n', '        Platform dependent. If the file lock could be\n', '        acquired, self._lock_file_fd holds the file descriptor\n', '        of the lock file.\n', '        """"""\n']"
parser/test3/utils/filelock.py:WindowsFileLock:_release,WindowsFileLock:_release,method,7,15,15,158,10.53,0,0,['self'],[None],[None],200,"['        """"""\n', '        Releases the lock and sets self._lock_file_fd to None.\n', '        """"""\n']"
parser/test3/utils/filelock.py:UnixFileLock:_acquire,UnixFileLock:_acquire,method,10,23,21,205,8.91,0,0,['self'],[None],[None],192,"['        """"""\n', '        Platform dependent. If the file lock could be\n', '        acquired, self._lock_file_fd holds the file descriptor\n', '        of the lock file.\n', '        """"""\n']"
parser/test3/utils/filelock.py:UnixFileLock:_release,UnixFileLock:_release,method,5,9,9,99,11.0,0,0,['self'],[None],[None],200,"['        """"""\n', '        Releases the lock and sets self._lock_file_fd to None.\n', '        """"""\n']"
parser/test3/utils/filelock.py:SoftFileLock:_acquire,SoftFileLock:_acquire,method,9,21,19,165,7.86,0,0,['self'],[None],[None],192,"['        """"""\n', '        Platform dependent. If the file lock could be\n', '        acquired, self._lock_file_fd holds the file descriptor\n', '        of the lock file.\n', '        """"""\n']"
parser/test3/utils/filelock.py:SoftFileLock:_release,SoftFileLock:_release,method,5,10,10,115,11.5,0,0,['self'],[None],[None],200,"['        """"""\n', '        Releases the lock and sets self._lock_file_fd to None.\n', '        """"""\n']"
parser/test3/utils/file_utils.py:init_hf_modules,init_hf_modules,function,10,30,21,392,13.07,0,3,"['hf_modules_cache', 'str]] ']","[' Optional[Union[Path', None]","[None, ' None']",44,"['    """"""\n', '    Add hf_modules_cache to the python path.\n', ""    By default hf_modules_cache='~/.cache/huggingface/modules'.\n"", '    It can also be set with the environment variable HF_MODULES_CACHE.\n', '    This is used to add modules such as `datasets_modules`\n', '    """"""\n']"
parser/test3/utils/file_utils.py:temp_seed,temp_seed,function,44,93,72,1219,13.11,0,9,"['seed', 'set_pytorch', 'set_tensorflow']","[' int', None, None]","[None, 'False', 'False']",64,"['    """"""Temporarily set the random seed. This works for python numpy, pytorch and tensorflow.""""""\n']"
parser/test3/utils/file_utils.py:is_remote_url,is_remote_url,function,4,11,11,93,8.45,0,0,['url_or_filename'],[' str'],[None],117,[]
parser/test3/utils/file_utils.py:is_local_path,is_local_path,function,2,6,6,98,16.33,0,0,['url_or_filename'],[' str'],[None],122,[]
parser/test3/utils/file_utils.py:is_relative_path,is_relative_path,function,2,6,6,78,13.0,0,0,['url_or_filename'],[' str'],[None],129,[]
parser/test3/utils/file_utils.py:hf_bucket_url,hf_bucket_url,function,8,19,14,253,13.32,0,3,"['identifier', 'filename', 'use_cdn', 'dataset']","[' str', ' str', None, None]","[None, None, 'False', 'True']",133,[]
parser/test3/utils/file_utils.py:head_hf_s3,head_hf_s3,function,3,15,14,164,10.93,0,0,"['identifier', 'filename', 'use_cdn', 'dataset', 'max_retries']","[' str', ' str', None, None, None]","[None, None, 'False', 'True', '0']",141,[]
parser/test3/utils/file_utils.py:hf_github_url,hf_github_url,function,10,20,17,258,12.9,0,1,"['path', 'name', 'dataset', 'version']","[' str', ' str', None, ' Optional[str] ']","[None, None, 'True', ' None']",153,[]
parser/test3/utils/file_utils.py:hf_hub_url,hf_hub_url,function,5,8,7,117,14.62,0,0,"['path', 'name', 'version']","[' str', ' str', ' Optional[str] ']","[None, None, ' None']",163,[]
parser/test3/utils/file_utils.py:url_or_path_join,url_or_path_join,function,4,9,8,120,13.33,0,1,"['base_name', '*pathnames']","[' str', ' str']","[None, None]",168,[]
parser/test3/utils/file_utils.py:url_or_path_parent,url_or_path_parent,function,5,8,7,114,14.25,0,1,['url_or_path'],[' str'],[None],175,[]
parser/test3/utils/file_utils.py:hash_url_to_filename,hash_url_to_filename,function,13,23,18,245,10.65,0,2,"['url', 'etag']","[None, None]","[None, 'None']",182,"['    """"""\n', '    Convert `url` into a hashed filename in a repeatable way.\n', ""    If `etag` is specified, append its hash to the url's, delimited\n"", '    by a period.\n', ""    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n"", '    so that TF 2.0 can identify it as a HDF5 file\n', '    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n', '    """"""\n']"
parser/test3/utils/file_utils.py:cached_path,cached_path,function,57,224,144,3137,14.0,0,10,"['url_or_filename', 'download_config', '**download_kwargs', '']","[None, None, None, None]","[None, 'None', None, None]",248,"['    """"""\n', '    Given something that might be a URL (or might be a local path),\n', ""    determine which. If it's a URL, download the file and cache it, and\n"", ""    return the path to the cached file. If it's already a local path,\n"", '    make sure the file exists and then return the path.\n', '\n', '    Return:\n', '        Local path (string)\n', '\n', '    Raises:\n', '        FileNotFoundError: in case of non-recoverable file\n', '            (non-existent or no cache on disk)\n', '        ConnectionError: in case of unreachable url\n', '            and no cache on disk\n', ""        ValueError: if it couldn't parse the url or filename correctly\n"", '        requests.exceptions.ConnectionError: in case of internet connection issue\n', '    """"""\n']"
parser/test3/utils/file_utils.py:get_datasets_user_agent,get_datasets_user_agent,function,13,57,30,549,9.63,0,5,"['user_agent', 'dict]] ']","[' Optional[Union[str', None]","[None, ' None']",376,[]
parser/test3/utils/file_utils.py:get_authentication_headers_for_url,get_authentication_headers_for_url,function,13,26,21,286,11.0,0,3,"['url', 'use_auth_token', 'bool]] ']","[' str', ' Optional[Union[str', None]","[None, None, ' None']",394,"['    """"""Handle the HF authentication""""""\n']"
parser/test3/utils/file_utils.py:_raise_if_offline_mode_is_enabled,_raise_if_offline_mode_is_enabled,function,2,20,15,129,6.45,0,1,['msg'],[' Optional[str] '],[' None'],414,"['    """"""Raise a OfflineModeIsEnabled error (subclass of ConnectionError) if HF_DATASETS_OFFLINE is True.""""""\n']"
parser/test3/utils/file_utils.py:_request_with_retry,_request_with_retry,function,14,54,50,481,8.91,1,1,"['method', 'url', 'max_retries', 'base_wait_time', 'max_wait_time', 'timeout', '**params', '']","[' str', ' str', ' int ', ' float ', ' float ', ' float ', None, None]","[None, None, ' 0', ' 0.5', ' 2', ' 10.0', None, None]",422,"['    """"""Wrapper around requests to retry in case it fails with a ConnectTimeout, with exponential backoff.\n', '\n', '    Note that if the environment variable HF_DATASETS_OFFLINE is set to 1, then a OfflineModeIsEnabled error is raised.\n', '\n', '    Args:\n', ""        method (str): HTTP method, such as 'GET' or 'HEAD'\n"", '        url (str): The URL of the ressource to fetch\n', '        max_retries (int): Maximum number of retries, defaults to 0 (no retries)\n', '        base_wait_time (float): Duration (in seconds) to wait before retrying the first time. Wait time between\n', '            retries then grows exponentially, capped by max_wait_time.\n', '        max_wait_time (float): Maximum amount of time between two retries, in seconds\n', '        **params: Params to pass to `requests.request`\n', '    """"""\n']"
parser/test3/utils/file_utils.py:ftp_head,ftp_head,function,5,17,16,171,10.06,0,0,"['url', 'timeout']","[None, None]","[None, '10.0']",461,[]
parser/test3/utils/file_utils.py:ftp_get,ftp_get,function,7,24,23,268,11.17,0,0,"['url', 'temp_file', 'timeout']","[None, None, None]","[None, None, '10.0']",471,[]
parser/test3/utils/file_utils.py:http_get,http_get,function,27,70,62,830,11.86,1,3,"['url', 'temp_file', 'proxies', 'resume_size', 'headers', 'cookies', 'timeout', 'max_retries']","[None, None, None, None, None, None, None, None]","[None, None, 'None', '0', 'None', 'None', '10.0', '0']",481,[]
parser/test3/utils/file_utils.py:http_head,http_head,function,7,19,18,315,16.58,0,0,"['url', 'proxies', 'headers', 'cookies', 'allow_redirects', 'timeout', 'max_retries']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'True', '10.0', '0']",515,[]
parser/test3/utils/file_utils.py:get_from_cache,get_from_cache,function,62,316,191,3073,9.72,1,15,"['url', 'cache_dir', 'force_download', 'proxies', 'etag_timeout', 'resume_download', 'user_agent', 'local_files_only', 'use_etag', 'max_retries', 'use_auth_token', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'False', 'None', '10', 'False', 'None', 'False', 'True', '0', 'None', None]",533,"['    """"""\n', '    Given a URL, look for the corresponding file in the local cache.\n', ""    If it's not there, download it. Then return the path to the cached file.\n"", '\n', '    Return:\n', '        Local path (string)\n', '\n', '    Raises:\n', '        FileNotFoundError: in case of non-recoverable file\n', '            (non-existent or no cache on disk)\n', '        ConnectionError: in case of unreachable url\n', '            and no cache on disk\n', '    """"""\n']"
parser/test3/utils/file_utils.py:is_gzip,is_gzip,function,5,13,12,82,6.31,0,0,['path'],[' str'],[None],696,"['    """"""from https://stackoverflow.com/a/60634210""""""\n']"
parser/test3/utils/file_utils.py:is_xz,is_xz,function,6,20,17,152,7.6,0,1,['path'],[' str'],[None],706,"['    """"""https://tukaani.org/xz/xz-file-format-1.0.4.txt""""""\n']"
parser/test3/utils/file_utils.py:is_rarfile,is_rarfile,function,8,21,20,194,9.24,0,1,['path'],[' str'],[None],719,"['    """"""https://github.com/markokr/rarfile/blob/master/rarfile.py""""""\n']"
parser/test3/utils/file_utils.py:add_start_docstrings,add_start_docstrings,function,4,16,14,129,8.06,0,0,['*docstr'],[None],[None],757,[]
parser/test3/utils/file_utils.py:add_end_docstrings,add_end_docstrings,function,4,16,14,129,8.06,0,0,['*docstr'],[None],[None],765,[]
parser/test3/utils/file_utils.py:estimate_dataset_size,estimate_dataset_size,function,2,6,6,44,7.33,0,0,['paths'],[None],[None],773,[]
parser/test3/utils/file_utils.py:DownloadConfig,DownloadConfig,class,22,50,36,457,9.14,0,0,[],[],[],207,[]
parser/test3/utils/file_utils.py:OfflineModeIsEnabled,OfflineModeIsEnabled,class,0,1,1,4,4.0,0,0,[],[],[],410,[]
parser/test3/utils/file_utils.py:ZstdExtractor,ZstdExtractor,class,17,57,46,470,8.25,0,2,[],[],[],732,[]
parser/test3/utils/file_utils.py:DownloadConfig:copy,DownloadConfig:copy,method,2,8,8,73,9.12,0,0,['self'],[None],[None],244,[]
parser/test3/utils/file_utils.py:ZstdExtractor:is_extractable,ZstdExtractor:is_extractable,method,7,20,18,142,7.1,0,1,['path'],[' str'],[None],734,"['        """"""https://datatracker.ietf.org/doc/html/rfc8878\n', '\n', '        Magic_Number:  4 bytes, little-endian format.  Value: 0xFD2FB528.\n', '        """"""\n']"
parser/test3/utils/file_utils.py:ZstdExtractor:extract,ZstdExtractor:extract,method,10,25,23,220,8.8,0,1,"['input_path', 'output_path']","[' str', ' str']","[None, None]",747,[]
parser/test3/utils/info_utils.py:verify_checksums,verify_checksums,function,12,65,48,739,11.37,1,5,"['expected_checksums', 'recorded_checksums', 'verification_name']","[' Optional[dict]', ' dict', None]","[None, None, 'None']",28,[]
parser/test3/utils/info_utils.py:verify_splits,verify_splits,function,10,51,40,619,12.14,1,5,"['expected_splits', 'recorded_splits']","[' Optional[dict]', ' dict']","[None, None]",60,[]
parser/test3/utils/info_utils.py:get_size_checksum_dict,get_size_checksum_dict,function,9,21,21,159,7.57,1,0,['path'],[' str'],[None],78,"['    """"""Compute the file size and the sha256 checksum of a file""""""\n']"
parser/test3/utils/info_utils.py:is_small_dataset,is_small_dataset,function,4,11,9,106,9.64,0,1,['dataset_size'],[None],[None],87,"['    """"""Check if `dataset_size` is smaller than `config.IN_MEMORY_MAX_SIZE`.\n', '\n', '    Args:\n', '        dataset_size (int): Dataset size in bytes.\n', '\n', '    Returns:\n', '        bool: Whether `dataset_size` is smaller than `config.IN_MEMORY_MAX_SIZE`.\n', '    """"""\n']"
parser/test3/utils/info_utils.py:ChecksumVerificationException,ChecksumVerificationException,class,0,0,0,0,0,0,0,[],[],[],12,[]
parser/test3/utils/info_utils.py:UnexpectedDownloadedFile,UnexpectedDownloadedFile,class,0,0,0,0,0,0,0,[],[],[],16,[]
parser/test3/utils/info_utils.py:ExpectedMoreDownloadedFiles,ExpectedMoreDownloadedFiles,class,0,0,0,0,0,0,0,[],[],[],20,[]
parser/test3/utils/info_utils.py:NonMatchingChecksumError,NonMatchingChecksumError,class,0,0,0,0,0,0,0,[],[],[],24,[]
parser/test3/utils/info_utils.py:SplitsVerificationException,SplitsVerificationException,class,0,0,0,0,0,0,0,[],[],[],44,[]
parser/test3/utils/info_utils.py:UnexpectedSplits,UnexpectedSplits,class,0,0,0,0,0,0,0,[],[],[],48,[]
parser/test3/utils/info_utils.py:ExpectedMoreSplits,ExpectedMoreSplits,class,0,0,0,0,0,0,0,[],[],[],52,[]
parser/test3/utils/info_utils.py:NonMatchingSplitsSizesError,NonMatchingSplitsSizesError,class,0,0,0,0,0,0,0,[],[],[],56,[]
parser/test3/utils/logging.py:_get_default_logging_level,_get_default_logging_level,function,12,29,26,288,9.93,0,2,[],[],[],41,"['    """"""\n', '    If DATASETS_VERBOSITY env var is set to one of the valid choices return that as the new default level.\n', '    If it is not - fall back to ``_default_log_level``\n', '    """"""\n']"
parser/test3/utils/logging.py:_get_library_name,_get_library_name,function,2,2,2,28,14.0,0,0,[],[],[],58,[]
parser/test3/utils/logging.py:_get_library_root_logger,_get_library_root_logger,function,2,2,2,44,22.0,0,0,[],[],[],62,[]
parser/test3/utils/logging.py:_configure_library_root_logger,_configure_library_root_logger,function,3,3,3,105,35.0,0,0,[],[],[],66,[]
parser/test3/utils/logging.py:_reset_library_root_logger,_reset_library_root_logger,function,3,3,3,91,30.33,0,0,[],[],[],72,[]
parser/test3/utils/logging.py:get_logger,get_logger,function,4,8,7,68,8.5,0,1,['name'],[' Optional[str] '],[' None'],77,"['    """"""Return a logger with the specified name.\n', '    This function can be used in dataset and metrics scripts.\n', '    """"""\n']"
parser/test3/utils/logging.py:get_verbosity,get_verbosity,function,2,2,2,52,26.0,0,0,[],[],[],86,"['    """"""Return the current level for the HuggingFace datasets library\'s root logger.\n', '    Returns:\n', '        Logging level, e.g., ``datasets.logging.DEBUG`` and ``datasets.logging.INFO``.\n', '    .. note::\n', '        HuggingFace datasets library has following logging levels:\n', '        - ``datasets.logging.CRITICAL``, ``datasets.logging.FATAL``\n', '        - ``datasets.logging.ERROR``\n', '        - ``datasets.logging.WARNING``, ``datasets.logging.WARN``\n', '        - ``datasets.logging.INFO``\n', '        - ``datasets.logging.DEBUG``\n', '    """"""\n']"
parser/test3/utils/logging.py:set_verbosity,set_verbosity,function,7,13,10,260,20.0,0,0,['verbosity'],[' int'],[None],101,"['    """"""Set the level for the HuggingFace datasets library\'s root logger.\n', '    Args:\n', '        verbosity:\n', '            Logging level, e.g., ``datasets.logging.DEBUG`` and ``datasets.logging.INFO``.\n', '    """"""\n']"
parser/test3/utils/logging.py:set_verbosity_info,set_verbosity_info,function,2,2,2,25,12.5,0,0,[],[],[],110,"['    """"""Set the level for the HuggingFace datasets library\'s root logger to INFO.\n', '\n', '    This will display most of the logging information and tqdm bars.\n', '\n', '    Shortcut to ``datasets.logging.set_verbosity(datasets.logging.INFO)``\n', '    """"""\n']"
parser/test3/utils/logging.py:set_verbosity_warning,set_verbosity_warning,function,2,2,2,28,14.0,0,0,[],[],[],120,"['    """"""Set the level for the HuggingFace datasets library\'s root logger to WARNING.\n', '\n', '    This will display only the warning and errors logging information (no tqdm bars).\n', '\n', '    Shortcut to ``datasets.logging.set_verbosity(datasets.logging.WARNING)``\n', '    """"""\n']"
parser/test3/utils/logging.py:set_verbosity_debug,set_verbosity_debug,function,2,2,2,26,13.0,0,0,[],[],[],130,"['    """"""Set the level for the HuggingFace datasets library\'s root logger to DEBUG.\n', '\n', '    This will display all the logging information and tqdm bars.\n', '\n', '    Shortcut to ``datasets.logging.set_verbosity(datasets.logging.DEBUG)``\n', '    """"""\n']"
parser/test3/utils/logging.py:set_verbosity_error,set_verbosity_error,function,2,2,2,26,13.0,0,0,[],[],[],140,"['    """"""Set the level for the HuggingFace datasets library\'s root logger to ERROR.\n', '\n', '    This will display only the errors logging information (no tqdm bars).\n', '\n', '    Shortcut to ``datasets.logging.set_verbosity(datasets.logging.ERROR)``\n', '    """"""\n']"
parser/test3/utils/logging.py:disable_propagation,disable_propagation,function,1,2,2,42,21.0,0,0,[],[],[],150,"['    """"""Disable propagation of the library log outputs.\n', '    Note that log propagation is disabled by default.\n', '    """"""\n']"
parser/test3/utils/logging.py:enable_propagation,enable_propagation,function,1,2,2,41,20.5,0,0,[],[],[],157,"['    """"""Enable propagation of the library log outputs.\n', ""    Please disable the HuggingFace datasets library's default handler to prevent double logging if the root logger has\n"", '    been configured.\n', '    """"""\n']"
parser/test3/utils/metadata.py:load_json_resource,load_json_resource,function,4,6,6,116,19.33,0,0,['resource'],[' str'],[None],26,[]
parser/test3/utils/metadata.py:yaml_block_from_readme,yaml_block_from_readme,function,6,27,24,221,8.19,0,1,['path'],[' Path'],[None],57,[]
parser/test3/utils/metadata.py:metadata_dict_from_readme,metadata_dict_from_readme,function,5,15,12,164,10.93,0,1,['path'],[' Path'],[None],68,"['    """"""Loads a dataset\'s metadata from the dataset card (REAMDE.md), as a Python dict""""""\n']"
parser/test3/utils/metadata.py:tagset_validator,tagset_validator,function,11,105,41,652,6.21,4,6,"['items', 'Dict[str', 'List[str]]]', 'reference_values', 'name', 'url', 'escape_validation_predicate_fn', 'bool]] ', '']","[' Union[List[str]', None, None, ' List[str]', ' str', ' str', ' Optional[Callable[[Any]', None, None]","[None, None, None, None, None, None, None, ' None', None]",80,[]
parser/test3/utils/metadata.py:validate_type,validate_type,function,30,196,96,1670,8.52,3,11,"['value', 'expected_type']","[' Any', ' Type']","[None, None]",111,[]
parser/test3/utils/metadata.py:validate_metadata_type,validate_metadata_type,function,12,39,35,419,10.74,2,2,['metadata_dict'],[' dict'],[None],171,[]
parser/test3/utils/metadata.py:NoDuplicateSafeLoader,NoDuplicateSafeLoader,class,16,51,39,490,9.61,2,2,[],[],[],42,[]
parser/test3/utils/metadata.py:DatasetMetadata,DatasetMetadata,class,122,609,320,6830,11.22,6,15,[],[],[],186,[]
parser/test3/utils/metadata.py:NoDuplicateSafeLoader:_check_no_duplicates_on_constructed_node,NoDuplicateSafeLoader:_check_no_duplicates_on_constructed_node,method,10,38,28,274,7.21,2,2,"['self', 'node']","[None, None]","[None, None]",43,[]
parser/test3/utils/metadata.py:NoDuplicateSafeLoader:construct_mapping,NoDuplicateSafeLoader:construct_mapping,method,4,6,5,115,19.17,0,0,"['self', 'node', 'deep']","[None, None, None]","[None, None, 'False']",51,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate,DatasetMetadata:validate,method,38,96,89,1687,17.57,1,2,['self'],[None],[None],199,[]
parser/test3/utils/metadata.py:DatasetMetadata:from_readme,DatasetMetadata:from_readme,method,5,19,18,162,8.53,0,1,"['cls', 'path']","[None, ' Path']","[None, None]",241,"['        """"""Loads and validates the dataset metadat from its dataset card (README.md)\n', '\n', '        Args:\n', '            path (:obj:`Path`): Path to the dataset card (its README.md file)\n', '\n', '        Returns:\n', ""            :class:`DatasetMetadata`: The dataset's metadata\n"", '\n', '        Raises:\n', '            :obj:`TypeError`: If the dataset card has no metadata (no YAML header)\n', ""            :obj:`TypeError`: If the dataset's metadata is invalid\n"", '        """"""\n']"
parser/test3/utils/metadata.py:DatasetMetadata:from_yaml_string,DatasetMetadata:from_yaml_string,method,4,7,7,91,13.0,0,0,"['cls', 'string']","[None, ' str']","[None, None]",261,"['        """"""Loads and validates the dataset metadat from a YAML string\n', '\n', '        Args:\n', '            string (:obj:`str`): The YAML string\n', '\n', '        Returns:\n', ""            :class:`DatasetMetadata`: The dataset's metadata\n"", '\n', '        Raises:\n', ""            :obj:`TypeError`: If the dataset's metadata is invalid\n"", '        """"""\n']"
parser/test3/utils/metadata.py:DatasetMetadata:validate_annotations_creators,DatasetMetadata:validate_annotations_creators,method,2,7,7,118,16.86,0,0,"['annotations_creators', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",277,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_language_creators,DatasetMetadata:validate_language_creators,method,2,5,5,107,21.4,0,0,"['language_creators', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",283,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_language_codes,DatasetMetadata:validate_language_codes,method,2,5,5,98,19.6,0,0,"['languages', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",287,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_licences,DatasetMetadata:validate_licences,method,8,18,16,174,9.67,0,0,"['licenses', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",291,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_task_categories,DatasetMetadata:validate_task_categories,method,5,16,14,189,11.81,0,0,"['task_categories', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",302,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_task_ids,DatasetMetadata:validate_task_ids,method,8,29,24,220,7.59,1,0,"['task_ids', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",312,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_mulitlinguality,DatasetMetadata:validate_mulitlinguality,method,7,14,12,190,13.57,0,0,"['multilinguality', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",322,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_size_catgeories,DatasetMetadata:validate_size_catgeories,method,2,5,5,99,19.8,0,0,"['size_cats', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",333,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_source_datasets,DatasetMetadata:validate_source_datasets,method,9,40,35,290,7.25,1,2,"['sources', 'Dict[str', 'List[str]]]']","[' Union[List[str]', None, None]","[None, None, None]",337,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_paperswithcode_id_errors,DatasetMetadata:validate_paperswithcode_id_errors,method,5,51,39,343,6.73,0,2,['paperswithcode_id'],[' Optional[str]'],[None],352,[]
parser/test3/utils/metadata.py:DatasetMetadata:validate_pretty_name,DatasetMetadata:validate_pretty_name,method,8,64,39,370,5.78,1,4,"['pretty_name', 'Dict[str', 'str]]']","[' Union[str', None, None]","[None, None, None]",365,[]
parser/test3/utils/metadata.py:DatasetMetadata:get_metadata_by_config_name,DatasetMetadata:get_metadata_by_config_name,method,21,84,67,753,8.96,2,4,"['self', 'name']","[None, ' str']","[None, None]",380,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager,MockDownloadManager,class,73,315,180,4205,13.35,7,11,[],[],[],33,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:__init__,MockDownloadManager:__init__,method,19,23,23,336,14.61,0,0,"['self', 'dataset_name', 'config', 'version', 'str]', 'cache_dir', 'use_local_dummy_data', 'load_existing_dummy_data', 'download_callbacks', '']","[None, ' str', ' str', ' Union[Version', None, ' Optional[str] ', ' bool ', ' bool ', ' Optional[List[Callable]] ', None]","[None, None, None, None, None, ' None', ' False', ' True', ' None', None]",37,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:dummy_file,MockDownloadManager:dummy_file,method,3,8,6,92,11.5,0,1,['self'],[None],[None],65,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:dummy_data_folder,MockDownloadManager:dummy_data_folder,method,3,12,10,132,11.0,0,1,['self'],[None],[None],71,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:dummy_zip_file,MockDownloadManager:dummy_zip_file,method,2,3,3,59,19.67,0,0,['self'],[None],[None],79,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:download_dummy_data,MockDownloadManager:download_dummy_data,method,5,20,19,297,14.85,0,0,['self'],[None],[None],82,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:local_path_to_dummy_data,MockDownloadManager:local_path_to_dummy_data,method,2,4,4,83,20.75,0,0,['self'],[None],[None],94,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:github_path_to_dummy_data,MockDownloadManager:github_path_to_dummy_data,method,3,10,8,138,13.8,0,1,['self'],[None],[None],98,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:manual_dir,MockDownloadManager:manual_dir,method,3,7,6,123,17.57,0,1,['self'],[None],[None],104,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:download_and_extract,MockDownloadManager:download_and_extract,method,9,24,17,338,14.08,0,2,"['self', 'data_url', '*args']","[None, None, None]","[None, None, None]",112,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:download,MockDownloadManager:download,method,5,20,19,297,14.85,0,0,['self'],[None],[None],129,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:download_custom,MockDownloadManager:download_custom,method,2,2,2,41,20.5,0,0,"['self', 'data_url', 'custom_download']","[None, None, None]","[None, None, None]",133,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:extract,MockDownloadManager:extract,method,2,2,2,10,5.0,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",137,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:get_recorded_sizes_checksums,MockDownloadManager:get_recorded_sizes_checksums,method,1,2,2,8,4.0,0,0,['self'],[None],[None],141,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:create_dummy_data_dict,MockDownloadManager:create_dummy_data_dict,method,16,61,38,760,12.46,4,3,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",144,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:create_dummy_data_list,MockDownloadManager:create_dummy_data_list,method,13,39,30,529,13.56,2,1,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",171,[]
parser/test3/utils/mock_download_manager.py:MockDownloadManager:create_dummy_data_single,MockDownloadManager:create_dummy_data_single,method,7,18,17,264,14.67,1,1,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",189,[]
parser/test3/utils/py_utils.py:size_str,size_str,function,5,48,35,320,6.67,1,2,['size_in_bytes'],[None],[None],59,"['    """"""Returns a human readable size string.\n', '\n', '    If size_in_bytes is None, then returns ""Unknown size"".\n', '\n', '    For example `size_str(1.5 * datasets.units.GiB) == ""1.50 GiB""`.\n', '\n', '    Args:\n', '        size_in_bytes: `int` or `None`, the size, in bytes, that we want to\n', '            format as a human-readable size string.\n', '    """"""\n']"
parser/test3/utils/py_utils.py:temporary_assignment,temporary_assignment,function,4,13,10,102,7.85,0,0,"['obj', 'attr', 'value']","[None, None, None]","[None, None, None]",84,"['    """"""Temporarily assign obj.attr to value.""""""\n']"
parser/test3/utils/py_utils.py:zip_dict,zip_dict,function,3,16,14,87,5.44,1,0,['*dicts'],[None],[None],94,"['    """"""Iterate over items of dictionaries grouped by their keys.""""""\n']"
parser/test3/utils/py_utils.py:_single_map_nested,_single_map_nested,function,24,101,62,805,7.97,0,5,['args'],[None],[None],136,"['    """"""Apply a function recursively to each element of a nested data struct.""""""\n']"
parser/test3/utils/py_utils.py:map_nested,map_nested,function,43,202,130,1641,8.12,3,10,"['function', 'data_struct', 'dict_only', 'map_list', 'map_tuple', 'map_numpy', 'num_proc', 'types', '']","[None, None, ' bool ', ' bool ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' False', ' True', ' False', ' False', ' None', 'None', None]",169,"['    """"""Apply a function recursively to each element of a nested data struct.\n', '    If num_proc > 1 and the length of data_struct is longer than num_proc: use multi-processing\n', '    """"""\n']"
parser/test3/utils/py_utils.py:zip_nested,zip_nested,function,6,36,27,279,7.75,0,2,"['arg0', '*args', '**kwargs']","[None, None, None]","[None, None, None]",241,"['    """"""Zip data struct together and return a data struct with the same shape.""""""\n']"
parser/test3/utils/py_utils.py:flatten_nest_dict,flatten_nest_dict,function,9,23,19,186,8.09,1,1,['d'],[None],[None],257,"['    """"""Return the dict with all nested keys flattened joined with \'/\'.""""""\n']"
parser/test3/utils/py_utils.py:has_sufficient_disk_space,has_sufficient_disk_space,function,5,11,9,115,10.45,0,0,"['needed_bytes', 'directory']","[None, None]","[None, '"".""']",283,[]
parser/test3/utils/py_utils.py:dump,dump,function,2,3,3,43,14.33,0,0,"['obj', 'file']","[None, None]","[None, None]",310,"['    """"""pickle an object to a file""""""\n']"
parser/test3/utils/py_utils.py:_no_cache_fields,_no_cache_fields,function,9,29,26,257,8.86,0,1,['obj'],[None],[None],317,[]
parser/test3/utils/py_utils.py:dumps,dumps,function,7,8,8,79,9.88,0,0,['obj'],[None],[None],336,"['    """"""pickle an object to a string""""""\n']"
parser/test3/utils/py_utils.py:pklregister,pklregister,function,4,8,6,63,7.88,0,0,['t'],[None],[None],344,[]
parser/test3/utils/py_utils.py:_save_code,_save_code,function,9,82,44,1027,12.52,0,3,"['pickler', 'obj']","[None, None]","[None, None]",394,"['    """"""\n', '    From dill._dill.save_code\n', '    This is a modified version that removes the origin (filename + line no.)\n', '    of functions created in notebooks or shells for example.\n', '    """"""\n']"
parser/test3/utils/py_utils.py:save_function,save_function,function,28,183,100,1674,9.15,1,13,"['pickler', 'obj']","[None, None]","[None, None]",475,"['    """"""\n', '    From dill._dill.save_function\n', '    This is a modified version that make globs deterministic since the order of\n', '    the keys in the output dictionary of globalvars can change.\n', '    """"""\n']"
parser/test3/utils/py_utils.py:copyfunc,copyfunc,function,5,10,9,161,16.1,0,0,['func'],[None],[None],548,[]
parser/test3/utils/py_utils.py:NonMutableDict,NonMutableDict,class,11,58,46,550,9.48,0,3,[],[],[],101,[]
parser/test3/utils/py_utils.py:classproperty,classproperty,class,3,7,7,74,10.57,0,0,[],[],[],129,[]
parser/test3/utils/py_utils.py:NestedDataStructure,NestedDataStructure,class,9,46,28,312,6.78,2,3,[],[],[],269,[]
parser/test3/utils/py_utils.py:Pickler,Pickler,class,11,27,25,327,12.11,0,1,[],[],[],291,[]
parser/test3/utils/py_utils.py:_CloudPickleTypeHintFix,_CloudPickleTypeHintFix,class,21,115,68,1047,9.1,0,2,[],[],[],352,[]
parser/test3/utils/py_utils.py:NonMutableDict:__init__,NonMutableDict:__init__,method,5,22,22,203,9.23,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",109,[]
parser/test3/utils/py_utils.py:NonMutableDict:__setitem__,NonMutableDict:__setitem__,method,4,10,10,117,11.7,0,1,"['self', 'key', 'value']","[None, None, None]","[None, None, None]",118,[]
parser/test3/utils/py_utils.py:NonMutableDict:update,NonMutableDict:update,method,4,15,14,139,9.27,0,1,"['self', 'other']","[None, None]","[None, None]",123,[]
parser/test3/utils/py_utils.py:classproperty:__get__,classproperty:__get__,method,2,3,3,39,13.0,0,0,"['self', 'obj', 'objtype']","[None, None, None]","[None, None, 'None']",132,[]
parser/test3/utils/py_utils.py:NestedDataStructure:__init__,NestedDataStructure:__init__,method,2,9,8,35,3.89,0,1,"['self', 'data']","[None, None]","[None, 'None']",270,[]
parser/test3/utils/py_utils.py:NestedDataStructure:flatten,NestedDataStructure:flatten,method,7,31,22,219,7.06,2,2,"['self', 'data']","[None, None]","[None, 'None']",273,[]
parser/test3/utils/py_utils.py:Pickler:save_global,Pickler:save_global,method,8,21,20,224,10.67,0,1,"['self', 'obj', 'name']","[None, None, None]","[None, None, 'None']",296,[]
parser/test3/utils/py_utils.py:_CloudPickleTypeHintFix:_is_parametrized_type_hint,_CloudPickleTypeHintFix:_is_parametrized_type_hint,method,6,34,18,232,6.82,0,0,['obj'],[None],[None],360,[]
parser/test3/utils/py_utils.py:_CloudPickleTypeHintFix:_create_parametrized_type_hint,_CloudPickleTypeHintFix:_create_parametrized_type_hint,method,2,2,2,18,9.0,0,0,"['origin', 'args']","[None, None]","[None, None]",367,[]
parser/test3/utils/py_utils.py:_CloudPickleTypeHintFix:_save_parametrized_type_hint,_CloudPickleTypeHintFix:_save_parametrized_type_hint,method,11,71,44,665,9.37,0,2,"['pickler', 'obj']","[None, None]","[None, None]",370,[]
parser/test3/utils/readme.py:load_yaml_resource,load_yaml_resource,function,4,6,6,120,20.0,0,0,['resource'],[' str'],[None],23,[]
parser/test3/utils/readme.py:Section,Section,class,49,370,199,3279,8.86,3,21,[],[],[],40,[]
parser/test3/utils/readme.py:ReadMe,ReadMe,class,53,334,188,2885,8.64,1,10,[],[],[],173,[]
parser/test3/utils/readme.py:Section:__init__,Section:__init__,method,12,22,20,240,10.91,0,1,"['self', 'name', 'level', 'lines', 'suppress_parsing_errors']","[None, ' str', ' str', ' List[str] ', ' bool ']","[None, None, None, ' None', ' False']",41,[]
parser/test3/utils/readme.py:Section:parse,Section:parse,method,22,149,84,1344,9.02,1,10,"['self', 'suppress_parsing_errors']","[None, ' bool ']","[None, ' False']",53,[]
parser/test3/utils/readme.py:Section:validate,Section:validate,method,23,157,92,1321,8.41,2,10,"['self', 'structure']","[None, ' dict']","[None, None]",95,"['        """"""Validates a Section class object recursively using the structure provided as a dictionary.\n', '\n', '        Args:\n', '            structute (:obj: `dict`): The dictionary representing expected structure.\n', '\n', '        Returns:\n', '            :obj: `ReadmeValidatorOutput`: The dictionary representation of the section, and the errors.\n', '        """"""\n']"
parser/test3/utils/readme.py:Section:to_dict,Section:to_dict,method,1,15,15,145,9.67,0,0,['self'],[None],[None],163,"['        """"""Returns the dictionary representation of a section.""""""\n']"
parser/test3/utils/readme.py:ReadMe:__init__,ReadMe:__init__,method,8,30,29,254,8.47,0,1,"['self', 'name', 'lines', 'structure', 'suppress_parsing_errors']","[None, ' str', ' List[str]', ' dict ', ' bool ']","[None, None, None, ' None', ' False']",174,[]
parser/test3/utils/readme.py:ReadMe:validate,ReadMe:validate,method,10,40,34,373,9.32,0,2,['self'],[None],[None],183,[]
parser/test3/utils/readme.py:ReadMe:from_readme,ReadMe:from_readme,method,6,12,12,135,11.25,0,0,"['cls', 'path', 'structure', 'suppress_parsing_errors']","[None, ' Path', ' dict ', ' bool ']","[None, None, ' None', ' False']",194,[]
parser/test3/utils/readme.py:ReadMe:from_string,ReadMe:from_string,method,4,7,7,109,15.57,0,0,"['cls', 'string', 'structure', 'root_name', 'suppress_parsing_errors']","[None, ' str', ' dict ', ' str ', ' bool ']","[None, None, ' None', ' ""root""', ' False']",200,[]
parser/test3/utils/readme.py:ReadMe:parse,ReadMe:parse,method,15,42,30,334,7.95,1,3,"['self', 'suppress_parsing_errors']","[None, ' bool ']","[None, ' False']",53,[]
parser/test3/utils/readme.py:ReadMe:__str__,ReadMe:__str__,method,2,2,2,25,12.5,0,0,['self'],[None],[None],223,"['        """"""Returns the string of dictionary representation of the ReadMe.""""""\n']"
parser/test3/utils/readme.py:ReadMe:_validate,ReadMe:_validate,method,17,150,85,1203,8.02,0,4,"['self', 'readme_structure']","[None, None]","[None, None]",227,[]
parser/test3/utils/streaming_download_manager.py:xjoin,xjoin,function,7,15,14,114,7.6,0,1,"['a', '*p']","[None, None]","[None, None]",20,"['    """"""\n', '    This function extends os.path.join to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xjoin function allows you to apply the join on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xjoin(""zip://folder1::https://host.com/archive.zip"", ""file.txt"")\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '    """"""\n']"
parser/test3/utils/streaming_download_manager.py:_add_retries_to_file_obj_read_method,_add_retries_to_file_obj_read_method,function,14,41,39,472,11.51,1,0,['file_obj'],[None],[None],47,[]
parser/test3/utils/streaming_download_manager.py:xopen,xopen,function,7,15,14,284,18.93,0,1,"['file', 'mode', '*args', '**kwargs']","[None, None, None, None]","[None, '""r""', None, None]",68,"['    """"""\n', '    This function extends the builin `open` function to support remote files using fsspec.\n', '\n', '    It also has a retry mechanism in case connection fails.\n', '    The args and kwargs are passed to fsspec.open, except `use_auth_token` which is used for queries to private repos on huggingface.co\n', '    """"""\n']"
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager,StreamingDownloadManager,class,36,120,86,1444,12.03,0,3,[],[],[],82,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:__init__,StreamingDownloadManager:__init__,method,11,12,11,159,13.25,0,0,"['self', 'dataset_name', 'data_dir', 'download_config', 'base_path', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[DownloadConfig] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' None', None]",90,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:manual_dir,StreamingDownloadManager:manual_dir,method,2,2,2,20,10.0,0,0,['self'],[None],[None],103,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:download,StreamingDownloadManager:download,method,3,6,5,83,13.83,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",106,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:_download,StreamingDownloadManager:_download,method,4,7,6,124,17.71,0,1,"['self', 'url_or_filename']","[None, None]","[None, None]",110,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:extract,StreamingDownloadManager:extract,method,3,6,5,78,13.0,0,0,"['self', 'path_or_paths']","[None, None]","[None, None]",116,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:_extract,StreamingDownloadManager:_extract,method,5,16,12,231,14.44,0,1,"['self', 'urlpath']","[None, None]","[None, None]",120,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:_get_extraction_protocol,StreamingDownloadManager:_get_extraction_protocol,method,6,30,26,277,9.23,0,1,"['self', 'urlpath']","[None, None]","[None, None]",131,[]
parser/test3/utils/streaming_download_manager.py:StreamingDownloadManager:download_and_extract,StreamingDownloadManager:download_and_extract,method,2,2,2,46,23.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",141,[]
parser/test3/utils/tqdm_utils.py:tqdm,tqdm,function,4,9,8,84,9.33,0,1,"['*args', '**kwargs']","[None, None]","[None, None]",52,[]
parser/test3/utils/tqdm_utils.py:async_tqdm,async_tqdm,function,4,9,8,82,9.11,0,1,"['*args', '**kwargs']","[None, None]","[None, None]",59,[]
parser/test3/utils/tqdm_utils.py:disable_progress_bar,disable_progress_bar,function,1,4,4,27,6.75,0,0,[],[],[],66,"['    """"""Disabled Tqdm progress bar.\n', '\n', '    Usage:\n', '\n', '    datasets.disable_progress_bar()\n', '    """"""\n']"
parser/test3/utils/tqdm_utils.py:_async_tqdm,_async_tqdm,function,6,30,24,164,5.47,0,0,"['*args', '**kwargs']","[None, None]","[None, None]",79,"['    """"""Wrapper around Tqdm which can be updated in threads.\n', '\n', '    Usage:\n', '\n', '    ```\n', '    with utils.async_tqdm(...) as pbar:\n', '        # pbar can then be modified inside a thread\n', '        # pbar.update_total(3)\n', '        # pbar.update()\n', '    ```\n', '\n', '    Args:\n', '        *args: args of tqdm\n', '        **kwargs: kwargs of tqdm\n', '\n', '    Yields:\n', '        pbar: Async pbar which can be shared between threads.\n', '    """"""\n']"
parser/test3/utils/tqdm_utils.py:EmptyTqdm,EmptyTqdm,class,11,39,27,332,8.51,0,1,[],[],[],25,[]
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync,_TqdmPbarAsync,class,19,36,28,366,10.17,1,0,[],[],[],105,[]
parser/test3/utils/tqdm_utils.py:EmptyTqdm:__init__,EmptyTqdm:__init__,method,2,2,2,26,13.0,0,0,"['self', '*args', '**kwargs)']","[None, None, '  # pylint: disable']","[None, None, 'unused-argumentself._iterator = args[0] if args else Noneself):']",28,[]
parser/test3/utils/tqdm_utils.py:EmptyTqdm:__iter__,EmptyTqdm:__iter__,method,2,2,2,26,13.0,0,0,['self'],[None],[None],31,[]
parser/test3/utils/tqdm_utils.py:EmptyTqdm:__getattr__,EmptyTqdm:__getattr__,method,2,9,9,81,9.0,0,0,"['self', '_']","[None, None]","[None, None]",34,"['        """"""Return empty function.""""""\n']"
parser/test3/utils/tqdm_utils.py:EmptyTqdm:__enter__,EmptyTqdm:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],42,[]
parser/test3/utils/tqdm_utils.py:EmptyTqdm:__exit__,EmptyTqdm:__exit__,method,1,1,1,6,6.0,0,0,"['self', 'type_', 'value', 'traceback']","[None, None, None, None]","[None, None, None, None]",45,[]
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync:__init__,_TqdmPbarAsync:__init__,method,5,5,5,80,16.0,0,0,"['self', 'pbar']","[None, None]","[None, None]",110,[]
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync:update_total,_TqdmPbarAsync:update_total,method,5,6,6,50,8.33,0,0,"['self', 'n']","[None, None]","[None, '1']",115,"['        """"""Increment total pbar value.""""""\n']"
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync:update,_TqdmPbarAsync:update,method,7,13,11,123,9.46,0,0,"['self', 'n']","[None, None]","[None, '1']",121,[]
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync:refresh,_TqdmPbarAsync:refresh,method,3,5,5,40,8.0,1,0,['self'],[None],[None],127,"['        """"""Refresh all.""""""\n']"
parser/test3/utils/tqdm_utils.py:_TqdmPbarAsync:clear,_TqdmPbarAsync:clear,method,1,1,1,21,21.0,0,0,['self'],[None],[None],132,"['        """"""Remove the tqdm pbar from the update.""""""\n']"
parser/test3/utils/version.py:_str_to_version,_str_to_version,function,15,53,41,382,7.21,0,3,"['version_str', 'allow_wildcard']","[None, None]","[None, 'False']",109,"['    """"""Return the tuple (major, minor, patch) version extracted from the str.""""""\n']"
parser/test3/utils/version.py:Version,Version,class,37,152,83,1431,9.41,0,1,[],[],[],31,[]
parser/test3/utils/version.py:Version:__post_init__,Version:__post_init__,method,4,4,4,66,16.5,0,0,['self'],[None],[None],52,[]
parser/test3/utils/version.py:Version:__repr__,Version:__repr__,method,1,2,2,36,18.0,0,0,['self'],[None],[None],55,[]
parser/test3/utils/version.py:Version:tuple,Version:tuple,method,4,4,4,38,9.5,0,0,['self'],[None],[None],59,[]
parser/test3/utils/version.py:Version:_validate_operand,Version:_validate_operand,method,5,20,18,174,8.7,0,1,"['self', 'other']","[None, None]","[None, None]",62,[]
parser/test3/utils/version.py:Version:__eq__,Version:__eq__,method,5,5,5,65,13.0,0,0,"['self', 'other']","[None, None]","[None, None]",69,[]
parser/test3/utils/version.py:Version:__ne__,Version:__ne__,method,5,5,5,65,13.0,0,0,"['self', 'other']","[None, None]","[None, None]",73,[]
parser/test3/utils/version.py:Version:__lt__,Version:__lt__,method,5,6,6,64,10.67,0,0,"['self', 'other']","[None, None]","[None, None]",77,[]
parser/test3/utils/version.py:Version:__le__,Version:__le__,method,5,5,5,65,13.0,0,0,"['self', 'other']","[None, None]","[None, None]",81,[]
parser/test3/utils/version.py:Version:__gt__,Version:__gt__,method,5,6,6,64,10.67,0,0,"['self', 'other']","[None, None]","[None, None]",85,[]
parser/test3/utils/version.py:Version:__ge__,Version:__ge__,method,5,5,5,65,13.0,0,0,"['self', 'other']","[None, None]","[None, None]",89,[]
parser/test3/utils/version.py:Version:match,Version:match,method,6,20,14,150,7.5,0,0,"['self', 'other_version']","[None, None]","[None, None]",93,"['        """"""Returns True if other_version matches.\n', '\n', '        Args:\n', '            other_version: string, of the form ""x[.y[.x]]"" where {x,y,z} can be a\n', '                number or a wildcard.\n', '        """"""\n']"
parser/test3/utils/version.py:Version:from_dict,Version:from_dict,method,3,18,14,106,5.89,0,0,"['cls', 'dic']","[None, None]","[None, None]",104,[]
parser/test3/filesystems/compression/gzip.py:GZipFileSystem,GZipFileSystem,class,27,112,87,1193,10.65,0,3,[],[],[],9,[]
parser/test3/filesystems/compression/gzip.py:GZipFileSystem:__init__,GZipFileSystem:__init__,method,14,30,30,429,14.3,0,1,"['self', 'fo', 'mode', 'target_protocol', 'target_options', 'block_size', '**kwargs', '']","[None, ' str ', ' str ', ' Optional[str] ', ' Optional[dict] ', ' int ', None, None]","[None, ' """"', ' ""rb""', ' None', ' None', ' DEFAULT_BLOCK_SIZE', None, None]",15,"['        """"""\n', '        The GZipFileSystem can be instantiated from any gzip file.\n', '        It read the contents of GZip archive as a file-system with one file inside.\n', '        The single file inside the filesystem is named after the Gzip file, without "".gz"" at the end.\n', '\n', '        Args:\n', '            fo (:obj:``str``): Path to file containing GZIP. Will fetch file using ``fsspec.open()``\n', ""            mode (:obj:``str``): Currently, only 'rb' accepted\n"", '            target_protocol(:obj:``str``, optional): To override the FS protocol inferred from a URL.\n', '            target_options (:obj:``dict``, optional): Kwargs passed when instantiating the target FS.\n', '        """"""\n']"
parser/test3/filesystems/compression/gzip.py:GZipFileSystem:_strip_protocol,GZipFileSystem:_strip_protocol,method,2,2,2,47,23.5,0,0,"['cls', 'path']","[None, None]","[None, None]",46,[]
parser/test3/filesystems/compression/gzip.py:GZipFileSystem:_get_dirs,GZipFileSystem:_get_dirs,method,2,11,10,98,8.91,0,1,['self'],[None],[None],50,[]
parser/test3/filesystems/compression/gzip.py:GZipFileSystem:cat,GZipFileSystem:cat,method,2,2,2,29,14.5,0,0,"['self', 'path']","[None, ' str']","[None, None]",55,[]
parser/test3/filesystems/compression/gzip.py:GZipFileSystem:_open,GZipFileSystem:_open,method,6,14,13,162,11.57,0,1,"['self', 'path', 'mode', 'block_size', 'autocommit', 'cache_options', '**kwargs', '']","[None, ' str', ' str ', ' Optional[int] ', ' bool ', ' Optional[dict] ', None, None]","[None, None, ' ""rb""', ' None', ' True', ' None', None, None]",58,[]
parser/test3/packaged_modules/csv/csv.py:CsvConfig,CsvConfig,class,65,188,124,2447,13.02,1,3,[],[],[],19,[]
parser/test3/packaged_modules/csv/csv.py:Csv,Csv,class,31,121,91,1283,10.6,3,4,[],[],[],112,[]
parser/test3/packaged_modules/csv/csv.py:CsvConfig:__post_init__,CsvConfig:__post_init__,method,4,14,9,109,7.79,0,2,['self'],[None],[None],60,[]
parser/test3/packaged_modules/csv/csv.py:CsvConfig:read_csv_kwargs,CsvConfig:read_csv_kwargs,method,8,50,49,1194,23.88,1,1,['self'],[None],[None],67,[]
parser/test3/packaged_modules/csv/csv.py:Csv:_info,Csv:_info,method,2,2,2,57,28.5,0,0,['self'],[None],[None],115,[]
parser/test3/packaged_modules/csv/csv.py:Csv:_split_generators,Csv:_split_generators,method,11,50,38,562,11.24,1,4,"['self', 'dl_manager']","[None, None]","[None, None]",118,"['        """"""We handle string, list and dicts in datafiles""""""\n']"
parser/test3/packaged_modules/csv/csv.py:Csv:_generate_tables,Csv:_generate_tables,method,15,59,49,543,9.2,2,0,"['self', 'files']","[None, None]","[None, None]",135,[]
parser/test3/packaged_modules/json/json.py:JsonConfig,JsonConfig,class,17,38,28,471,12.39,0,0,[],[],[],15,[]
parser/test3/packaged_modules/json/json.py:Json,Json,class,36,196,124,2023,10.32,3,8,[],[],[],37,[]
parser/test3/packaged_modules/json/json.py:JsonConfig:pa_read_options,JsonConfig:pa_read_options,method,2,3,3,78,26.0,0,0,['self'],[None],[None],25,[]
parser/test3/packaged_modules/json/json.py:JsonConfig:pa_parse_options,JsonConfig:pa_parse_options,method,2,2,2,66,33.0,0,0,['self'],[None],[None],29,[]
parser/test3/packaged_modules/json/json.py:JsonConfig:schema,JsonConfig:schema,method,2,9,8,67,7.44,0,0,['self'],[None],[None],33,[]
parser/test3/packaged_modules/json/json.py:Json:_info,Json:_info,method,2,2,2,57,28.5,0,0,['self'],[None],[None],40,[]
parser/test3/packaged_modules/json/json.py:Json:_split_generators,Json:_split_generators,method,11,50,38,562,11.24,1,4,"['self', 'dl_manager']","[None, None]","[None, None]",43,"['        """"""We handle string, list and dicts in datafiles""""""\n']"
parser/test3/packaged_modules/json/json.py:Json:_generate_tables,Json:_generate_tables,method,21,134,84,1282,9.57,2,4,"['self', 'files']","[None, None]","[None, None]",60,[]
parser/test3/packaged_modules/pandas/pandas.py:Pandas,Pandas,class,20,70,51,776,11.09,2,4,[],[],[],9,[]
parser/test3/packaged_modules/pandas/pandas.py:Pandas:_info,Pandas:_info,method,2,2,2,28,14.0,0,0,['self'],[None],[None],10,[]
parser/test3/packaged_modules/pandas/pandas.py:Pandas:_split_generators,Pandas:_split_generators,method,11,50,38,562,11.24,1,4,"['self', 'dl_manager']","[None, None]","[None, None]",13,"['        """"""We handle string, list and dicts in datafiles""""""\n']"
parser/test3/packaged_modules/pandas/pandas.py:Pandas:_generate_tables,Pandas:_generate_tables,method,5,10,8,96,9.6,1,0,"['self', 'files']","[None, None]","[None, None]",30,[]
parser/test3/packaged_modules/parquet/parquet.py:ParquetConfig,ParquetConfig,class,5,9,9,96,10.67,0,0,[],[],[],17,[]
parser/test3/packaged_modules/parquet/parquet.py:Parquet,Parquet,class,38,176,122,1766,10.03,3,8,[],[],[],25,[]
parser/test3/packaged_modules/parquet/parquet.py:Parquet:_info,Parquet:_info,method,4,23,23,217,9.43,0,1,['self'],[None],[None],28,[]
parser/test3/packaged_modules/parquet/parquet.py:Parquet:_split_generators,Parquet:_split_generators,method,11,50,38,562,11.24,1,4,"['self', 'dl_manager']","[None, None]","[None, None]",35,"['        """"""We handle string, list and dicts in datafiles""""""\n']"
parser/test3/packaged_modules/parquet/parquet.py:Parquet:_generate_tables,Parquet:_generate_tables,method,20,93,65,862,9.27,2,3,"['self', 'files']","[None, None]","[None, None]",52,[]
parser/test3/packaged_modules/text/text.py:TextConfig,TextConfig,class,7,16,16,115,7.19,0,0,[],[],[],13,[]
parser/test3/packaged_modules/text/text.py:Text,Text,class,32,112,87,1215,10.85,3,5,[],[],[],22,[]
parser/test3/packaged_modules/text/text.py:Text:_info,Text:_info,method,2,2,2,57,28.5,0,0,['self'],[None],[None],25,[]
parser/test3/packaged_modules/text/text.py:Text:_split_generators,Text:_split_generators,method,11,50,38,562,11.24,1,4,"['self', 'dl_manager']","[None, None]","[None, None]",28,"['        """"""The `data_files` kwarg in load_dataset() can be a str, List[str], Dict[str,str], or Dict[str,List[str]].\n', '\n', ""        If str or List[str], then the dataset returns only the 'train' split.\n"", '        If dict, then keys should be from the `datasets.Split` enum.\n', '        """"""\n']"
parser/test3/packaged_modules/text/text.py:Text:_generate_tables,Text:_generate_tables,method,15,50,44,474,9.48,2,1,"['self', 'files']","[None, None]","[None, None]",49,[]
